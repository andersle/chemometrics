{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801b2291",
   "metadata": {},
   "source": [
    "# Exercise set 6: Partial least squares regression and model evaluation\n",
    "\n",
    "The main goals of this exercise are to perform Partial Least Squares (PLS) regression and use training and testing sets. Using training and testing sets allows us to assess the model's ability to generalise to unseen data and avoid overfitting. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Create a PLS regression model.\n",
    "- Create and use training and test sets.\n",
    "- Assess your regression model by calculating root mean squared errors.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [6.1(a)](#6.1(a)) and [6.1(d)](#6.1(d)): To show that you can create a training set and a test set, a PLS regression model, and evaluate your PLS model. You might find it helpful to also do [6.1(b)](#6.1(b)), [6.1(c)](#6.1(c)).\n",
    "\n",
    "**Files required for this exercise:**\n",
    "* [Exercise 6.1](#Exercise-6.1): [egg-storage.csv](egg-storage.csv)\n",
    "* [Exercise 6.2](#Exercise-6.2): [forbes.csv](forbes.csv)\n",
    "\n",
    "Please ensure that these files are saved in the same directory as this notebook.\n",
    "\n",
    "**Note:** [Exercise 6.2](#Exercise-6.2) is optional. It shows you how you can use cross-validation when we have too few samples to split into a training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b4748",
   "metadata": {},
   "source": [
    "## Exercise 6.1 \n",
    "\n",
    "In this exercise, we will use a data set that contains [NIR spectra of poultry eggs at different storage days](https://data.mendeley.com/datasets/6hn67h2trb/1). We will use these spectra to create a regression model for predicting the number of days an egg has been stored. The data is given in the file [egg-storage.csv](egg-storage.csv) and it has already been preprocessed so you can use it directly.\n",
    "\n",
    "You can load and visualise the spectra as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f161cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86244a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"egg-storage.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a454535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the days:\n",
    "y = data[\"days\"]\n",
    "# Extract the spectra:\n",
    "xvars = [i for i in data.columns if i not in (\"days\",)]\n",
    "X = data[xvars].to_numpy()\n",
    "# Extract the wavelengths for plotting:\n",
    "wavelengths = np.array([float(i.split(\"nm\")[0]) for i in xvars])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769aca2",
   "metadata": {},
   "source": [
    "To visualise the data, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "norm = Normalize(y.min(), y.max())\n",
    "y_normed = norm(y)\n",
    "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "color_days = cmap(y_normed)\n",
    "\n",
    "fig, ax1 = plt.subplots(constrained_layout=True)\n",
    "for i, spec in enumerate(X):\n",
    "    ax1.plot(wavelengths, spec, color=color_days[i])\n",
    "    \n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax1, label='Days')\n",
    "tick_locations = np.arange(0, y.max()+1)\n",
    "cbar.set_ticks(tick_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dad47d",
   "metadata": {},
   "source": [
    "### 6.1(a)\n",
    "\n",
    "**Task: Create a training set and a testing set by running the code below. How many spectra are in the training set, and how many are in the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec895322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=2026  # Make the data splitting process reproducible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80184e",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(a): How many spectra are in the training and test set, respectively?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab6e77",
   "metadata": {},
   "source": [
    "### 6.1(b)\n",
    "\n",
    "**Task: Create a least squares model using the training data. Calculate the R² value and the root mean squared error for the calibration (RMSEC).**\n",
    "\n",
    "**Hint:** Use [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from scikit-learn to create the regression model, e.g.,\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "The R² and RMSEC can be calculated using:\n",
    "```python\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "y_pred_train = model1.predict(X_train)  # Predict using the model\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmsec = root_mean_squared_error(y_train, y_pred_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a567",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(b): What values did you get for R² and RMSEC?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f980a",
   "metadata": {},
   "source": [
    "### 6.1(c)\n",
    "\n",
    "**Task: Evaluate your linear regression model by calculating R² and the root mean squared error of prediction (RMSEP) for the test set. How do you interpret these values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff977844",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(c): What values did you get for R² and RMSEP for the test set? How do you interpret these?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842d87d",
   "metadata": {},
   "source": [
    "### 6.1(d)\n",
    "\n",
    "**Task: Create a partial least squares (PLS) regression model by running the code below. Calculate the R² for the training and testing set, RMSEC and RMSEP. How do you interpret these values?**\n",
    "\n",
    "**Note:** If you have time, try generating the y vs. ŷ (observed vs. predicted) plot for the training set and for the test set. Visualising the results this way may help you spot outliers or non-linear trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "model_pls = PLSRegression(\n",
    "    n_components=25,  # Use 25 latent variables, more on this later.\n",
    "    scale=False  # Do not scale the X and y, since the spectra have been preprocessed.\n",
    ")\n",
    "\n",
    "model_pls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b855114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc75cb0",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(d): What values did you get for R², RMSEC, and RMSEP, and how do you interpret them?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f484ff",
   "metadata": {},
   "source": [
    "### 6.1(e)\n",
    "\n",
    "The PLS regression model you made above uses 25 latent variables. That sounds like a high number! We will check if that is correct by running cross-validation\n",
    "**Task: Run the code below and determine the number of latent variables to use for the PLS regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a grid search with cross-validation to look for the\n",
    "# best number of latent variables:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The parameter we are going to optimise is the number\n",
    "# of PLS components. We assume that this is in the range\n",
    "# from 1 to 50.\n",
    "parameters = {\"n_components\": range(1, 51)}\n",
    "\n",
    "# Next, we set up the grid search:\n",
    "grid = GridSearchCV(\n",
    "        PLSRegression(scale=False),  # This is the model we will make.\n",
    "        parameters,  # The parameters we optimise.\n",
    "        scoring=\"neg_root_mean_squared_error\",  # We use a negative RMSE as a scoring for models.\n",
    "        cv=10,  # Use 10 splits.\n",
    "        refit=True,  # Fit the best model to the whole data set.\n",
    "        n_jobs=4,  # Run in parallel, of 4 processes.\n",
    "        verbose=1,  # Print out slightly more while fitting.\n",
    ")\n",
    "# Why do we use a negative RMSE? It is because the GridSearchCV is maximising\n",
    "# the score, and to make a smaller RMSE better, we turn it into a negative\n",
    "# value.\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "# Store the optimised model:\n",
    "model_pls_opt = grid.best_estimator_\n",
    "print(model_pls_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044af54b",
   "metadata": {},
   "source": [
    "The grid search above might have picked many components (25) just because the error is slightly less than with fewer components. We can check where the error levels off to see if we should use fewer components. We can do this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71352cdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "score = -1.0 * grid.cv_results_[\"mean_test_score\"]\n",
    "score_std = grid.cv_results_[\"std_test_score\"]\n",
    "\n",
    "ax.errorbar(\n",
    "    parameters[\"n_components\"],\n",
    "    score,\n",
    "    yerr=score_std,\n",
    "    marker=\"o\",\n",
    "    markerfacecolor=\"none\",\n",
    "    ms=10,\n",
    ")\n",
    "ax.set(xlabel=\"PLS components\", ylabel=\"RMSE\")\n",
    "ax.set_title(\"Results from cross-validation (grid search)\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408da68",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(e): How many PLS components do you recommend using?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44457c41",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "\n",
    "It is not always feasible to do the split into training and test sets when we have few samples. Another option then is to use something called **Leave-one-out cross-validation** (LOOCV). LOOCV involves training the model on all but one data point and using the remaining point for testing, repeating this process for each data point. We will use that method in this exercise.\n",
    "\n",
    "We will use the data of [Forbes](https://doi.org/10.1017/S0080456800032075) (from Exercise 5) who investigated the\n",
    "relationship between the boiling point of water and the atmospheric pressure, and collected data in the Alps and Scotland. Forbes' goal was to estimate altitudes from the boiling point alone.\n",
    "\n",
    "LOOCV is described in [appendix A](#A.-Leave-one-out-cross-validation). It can be a good idea to read this before starting this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e9626",
   "metadata": {},
   "source": [
    "### 6.2(a)\n",
    "\n",
    "**Task: Load the data from Forbes (data file [forbes.csv](forbes.csv)), plot it, and create a linear regression model (use scikit-learn)\n",
    "that predicts the atmospheric pressure from the temperature. Report the R² and [root mean\n",
    "squared error (RMSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html) for your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcdcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f8424",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(a): What value did you get for R² and the RMSE?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf323ee5",
   "metadata": {},
   "source": [
    "### 6.2(b)\n",
    "\n",
    "**Task: Estimate the error you can expect to make if you use your model for predicting the pressure.\n",
    "Do this by LOOCV and calculate the root mean squared error of cross-validation (RMSECV)**\n",
    "\n",
    "**Note:** LOOCV is a special case of **training** and **testing**, and you can find a short description of it\n",
    "in [appendix A](#A.-Leave-one-out-cross-validation) with example code for running LOOCV. The code example for LOOCV is concise, so make sure you understand what goes on here (that is, what LOOCV is doing). If you are working with someone, try explaining testing/training and how LOOCV works to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d299e",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(b): What value did you get for RMSECV?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deefc17",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c72be0",
   "metadata": {},
   "source": [
    "## A. Leave-one-out cross-validation\n",
    "\n",
    "In Leave-one-out cross-validation (LOOCV), we first pick one sample,\n",
    "measurement number $j$, and we fit the model using the $n-1$ other points\n",
    "(all points except $j$). After the fitting, we check how well the model can predict\n",
    "measurement $j$ by calculating the difference between the\n",
    "measured ($y_j$) and predicted ($\\tilde{y}_j$) value. This difference, $r_j = y_{j} - \\tilde{y}_j$, is\n",
    "called the predicted residual, and it tells us the error we just made.\n",
    "\n",
    "There is nothing special about picking point $j$, and we can try all possibilities\n",
    "of leaving one point out, fitting the model using the remaining $n-1$\n",
    "measurements, and predicting the value we left out.\n",
    "After doing this for all possibilities, we have fitted the model\n",
    "$n$ times and calculated $n$ predicted residuals. The mean squared error (obtained from the squared\n",
    "residuals), $\\mathrm{MSE}_{\\mathrm{CV}}$, can then be used\n",
    "to estimate the error in the model,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the measured $y$ in experiment $i$, and $\\tilde{y}_i$ is the\n",
    "predicted $y$, using a model which was fitted using all points *except* $y_i$.\n",
    "\n",
    "For a polynomial fitting, there is an alternative to refitting the model $n$ times. In fact,\n",
    "we can show that for polynomial fitting, the mean squared error can\n",
    "be obtained by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2 =\n",
    "\\frac{1}{n}\\sum_{i=1}^{m} \\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{y}_i$'s are predicted values using the\n",
    "model fitted with *all data points*,\n",
    "and $h_{ii}$ is the $i$'th diagonal element of the\n",
    "$\\mathbf{H}$ matrix (the projection matrix,\n",
    "see Eq.(4.49) on page 49 in our textbook),\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H} =\n",
    "\\mathbf{X} \n",
    "\\left( \n",
    "  \\mathbf{X}^\\mathrm{T} \\mathbf{X}\n",
    "\\right)^{-1}\n",
    "\\mathbf{X}^\\mathrm{T} = \\mathbf{X} \\mathbf{X}^+,\n",
    "\\end{equation}\n",
    "\n",
    "Note the difference between $\\hat{y}_i$ and $\\tilde{y}_i$, and the\n",
    "fact that we  do not have to do the\n",
    "refitting(!) to obtain the $\\mathrm{MSE}_{\\mathrm{CV}}$.\n",
    "\n",
    "When you calculate $\\mathrm{MSE}_{\\mathrm{CV}}$, use one of the two approaches above or both\n",
    "if you want to see if they give the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The examples below assume:\n",
    "# - that the matrix X is called X_temp\n",
    "# - that y is stored in the variable pressure.\n",
    "\n",
    "# Example 1 of LOOCV:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# scikit-learn has a method to pick out samples for leave-one-out:\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "# Split the X-data in X_temp into training and testing:\n",
    "for train_index, test_index in loo.split(X_temp):\n",
    "    # train_index = index of samples to use for training\n",
    "    # test_index = index of samples to use for testing\n",
    "    # Pick out samples (for training and testing):\n",
    "    X_train, X_test = X_temp[train_index], X_temp[test_index]\n",
    "    y_train, y_test = pressure[train_index], pressure[test_index]\n",
    "    # Fit a new model with the training set:\n",
    "    model = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    # Predict y for the test set:\n",
    "    y_hat = model.predict(X_test)\n",
    "    # Compare the predicted y values in the test set with the measured ones:\n",
    "    error.append((y_test - y_hat) ** 2)\n",
    "rmsecv_1 = np.sqrt(np.mean(error))\n",
    "print(f\"RMSECV = {rmsecv_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 of LOOCV:\n",
    "\n",
    "# scikit-learn has a method for leave-one-out selection, and a method for\n",
    "# cross-validation. And these two can be combined:\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# Create \"empty\" model for fitting:\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "# Run cross-validation, where we select testing and training with LeaveOneOut:\n",
    "scores = cross_val_score(\n",
    "    model, X_temp, pressure, scoring=\"neg_mean_squared_error\", cv=LeaveOneOut()\n",
    ")\n",
    "rmsecv_2 = np.sqrt(np.mean(-scores))\n",
    "print(f\"RMSECV = {rmsecv_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 of LOOCV:\n",
    "\n",
    "# We calculate the H matrix and use that:\n",
    "# OBS! First, a detail that is easy to miss; The X used for H includes the column of ones!\n",
    "X_matrix = np.column_stack((np.ones_like(temperature), temperature))\n",
    "H = X_matrix @ np.linalg.pinv(X_matrix)\n",
    "hii = np.diagonal(H)\n",
    "residuals_loo = (pressure - pressure_hat) / (1 - hii)\n",
    "rmsecv_3 = np.sqrt(np.mean(residuals_loo**2))\n",
    "print(f\"RMSECV = {rmsecv_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c9794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
