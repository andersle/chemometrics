{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e2b5f9",
   "metadata": {},
   "source": [
    "# Partial least squares regression \"by hand\"\n",
    "\n",
    "Here, we will find the latent variables in partial least squares (PLS) regression by hand. As an\n",
    "example, we will consider the solubility data again but in reduced form:\n",
    "\n",
    "* We will only consider alcohols\n",
    "\n",
    "* We use only logP and the number of atoms in the molecule as our X\n",
    "\n",
    "* We use the molecular weight as our Y\n",
    "\n",
    "From the regression we did on the full data set, we know that logP is important for predicting the\n",
    "solubility and we also expect that the molecular weight is correlated with the number of atoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af7e74",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in the file [solubility_alc.csv](./solubility_alc.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d24b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import svd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61976b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"solubility_descriptors.csv.zip\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "xvars = [\"MolLogP\", \"nAtom\"]\n",
    "yvars = [\"Molecular Weight\"]\n",
    "\n",
    "data[\"nC + nH\"] = scale(data[\"nC\"]) + scale(data[\"nH\"])\n",
    "\n",
    "#xvars = [\"nAtom\", \"LabuteASA\",]# \"Polar Surface Area\"]\n",
    "xvars = [\"nC\", \"nH\",]# \"Polar Surface Area\"]\n",
    "#yvars = [\"Molecular Weight\"]\n",
    "yvars = [\"nC + nH\"]\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "\n",
    "y = scaler_y.fit_transform(data[yvars].to_numpy())\n",
    "X = scaler_x.fit_transform(data[xvars].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a6675",
   "metadata": {},
   "source": [
    "### Plotting X- and Y-data\n",
    "\n",
    "Before we start, let us first plot the X- and Y-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], y)\n",
    "ax.set(xlabel=xvars[0], ylabel=xvars[1], zlabel=yvars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a688800",
   "metadata": {},
   "source": [
    "## The PLS method\n",
    "\n",
    "In PLS, we are looking for scores $\\mathbf{t}$ (for X) so that the covariance with $\\mathbf{y}$\n",
    "is maximized. The main idea is that the scores should explain the variance in X and the\n",
    "covariance between X and y.\n",
    "\n",
    "The covariance can be calculated by $\\mathbf{t}^\\top \\mathbf{y}$.\n",
    "We shall see later how we can find the directions in X and Y so that the covariance is\n",
    "maximized, but let us assume that we have found them. In PLS, these directions\n",
    "are referred to as the *weights* $\\mathbf{w}_x$ (for X).\n",
    "We can use these *weights* to calculate the scores:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{t} = \\mathbf{X} \\mathbf{w}_x\n",
    "\\end{equation}\n",
    "\n",
    "Assume now that we are going to predict $\\mathbf{y}$ from some measured $\\mathbf{X}$. \n",
    "Ideally, we would have a perfect correlation between the scores so\n",
    "that $\\mathbf{y} = g \\mathbf{t}$ where $g$ is some number.\n",
    "But in general, this is not the case. Instead, we will approximate $\\mathbf{y}$ from $\\mathbf{t}$ using\n",
    "this relation. The least squares approximation for $g$ is,\n",
    "\n",
    "\\begin{equation}\n",
    "g = \\frac{\\mathbf{t}^\\top \\mathbf{y}}{\\mathbf{t}^\\top \\mathbf{t}},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "so that the y approximated from the X-scores are $\\hat{\\mathbf{y}} = g \\mathbf{t}$.\n",
    "We can rewrite this as,\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = g \\mathbf{t} = g \\mathbf{X} \\mathbf{w}_x,\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and comparing with a linear equation on the form $\\mathbf{y} = \\mathbf{X} \\mathbf{b}_\\text{PLS}$ with regression\n",
    "coefficients $b_\\text{PLS}$ we see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b}_\\text{PLS} = g \\mathbf{w}_x .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81cd53",
   "metadata": {},
   "source": [
    "## A short example\n",
    "\n",
    "We set set $\\mathbf{w}_x = (0.0, 1.0)^\\top$, then \n",
    "the product $\\mathbf{X} \\mathbf{w}_x$ will just pick out\n",
    "the second column of $\\mathbf{X}$. Then a plot of\n",
    "$\\mathbf{t}$ vs. $\\mathbf{y}$ will show how the number of carbon + hydrogen is correlated with the number of hydrogens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(X, y, wx):\n",
    "    \"\"\"Plot X and t vs. y (calculated using wx)\"\"\"\n",
    "    fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "    axes[0].scatter(X[:, 0], X[:, 1])\n",
    "    axes[0].set(xlabel=xvars[0], ylabel=xvars[1], title='X')\n",
    "\n",
    "    vecx = wx.flatten()\n",
    "    vecy = wy.flatten()\n",
    "    \n",
    "    axes[0].quiver(0, 0, vecx[0], vecx[1], color='black',\n",
    "                   angles='xy', scale_units='xy', scale=0.2, width=0.015)\n",
    "    \n",
    "    \n",
    "    t = X @ (wx / norm(wx)) \n",
    "    # Technical detail: we norm both wx and wy here to get similar reults from different methods\n",
    "    \n",
    "    cov = t.T @ y\n",
    "\n",
    "    axes[1].scatter(t[:, 0], y[:, 0])\n",
    "    axes[1].set(\n",
    "        xlabel='X-scores (t)',\n",
    "        ylabel='y',\n",
    "        title=f'Covariance: {cov[0][0]:.4g}')\n",
    "    sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights by hand:\n",
    "wx = np.array([0.0, 1.0])\n",
    "wx = wx.reshape(2, -1)  # Make it a column vector\n",
    "print(f\"wx.T = {wx.T}, shape of w: {wx.shape}\")\n",
    "\n",
    "make_plot(X, y, wx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78880d",
   "metadata": {},
   "source": [
    "In the plot above, the X-data is projected onto the black vector and this gives the X-scores.\n",
    "The rightmost plot shows the X-scores plotted against y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_coefficients(X, y, wx):\n",
    "    t = X @ wx\n",
    "    # Find g (approximate u from t):\n",
    "    g = t.T @ y / (t.T @ t)\n",
    "    # Find b:\n",
    "    B = g * wx\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, y, wx)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68425e3c",
   "metadata": {},
   "source": [
    "In the regression coefficients above, we see that the first row is just zero. Effectively this means that we\n",
    "are predicting y-variables using only the number of hydrogens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=f\"R² = {r2_score(y, y_hat):.3f}\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc471b3",
   "metadata": {},
   "source": [
    "## Maximizing the covariance\n",
    "\n",
    "The covariance between $\\mathbf{t}$ and $\\mathbf{y}$ is\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{t}^\\top \\mathbf{y} = (\\mathbf{X} \\mathbf{w}_x)^\\top (\\mathbf{y})\n",
    "= \\mathbf{w}_x^\\top \\mathbf{X}^\\top \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "We set $\\mathbf{v} = \\mathbf{X}^\\top \\mathbf{y}$. What vector $\\mathbf{w}_x$ will\n",
    "maximize the dot product $\\mathbf{w}_x^\\top \\mathbf{v}$? Well, if we make $\\mathbf{w}_x$ parallel\n",
    "to $\\mathbf{v}$, then the dot product will be maximized! So we can set: $\\mathbf{w}_x = \\lambda \\mathbf{v} =\n",
    "\\lambda \\mathbf{X}^\\top \\mathbf{y}$ for some number $\\lambda$. If we at the same time normalize $\\mathbf{w}_x$,\n",
    "then we don't need to find $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5455b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = X.T @ y\n",
    "wx /= norm(wx)\n",
    "make_plot(X, y, wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, y, wx)\n",
    "print(B)\n",
    "y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=f\"R² = {r2_score(y, y_hat):.3f}\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acdc76",
   "metadata": {},
   "source": [
    "## Comparing with `PLSRegression` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLSRegression(scale=False, n_components=2)\n",
    "model.fit(X, y)\n",
    "make_plot(X, y, model.x_weights_[:, 0].reshape(2, -1))\n",
    "make_plot(X, y, model.x_weights_[:, 1].reshape(2, -1))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(B / model.coef_)\n",
    "y_hat = model.predict(X)\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=f\"R² = {r2_score(y, y_hat):.3f}\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64ef5a",
   "metadata": {},
   "source": [
    "## Finding the next latent variables\n",
    "\n",
    "So far, we have found one latent variable. The strategy to find the next PLS components is\n",
    "to essentially repeat the process, but we first \"subtract\" the latent variable we just found\n",
    "from $\\mathbf{X}^T \\mathbf{y}$. This is referred to as deflation.\n",
    "In addition, we make sure that the next latent variable\n",
    "we find will have scores orthogonal to the previous one. A consequence of this is\n",
    "that loadings and scores we find in the next steps of the method do not\n",
    "directly operate on $\\mathbf{X}$\n",
    "and $\\mathbf{Y}$, but on the deflated versions of these matrices.\n",
    "\n",
    "It would be nice to have loadings that we could apply directly \n",
    "to $\\mathbf{X}$ and $\\mathbf{Y}$ (and not the deflated version).\n",
    "PLS methods will typically calculate these as well, and in `sklearn`, they\n",
    "are called *rotations*. We will use these later for interpretation of correlations between\n",
    "variables. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first direction:\n",
    "wx1 = X.T @ y\n",
    "wx1 /= norm(wx1)\n",
    "print(wx1 / model.x_weights_[:, 0].reshape(-2, 1))\n",
    "\n",
    "# Calculate scores:\n",
    "t1 = X @ wx1\n",
    "# Calculate the predicted y:\n",
    "g = t1.T @ y / (t1.T @ t1)\n",
    "y_hat = g * t1\n",
    "# Subtract the predicted y:\n",
    "y2 = y - y_hat\n",
    "# Calculate loadings to remove the part of\n",
    "# X we have described with t:\n",
    "p = (X.T @ t1) / (t1.T @ t1)\n",
    "X2 = X - t1 @ p.T\n",
    "\n",
    "# Find next direction:\n",
    "wx2 = X2.T @ y2\n",
    "wx2 /= norm(wx2)\n",
    "print(wx2 / model.x_weights_[:, 1].reshape(-2, 1))\n",
    "make_plot(X, y, wx1)\n",
    "make_plot(X2, y2, wx2)\n",
    "# Convert the next direction to rotations:\n",
    "r2 = wx2 - wx1 @ p.T @ wx2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
