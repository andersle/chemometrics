{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY314,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166dea2",
   "metadata": {},
   "source": [
    "# Solution to Exercise set 5\n",
    "\n",
    "The main goals of this exercise are to use PCA\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Use PCA to find interesting variables.\n",
    "- Find outliers in connection with least squares.\n",
    "- Run agglomerative clustering and interpret a dendrogram.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "* [5.2(b)](#5.2(b)) and [5.2(c)](#5.2(c)): To show that you find outliers in connection with least squares.\n",
    "* [5.3(a)](#5.3(a)) and [5.3(b)](#5.3(b)): To show that you can run agglomerative clustering, make a dendrogram and interpret it.\n",
    "\n",
    "**Files required for this exercise:**\n",
    "* [Exercise 5.1](#Exercise-5.1): [ovo.csv](ovo.csv)\n",
    "* [Exercise 5.2](#Exercise-5.2): [forbes.csv](forbes.csv)\n",
    "* [Exercise 5.3](#Exercise-5.3): [zoo.csv](zoo.csv)\n",
    "\n",
    "Please ensure that these files are saved in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b3382",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "[Schummer *et al.*](https://doi.org/10.1016/S0378-1119(99)00342-X) used microarray technology to analyse the expression of 1536 genes in ovarian cancer and non-cancer tissues. Their primary objective was to identify differentially expressed genes in ovarian cancer versus non-cancer tissues to discover genes with diagnostic potential.\n",
    "\n",
    "The data file [`ovo.csv`](ovo.csv) contains numerical gene expressions (for 1536 genes) for 54 tissue samples. Each column corresponds to a specific gene, named `X.1`, `X.2`, and so on. Each tissue sample has been classified as non-cancer (`N`) or cancer (`C`) tissue, and these labels can be found in the column `class`. The raw data has been preprocessed by centring each gene expression so that no further preprocessing is needed. The raw data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a776ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "data_ovo = pd.read_csv(\"ovo.csv\")\n",
    "classes = data_ovo[\"class\"]  # Classification of samples.\n",
    "X_ovo = data_ovo.filter(like=\"X.\", axis=1)  # Gene expressions for samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bca546",
   "metadata": {},
   "source": [
    "### 5.1(a)\n",
    "\n",
    "**Task: Explore the raw data. Do you find genes that appear to show significant differences in expression between non-cancer and cancer tissue?**\n",
    "\n",
    "**Hint:** You can, for instance, explore the data by running a principal component analysis. If you want to avoid making the PCA plots from scratch, try the [yellowbrick library](https://www.scikit-yb.org/en/latest/api/features/pca.html) to visualise the scores and loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 1], hue=\"class\", ax=axes[0]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 2], hue=\"class\", ax=axes[1]\n",
    ")\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "axes[0].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "axes[1].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC3 ({percent[2]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "    ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1dca2",
   "metadata": {},
   "source": [
    "The PC1 scores separate the two tissue types. To understand what variables are important for this separation, we inspect the loadings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "\n",
    "\n",
    "loadings = pca.components_.T\n",
    "pc1_loadings = loadings[:, 0]\n",
    "pc2_loadings = loadings[:, 1]\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "ax.set(\n",
    "    xlabel=f\"Loadings, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Loadings, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "# Get the 10 largest loadings along PC1 for highlighting:\n",
    "highlighted_indices = np.argsort(abs(pc1_loadings))[-10:]\n",
    "print(f\"10 largest along PC1: {highlighted_indices}\")\n",
    "\n",
    "for i, (xi, yi) in enumerate(zip(pc1_loadings, pc2_loadings)):\n",
    "    if i in highlighted_indices:\n",
    "        txt = ax.text(xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\")\n",
    "        txt.set_path_effects(\n",
    "            [\n",
    "                pe.withStroke(linewidth=1.5, foreground=\"yellow\"),\n",
    "                pe.Normal(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        txt = ax.text(\n",
    "            xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\", color=\"0.7\"\n",
    "        )\n",
    "\n",
    "\n",
    "ax.set_xlim(-0.15, 0.15)\n",
    "ax.set_ylim(-0.15, 0.15)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8c09f",
   "metadata": {},
   "source": [
    "Let us check two of the highlighted genes by creating a scatterplot of the original samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5226d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene1 = 1490\n",
    "gene2 = 92\n",
    "grid = sns.jointplot(\n",
    "    data=data_ovo,\n",
    "    x=f\"X.{gene1 + 1}\",\n",
    "    y=f\"X.{gene2 + 1}\",\n",
    "    hue=\"class\",\n",
    ")\n",
    "ax = grid.fig.axes[0]\n",
    "ax.set_xlabel(f\"Gene expression for X.{gene1 + 1}\")\n",
    "ax.set_ylabel(f\"Gene expression for X.{gene2 + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076b442",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(a): Did you find any promising genes?\n",
    "\n",
    "Yes, genes with the highest absolute loadings on Principal Component 1 (PC1) appear promising for distinguishing between the samples. For example, number (counting from zero) 1490 and number 92 as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750c157",
   "metadata": {},
   "source": [
    "## Exercise 5.2\n",
    "[Forbes](https://doi.org/10.1017/S0080456800032075) investigated the\n",
    "relationship between the boiling point of water and the atmospheric pressure, and collected data in the Alps and Scotland. Forbes' goal was to estimate altitudes from the boiling point alone.\n",
    "\n",
    "We will use Forbes' data to make a linear model for predicting the atmospheric pressure, and we will investigate if there are any outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a7a05",
   "metadata": {},
   "source": [
    "### 5.2(a)\n",
    "\n",
    "**Task: Create a linear model that predicts the atmospheric pressure\n",
    "from the boiling point\n",
    "with `statsmodels`. Plot your model together with the raw data, and plot the residuals. Do you have\n",
    "any comments about the residuals?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fdf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the code above fails because you do not have statsmodels installed,\n",
    "# you can uncomment and run this:\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data can be loaded with:\n",
    "data_forbes = pd.read_csv(\"forbes.csv\")\n",
    "data_forbes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the least squares model with statsmodels:\n",
    "x = data_forbes[\"Temperature (F)\"]\n",
    "y = data_forbes[\"Pressure (inches Hg)\"]\n",
    "\n",
    "X = sm.add_constant(x)  # Make a matrix with a column of ones and then x.\n",
    "\n",
    "model = sm.OLS(y, X)  # Set up for OLS = Ordinary Least Squares.\n",
    "results = model.fit()  # Find parameters.\n",
    "y_hat = results.predict(X)  # Use the model to predict y_hat.\n",
    "\n",
    "# Print a small summary to show R² and the coefficients:\n",
    "print(results.summary(slim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbde6ed",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in the [Appendix](#Appendix:-The-summary-results-from-statsmodels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2317b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].scatter(x, y, label=\"Raw data\")\n",
    "axes[0].set(xlabel=\"Temperature (F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "axes[0].plot(\n",
    "    x,\n",
    "    y_hat,\n",
    "    label=f'Model: y = {results.params[\"const\"]:.3g} + {results.params[\"Temperature (F)\"]:.3g}x',\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_hat, y - y_hat)\n",
    "axes[1].set(xlabel=\"ŷ\", ylabel=\"y-ŷ\")\n",
    "axes[1].axhline(y=0, ls=\":\", color=\"k\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf872596",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(a): Do you have any comments about the residuals?\n",
    "\n",
    "There seems to be some kind of trend in the residuals, and one of the points is far away from the others.\n",
    "It could be that this is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac28d6",
   "metadata": {},
   "source": [
    "### 5.2(b)\n",
    "\n",
    "**Task: Obtain different outlier measures and plot them. Do you see any potential outliers?**\n",
    "\n",
    "**Hint**: Calculate [influence/outlier measures with statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.OLSInfluence.html#statsmodels.stats.outliers_influence.OLSInfluence) and plot the following:\n",
    "\n",
    "* (i) the studentised residuals,\n",
    "* (ii) the leverage ($h_{ii}$ from the $\\mathbf{H}$-matrix),\n",
    "* (iii) the Cook's distance, and\n",
    "* (iv) the [influence plot](https://www.statsmodels.org/dev/generated/statsmodels.graphics.regressionplots.influence_plot.html).\n",
    "\n",
    "You can find example code for doing this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The influence measures can be computed with:\n",
    "influence = results.get_influence()\n",
    "\n",
    "# The measures can be converted to a pandas data frame with:\n",
    "influence_table = influence.summary_frame()\n",
    "\n",
    "# And they can be accessed as follows:\n",
    "# (i) studentised residuals:\n",
    "studentised_residuals = influence_table[\"student_resid\"]\n",
    "\n",
    "# (ii) the leverage:\n",
    "hii = influence_table[\"hat_diag\"]\n",
    "\n",
    "# (iii) Cook's distance\n",
    "cooks_distance = influence_table[\"cooks_d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9, 3))\n",
    "\n",
    "axes[0].scatter(y_hat, studentised_residuals)\n",
    "axes[0].axhline(y=-2, ls=\":\", color=\"k\")\n",
    "axes[0].axhline(y=2, ls=\":\", color=\"k\")\n",
    "axes[0].set(xlabel=\"ŷ\", ylabel=\"Studentised residuals\")\n",
    "axes[0].set_title(\"Residuals\", loc=\"left\")\n",
    "axes[0].set_ylim(-4.5, 4.5)\n",
    "\n",
    "xpos = np.arange(len(hii))\n",
    "axes[1].bar(xpos, hii)\n",
    "axes[1].set(xlabel=\"Observation no.\", ylabel=\"$h_{ii}$\")\n",
    "axes[1].set_title(\"Influence\", loc=\"left\")\n",
    "\n",
    "axes[2].bar(xpos, cooks_distance)\n",
    "axes[2].set(xlabel=\"Observation no.\", ylabel=\"$D_{i}$\")\n",
    "axes[2].set_title(\"Cook's distance\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The influence plot, part (iv) can be created with:\n",
    "fig = influence.plot_influence()\n",
    "ax = fig.get_axes()[0]\n",
    "ax.set_ylim(-1.3, 3.5)\n",
    "ax.set_title(\"\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f955be",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(b): Do you see any potential outliers?\n",
    "\n",
    "Yes, one point is a bit suspicious: observation no. 11 (counting from zero) has a large studentised residual and a large influence (Cook's distance). This could be a potential outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a7e7c",
   "metadata": {},
   "source": [
    "### 5.2(c)\n",
    "\n",
    "**Task: Run a hypothesis test for outliers. Are any points marked as outliers?**\n",
    "\n",
    "**Hint:** You use the [outlier test](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.outlier_test.html)\n",
    "method from `statsmodels` to run the hypothesis test (see the code in the cell below).\n",
    "\n",
    "This test outputs:\n",
    "* `student_resid`: The studentised residuals.\n",
    "* `unadj_p`: The unadjusted p-value for the hypothesis test that the expected value of the studentized residual for point *i* is zero, under the null hypothesis that the point is not an outlier.\n",
    "* `bonf(p)`: A Bonferroni corrected p-value, which adjusts for the increased risk of Type I errors (mistaken rejection of a true null hypothesis) due to multiple comparisons.\n",
    "\n",
    "The `outlier_test` method tests the null hypothesis that each point is not an outlier by considering if its studentized residual is significantly different from zero. Since we perform this test *N* times for *N* points, the risk of incorrectly labelling at least one point as an outlier (Type I error) increases. To mitigate this, `outlier_test` will apply a [correction](https://en.wikipedia.org/wiki/Bonferroni_correction), and we should base our decisions on these corrected p-values.\n",
    "\n",
    "You can identify a point as an outlier if the corrected p-value is smaller than the significance level, set to `alpha = 0.05` by default in the `outlier_test()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c711584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you run the hypothesis test:\n",
    "test = results.outlier_test()\n",
    "test[test[\"bonf(p)\"] < 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87cec0",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(c): Were any points identified as outliers?\n",
    "\n",
    "Yes, point no. 11 is identified as an outlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb7f50",
   "metadata": {},
   "source": [
    "### 5.2(d)\n",
    "\n",
    "**Task: You should have found one outlier in the previous problem. Remake the model without this point and\n",
    "compare it with the model you made in part [5.2(a)](#5.2(a)). Did the model change substantially?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the suspicious point:\n",
    "x_new = x.drop(11)\n",
    "y_new = y.drop(11)\n",
    "X_new = sm.add_constant(x_new)\n",
    "\n",
    "# And remake the model:\n",
    "new_model = sm.OLS(y_new, X_new).fit()\n",
    "\n",
    "y_hat_new = new_model.predict(X_new)\n",
    "\n",
    "print(new_model.summary(slim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c43f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].scatter(x, y, label=\"Raw data\")\n",
    "axes[0].set(xlabel=\"Temperature (F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "axes[0].plot(\n",
    "    x,\n",
    "    y_hat,\n",
    "    label=f'Model 1: y = {results.params[\"const\"]:.3g} + {results.params[\"Temperature (F)\"]:.3g}x',\n",
    ")\n",
    "axes[0].plot(\n",
    "    x_new,\n",
    "    y_hat_new,\n",
    "    label=f'Model 2: y = {new_model.params[\"const\"]:.3g} + {new_model.params[\"Temperature (F)\"]:.3g}x',\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_hat, y - y_hat, label=\"Model 1\")\n",
    "axes[1].scatter(y_hat_new, y_new - y_hat_new, label=\"Model 2\")\n",
    "axes[1].set(xlabel=\"ŷ\", ylabel=\"y-ŷ\")\n",
    "axes[1].axhline(y=0, ls=\":\", color=\"k\")\n",
    "axes[1].legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fa200",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(d): Did removing the outlier change the model substantially?\n",
    "\n",
    "Removing the point does not seem to change the model a lot. The calculated Cook's distance is not a lot\n",
    "bigger for this point, compared to some of the other points. So in this case, the outlier was not so\n",
    "important for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a006e5",
   "metadata": {},
   "source": [
    "## Exercise 5.3\n",
    "The data file [zoo.csv](zoo.csv) contains some data on different animals:\n",
    "\n",
    "| Column | Description |\n",
    "| :---  | :--- |\n",
    "| `animal name` | The name for the animal (e.g., lion, penguin). |\n",
    "| `hair` | Does the animal have hair or fur? (0=no/1=yes) |\n",
    "| `feathers` | Does the animal have feathers? (0=no/1=yes) |\n",
    "| `eggs` | Does the animal lay eggs? (0=no/1=yes) |\n",
    "| `milk` | Does the animal provide milk for its young? (0=no/1=yes) |\n",
    "| `airborne` | Does the animal fly? (0=no/1=yes) |\n",
    "| `aquatic` | Does the animal live in or depend on water? (0=no/1=yes) |\n",
    "| `predator` | Does the animal hunt other animals? (0=no/1=yes) |\n",
    "| `toothed` | Does the animal have teeth? (0=no/1=yes) |\n",
    "| `backbone` | Does the animal have a spine? (0=no/1=yes) |\n",
    "| `breathes` | Does it breathe air (using lungs)? (0=no/1=yes) |\n",
    "| `venomous` | Does the animal produce toxins or venom? (0=no/1=yes) |\n",
    "| `fins` | Does the animal have fins for swimming? (0=no/1=yes) |\n",
    "| `legs` | The number of legs (normalised to the range 0 to 1). |\n",
    "| `tail` | Does the animal have a tail? (0=no/1=yes) |\n",
    "| `domestic` | Is it commonly domesticated by humans? (0=no/1=yes)|\n",
    "| `catsize` | Is the animal larger than a housecat? (0=no/1=yes) |\n",
    "\n",
    "\n",
    "We will use clustering on this data set to see if we can group animals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d39bf0",
   "metadata": {},
   "source": [
    "### 5.3(a)\n",
    "\n",
    "**Task: Run the code below to perform [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for the data in [zoo.csv](zoo.csv).**\n",
    "\n",
    "\n",
    "**Note:** As you may have noticed, we have many binary variables and one numerical variable (`legs`). Mixing variable types can be problematic when we are calculating a distance. We will ignore the potential issues related to this in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad56f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib notebook\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data:\n",
    "data_zoo = pd.read_csv(\"zoo.csv\")\n",
    "data_zoo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some variables\n",
    "animal_names = data_zoo[\"animal name\"].values\n",
    "variables = [i for i in data_zoo.columns if i not in (\"animal name\",)]\n",
    "X_zoo = data_zoo[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clustering class:\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clustering:\n",
    "model0 = AgglomerativeClustering(\n",
    "    n_clusters=1,\n",
    "    linkage=\"average\",\n",
    "    metric=\"sokalmichener\",\n",
    "    compute_distances=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Here:\n",
    "# n_clusters: Stop the clustering when n_clusters are found. Here it is set to 1, meaning that we will stop\n",
    "# when there is one cluster.\n",
    "# linkage: Selects the method for computing distances between clusters. Here it is set to average, meaning that\n",
    "# it calculates the average distance between the points in the clusters.\n",
    "# metric: Selects how the distances are calculated. Here \"sokalmichener\" selects a Sokal-Michener distance that takes\n",
    "# both matches and dissimilarities into account. We are not using an Euclidean distance since we have\n",
    "# almost exclusively boolean variables (0/1).\n",
    "# compute_distances: Set to True, meaning that distances (for visualisation in a dendrogram) are computed.\n",
    "\n",
    "# Run the clustering:\n",
    "model0.fit(X_zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f75d6a",
   "metadata": {},
   "source": [
    "**Note:** You can select different options for the `linkage` and the `metric` to see how this influences the results (the dendrogram produced in the next problem). Please see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723328a",
   "metadata": {},
   "source": [
    "### 5.3(b)\n",
    "\n",
    "**Task: Run the code below to create the dendrogram of the clustering performed above. Then consider the following:**\n",
    "1. What animal is \"human\" most similar to?\n",
    "2. What animal is \"platypus\" most similar to?\n",
    "3. What animal is least similar to any of the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05154e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the dendrogram, use this code:\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # Create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19255853",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, figsize=(8, 4))\n",
    "plot_dendrogram(\n",
    "    model0,\n",
    "    truncate_mode=\"level\",\n",
    "    labels=animal_names,\n",
    "    ax=axes,\n",
    "    p=100,\n",
    "    leaf_rotation=90,\n",
    ")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cec7bd",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(b):\n",
    "\n",
    "**Note:** Your answer to these might depend on the selected `linkage` and `metric`.\n",
    "\n",
    "1. What animal is \"human\" most similar to? \n",
    "   - The human is merged first with the bear. In hierarchical clustering, the earliest\n",
    "     merges represent the smallest distances, indicating that the human is more similar to\n",
    "     the bear than to any other animal\n",
    "2. What animal is \"platypus\" most similar to?\n",
    "   - The platypus merges with a cluster of mammals (including the fruitbat and elephant).\n",
    "     This indicates that despite being an egg-layer, it shares mammalian traits (like milk and hair)\n",
    "     make it more similar to the mammals\n",
    "3. What animal is least similar to any of the others?\n",
    "   - The honeybee is the animal that is merged last with the other animals,\n",
    "     indicating that it is different from the rest (there are no other insects in this data set)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
