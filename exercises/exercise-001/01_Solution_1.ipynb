{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Solution to Exercise set 1: Least squares regression\n",
    "\n",
    "This exercise focuses on practical applications of least squares regression with Python. You will learn how to apply least squares regression to fit models of different kinds and how to evaluate the results.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Apply ordinary least squares regression to fit a linear model to data, using [NumPy](https://numpy.org/), [SciPy](https://scipy.org/), [scikit-learn](https://scikit-learn.org/), and [statsmodels](https://www.statsmodels.org/).\n",
    "* Compare the quality of different regression models by inspecting their residuals.\n",
    "* Apply weighting to data points in least squares regression.\n",
    "* Estimate errors for coefficients by obtaining their confidence intervals.\n",
    "* Use non-linear least squares regression.\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [1.1(c)](#1.1(c)): Ordinary least squares regression and comparison of quality by residuals.\n",
    "- [1.3(b)](#1.3(b)): Non-linear least squares regression.\n",
    "\n",
    "**Files required for this exercise:**\n",
    "* For [Exercise 1.1](#Exercise-1.1:-Polynomial-regression-with-least-squares): [temperature.csv](temperature.csv)\n",
    "* For [Exercise 1.2](#Exercise-1.2:-Weighted-least-squares): [erdinger.csv](erdinger.csv)\n",
    "Please ensure that these files are saved in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1759",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Polynomial regression with least squares\n",
    "\n",
    "The temperature (°C) is measured continuously over time at a high-altitude\n",
    "in the atmosphere using a\n",
    "weather balloon. Every hour, a measurement is made and sent to an on-board computer.\n",
    "The measurements are \n",
    "shown in Figure 1 and can be found in the [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) file [temperature.csv](temperature.csv).\n",
    "\n",
    "<figure>\n",
    "<img src=\"Fig_1_1.png\" width=\"50%\">\n",
    "<figcaption><p style='text-align: center;'><b>Figure 1:</b> Measured temperature as a function of time.</p></figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75853a",
   "metadata": {},
   "source": [
    "### 1.1(a)\n",
    "\n",
    "To model the temperature ($y$) as a function of time ($x$), we choose a second-order polynomial:\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2.\n",
    "\\end{equation}\n",
    "\n",
    "Explain how you can formulate this in a form suitable for least-squares regression,\n",
    "$\\mathbf{y} = \\mathbf{X} \\mathbf{b}$. That is, **what do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ and the matrix $\\mathbf{X}$ contain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418ebb",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(a): What do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ and the matrix $\\mathbf{X}$ contain?\n",
    "\n",
    "First, we rewrite the given model as a linear model by introducing\n",
    "the variables $z_1 = x$ and $z_2 = x^2$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = b_0 + b_1 x + b_2 x^2 = b_0 + b_1 z_1 + b_2 z_2 .\n",
    "\\end{equation*}\n",
    "\n",
    "We then let $y_{i}$ be the result of\n",
    "measurement no. $i$ and\n",
    "$z_{ij}$ the value of variable $z_j$ in the same measurement. Our model for this particular\n",
    "point is then:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i = b_0 + b_1 z_{i1} + b_2 z_{i2} .\n",
    "\\end{equation*}\n",
    "\n",
    "In matrix form (assuming we have $n$ measurements), we get,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 & z_{11} & z_{12} \\\\\n",
    "1 & z_{21} & z_{22} \\\\\n",
    "\\vdots & \\vdots & \\vdots  \\\\\n",
    "1 & z_{n1} & z_{n2} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "We can also write the design matrix $\\mathbf{X}$ using the original variables,\n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & z_{11} & z_{12} \\\\\n",
    "1 & z_{21} & z_{22} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & z_{n1} & z_{n2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^2 \\\\\n",
    "1 & x_{2} & x_{2}^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{n} & x_{n}^2 \n",
    "\\end{bmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "Thus we have the following:\n",
    "\n",
    "1. $\\mathbf{y}$ contains the measured $y_i$ values and $\\mathbf{b}$ contains the parameters $b_0$, $b_1$, and $b_2$.\n",
    "2. $\\mathbf{X}$ contains the variables as columns and a column of $1$'s\n",
    "  to account for the constant term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e143df",
   "metadata": {},
   "source": [
    "### 1.1(b)\n",
    "\n",
    "Fit a second-order polynomial model,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "to the given data by making use of [numpy.polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). \n",
    "\n",
    "1. Obtain the parameters $b_0$, $b_1$, and $b_2$.\n",
    "2. Plot your model: Create a scatter plot of the original data points and overlay the fitted quadratic curve to visualise the model's fit.\n",
    "3. Calculate the [residuals](https://en.wikipedia.org/wiki/Errors_and_residuals) and create a scatter plot of the residuals against the fitted values. \n",
    "4. Based on your results, how do you assess your model? Please see [What is Considered a Good vs. Bad Residual Plot?](https://www.statology.org/good-vs-bad-residual-plot/) for a short explanation of what to look for in the residual plot.\n",
    "\n",
    "Below, you will find some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data:\n",
    "data = pd.read_csv(\"temperature.csv\")\n",
    "# Show the first few rows of the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452d298",
   "metadata": {},
   "source": [
    "To fit a polynomial to your data, you have several options. Here are three popular choices:\n",
    "\n",
    "1. [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). This is the simplest option, suitable for most basic polynomial fitting tasks. It uses a least squares approach to fit a polynomial of a given degree to your data. \n",
    "2. [Ordinary least squares (OLS)](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html) from [statsmodels](https://www.statsmodels.org): This method provides more detailed results than polyfit, including error estimates for the coefficients. It is a good choice if you need more information about the fit. \n",
    "3. [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [scikit-learn](https://scikit-learn.org/): This is a more general approach that can be used for a variety of regression tasks, including polynomial fitting. It is particularly useful if you want to combine polynomial fitting with other methods, such as cross-validation.\n",
    "\n",
    "For options 2 and 3, we have to \"construct\" the $\\mathbf{X}$-matrix (see [1.1(a)](#1.1(a))), while `polyfit` does this automatically. We select the simplest option and use `polyfit` in this exercise.\n",
    "\n",
    "Here is an example for finding a first-order polynomial to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data:\n",
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "\n",
    "# Fit a first-order polynomial:\n",
    "param = np.polyfit(x, y, deg=1)  \n",
    "\n",
    "# Show the parameters found (stored in `param`):\n",
    "equation = f\"y = {param[0]:.3f}x + {param[1]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval.\n",
    "# This will evaluate the polynomial using the parameters\n",
    "# we found at each x-value:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# Compute the residuals:\n",
    "residual = y - y_hat\n",
    "\n",
    "# Plot the fitted polynomial and residuals:\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True,  # Remove some whitespace in the figure\n",
    "    ncols=2,  # Create two columns\n",
    "    figsize=(8, 4)  # Adjust the size of the plot\n",
    ")\n",
    "# Plot the raw data:\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "\n",
    "# Plot the fitted curve:\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "\n",
    "# Show a legend:\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the residuals:\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "\n",
    "# Remove some of the spines (top and right lines around the plot):\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe755f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for fitting the second-order model, plotting it, and the residuals\n",
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "param = np.polyfit(x, y, deg=2)  # Selects a second order polynomial\n",
    "\n",
    "# param now contains the parameters:\n",
    "equation = f\"y = {param[0]:.3f}x² + {param[1]:.3f}x + {param[2]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# And we can find the residuals/errors\n",
    "residual = y - y_hat\n",
    "\n",
    "ssr = np.sum(residual**2)  # Sum of squared residuals\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "ax1.legend()\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "ax2.set_title(f\"SSR = {ssr:.3f}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c95ca",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(b): What are the coefficients of the second-order polynomial and how do you assess (based on the two plots you made) your model?\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table A:</b> Coefficients for the quadratic polynomial</p>  \n",
    "\n",
    "|  Coefficient | Value       |\n",
    "|--------------|-------------|\n",
    "| $b_0$        | 12.96       |\n",
    "| $b_1$        | 0.0091      |\n",
    "| $b_2$        | -0.012      |\n",
    "\n",
    "\n",
    "The quadratic curve captures the general downward trend but fails to account for the systematic \"waves\" or oscillations in the temperature data. The residuals are not randomly distributed around zero and they show a trend. In regression analysis, such a trend indicates that the model is \"underfitting\". This suggests that a higher-order polynomial or a different model type might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee026a2",
   "metadata": {},
   "source": [
    "### 1.1(c)\n",
    "\n",
    "In this problem, you will explore how the choice of polynomial degree affects the model's ability to fit the temperature data.\n",
    "\n",
    "1. Extend your code from [1.1(b)](#1.1(b)) to fit polynomial models of degrees 1 to 5 to the temperature data.\n",
    "2. Create a single plot displaying the raw data as a scatter plot and overlay the fitted curves for all five models as lines.\n",
    "3. Plot the residuals for each model in separate scatter plots.\n",
    "\n",
    "Which polynomial degree best models temperature as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "\n",
    "\n",
    "order = [1, 2, 3, 4, 5]\n",
    "models = [np.polyfit(x, y, deg=orderi) for orderi in order]\n",
    "\n",
    "fig1, ax1 = plt.subplots(constrained_layout=True)\n",
    "ax1.scatter(x, y, label=\"Raw data\", color=\"black\")\n",
    "\n",
    "fig2, axes2 = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(order),\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=(len(order) * 4, 4),\n",
    ")\n",
    "fig2.suptitle(\"Residuals:\")\n",
    "\n",
    "\n",
    "ssr_all = []\n",
    "for i, orderi in enumerate(order):\n",
    "    param = np.polyfit(x, y, deg=orderi)\n",
    "    y_hat = np.polyval(param, x)\n",
    "    residual = y - y_hat\n",
    "    ssr = np.sum(residual**2)\n",
    "    ssr_all.append(ssr)\n",
    "\n",
    "    ax1.plot(x, y_hat, label=f\"Order: {orderi}\")\n",
    "    axes2[i].scatter(y_hat, residual)\n",
    "    if i == 0:\n",
    "        axes2[i].set(ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "    axes2[i].set(xlabel=\"Predicted by model ($ŷ_i$)\")\n",
    "    axes2[i].set_title(f\"Order: {orderi}, SSR = {ssr:.3f}\", loc=\"left\")\n",
    "\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.legend()\n",
    "\n",
    "sns.despine(fig=fig1)\n",
    "sns.despine(fig=fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5874b4",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(c): Which polynomial degree best models temperature as a function of time?\n",
    "\n",
    "The residual plots for the third, fourth, and fifth-order models show no clear patterns or trends, indicating that these models are capturing the underlying structure of the data well. These polynomial degrees also fit the raw data well (see the first figure). While higher-order models (degree 4 and 5) provide a marginally better fit to the raw data (lower SSR values), the improvement is small. \n",
    "\n",
    "As there is no compelling evidence to prefer a higher-degree model, we follow the principle of parsimony (Occam's Razor) and recommend the third-degree polynomial. The third-degree polynomial captures the main features of the data without unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863e1c9",
   "metadata": {},
   "source": [
    "### 1.1(d)\n",
    "Obtain the [sum of squared residuals](https://en.wikipedia.org/wiki/Residual_sum_of_squares) for each polynomial you made in [1.1(c)](#1.1(c)) and plot this as a function of the\n",
    "polynomial degree. \n",
    "\n",
    "Use this plot to determine (from visual inspection) the best polynomial\n",
    "degree for modelling the temperature as a function of time. Does this agree with your\n",
    "assessment from [1,1(c)](#1.1(c))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56429fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data stored from 1.1(c):\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(order, ssr_all, ls=\"--\", marker=\"o\")\n",
    "ax.set_title(\"Sum of squared residuals\", loc=\"left\")\n",
    "ax.set_xticks(order)\n",
    "ax.set_xlabel(\"Polynomial degree\")\n",
    "ax.set_ylabel(\"Sum of squared residuals (°C)²\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ca394",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(d): What polynomial degree do you recommend?\n",
    "\n",
    "We observe a significant drop in the sum of squared residuals when going from a second to a third-order polynomial. After that, the decrease in the sum of squared residuals becomes negligible. This suggests that further increases in complexity (for instance, going to a 4th or 5th order polynomial) does not substantially improve the fit. This is consistent with our findings in 1.1(c), where we observed that a third-order polynomial captured the main features of the data without unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36ee05",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Weighted least squares\n",
    "\n",
    "In this exercise, we will use least-squares regression to investigate a real-world phenomenon: the decay of beer froth over time. The goal is to illustrate how regression can be used to extract meaningful physical quantities and quantify their associated uncertainties.\n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this [research on the decay of beer froth](https://doi.org/10.1088/0143-0807/23/1/304), and we will here reproduce the data analysis. In particular, we will use the reported raw data and carry out a weighted least squares regression. In addition, we will also obtain an error estimate (as a confidence interval) for the determined physical quantity.\n",
    "\n",
    "\n",
    "The file [erdinger.csv](erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights. \n",
    "\n",
    "\n",
    "**Please use [scikit-learn](https://scikit-learn.org/) and [statsmodels](https://www.statsmodels.org) for the fitting in this exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c00b5",
   "metadata": {},
   "source": [
    "### 1.2(a)\n",
    "Create a linear model (first-order polynomial) for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the [coefficient of determination ($R^2$)](https://en.wikipedia.org/wiki/Coefficient_of_determination), and plot\n",
    "the residuals. Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "**Note:** You do not need to write the code for this part, but ensure that you understand the implementation and the resulting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data:\n",
    "data = pd.read_csv(\"erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data as NumPy arrays:\n",
    "time = data[\"time\"].to_numpy()\n",
    "height = data[\"height\"].to_numpy()\n",
    "height_error = data[\"height-error\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for using scikit-learn:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# To fit a model with scikit-learn, we do the following:\n",
    "model1 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# We create an X-matrix for the fitting by reshaping 'time':\n",
    "X = time.reshape(-1, 1)\n",
    "model1.fit(X, height)\n",
    "\n",
    "# We can use the model for prediction by:\n",
    "y_hat_1 = model1.predict(X)\n",
    "\n",
    "# To calculate R²:\n",
    "r2_model1 = model1.score(X, height)\n",
    "# or:\n",
    "r2_model1 = r2_score(height, y_hat_1)\n",
    "\n",
    "# To calculate the mean squared error (MSE) for the model:\n",
    "mse_model1 = mean_squared_error(height, y_hat_1)\n",
    "# To summarise the model with a short text string:\n",
    "model1_txt = f\"y = {model1.coef_[0]:.3g}x + {model1.intercept_:.3g}\"\n",
    "model1_txt = f\"{model1_txt}\\n(R² = {r2_model1:.3g}, MSE = {mse_model1:.3g})\"\n",
    "print(model1_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ed443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results:\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "# Add the raw data with error bars:\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Show only the markers, no connecting lines\n",
    "    capsize=4,  # Adjust the width of the error bar caps\n",
    ")\n",
    "# Plot the fitted curve:\n",
    "ax1.plot(\n",
    "    time,\n",
    "    y_hat_1,\n",
    "    lw=3,\n",
    "    label=model1_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "# Plot the residuals:\n",
    "ax2.scatter(y_hat_1, height - y_hat_1)\n",
    "ax2.set(xlabel=\"Predicted by the model (ŷ)\", ylabel=\"Residuals (y - ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faead54",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(a): Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "The linear model captures most of the trend in the decreasing height as a function of time and R² is quite high (0.944). However, it fails to account for the curvature in the data, especially at earlier times. The residual plot shows a clear pattern (U-shape), and the presence of this pattern suggests that the linear model does not capture the relationship between time and height accurately. Furthermore, the linear model predicts a negative froth height at around 480 seconds, which is physically impossible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate to see where we get negative heights:\n",
    "X2 = np.arange(400, 500, 10).reshape(-1, 1)\n",
    "model1.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a6adb",
   "metadata": {},
   "source": [
    "### 1.2(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that this leads to\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter (characteristic time constant).\n",
    "We will assume that $h(0)$ is a known parameter, equal to the initial height of the froth.\n",
    "\n",
    "Show how you can transform the equation above into a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12feeb3",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(b):\n",
    "If we take the natural logarithm on both sides of the equation, we get,\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln \\left( \\frac{h(t)}{h(0)} \\right) = -\\frac{t}{\\tau} = -\\frac{1}{\\tau} \\times t .\n",
    "\\end{equation}\n",
    "\n",
    "Setting,\n",
    "\\begin{equation}\n",
    "y = \\ln \\left( \\frac{h(t)}{h(0)} \\right), \\quad x = t, \\quad b=-\\frac{1}{\\tau},\n",
    "\\end{equation}\n",
    " we get,\n",
    "\\begin{equation}\n",
    "\\underbrace{\\ln \\left( \\frac{h(t)}{h(0)} \\right)}_{y} = -\\frac{t}{\\tau} = \\underbrace{-\\frac{1}{\\tau}}_{b} \\times \\underbrace{t}_{x},\n",
    "\\end{equation}\n",
    "\n",
    "or $y = bx$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941180",
   "metadata": {},
   "source": [
    "### 1.2(c)\n",
    "Use the linear transformation you found in [1.2(b)](#1.2(b)) to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$.\n",
    "\n",
    "**Hint:** The equation, $y=bx$, above does not include the usual constant term.\n",
    "This will modify the least-squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept).\n",
    "To do the fitting without the intercept, set `fit_intercept=False` when creating the linear regression model:\n",
    "```python\n",
    "model2 = LinearRegression(fit_intercept=False)  # New model, without intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, transform y:\n",
    "y = np.log(height / height[0])\n",
    "X = time.reshape(-1, 1)\n",
    "\n",
    "# Create the new model, without intercept:\n",
    "model2 = LinearRegression(fit_intercept=False)\n",
    "model2.fit(X, y)\n",
    "\n",
    "# Calculate R²:\n",
    "r2_model2 = model2.score(X, y)\n",
    "\n",
    "# Convert predicted y back to heights:\n",
    "y_hat_2 = model2.predict(X)\n",
    "height_hat_2 = height[0] * np.exp(y_hat_2)\n",
    "\n",
    "# Calculate the mean squared error, based on the heights\n",
    "# (this is to compare with model 1)\n",
    "mse_model2 = mean_squared_error(height, height_hat_2)\n",
    "\n",
    "tau = -1.0 / model2.coef_[0]\n",
    "print(f\"τ = {tau:.4g} s\")\n",
    "\n",
    "model2_txt = f\"h(t) = h(0) exp(-t/{tau:4g})\"\n",
    "model2_txt = f\"{model2_txt}\\n(R² = {r2_model2:.3g}, MSE = {mse_model2:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d246fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    ")\n",
    "# Plot the new model:\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_2,\n",
    "    lw=3,\n",
    "    label=model2_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Show residuals in height for the new model:\n",
    "ax2.scatter(height_hat_2, height - height_hat_2)\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Residual, y - ŷ\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade452",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(c): What value did you get for $\\tau$?\n",
    "From the coefficient found in the least squares fit: $\\tau \\approx 290$. We see that the residuals are now all\n",
    "smaller in magnitude, but we are overestimating the height for a lot of the points. The residuals still show a slight curvature, suggesting that the exponential model might not be perfectly capturing the decay of the froth height.\n",
    "\n",
    "The R² value is now very close to 1, indicating that the exponential model explains a large proportion of the variance in the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae3b04",
   "metadata": {},
   "source": [
    "### 1.2(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276 \\pm 14$s, which is likely lower than the value you obtained in [1.2(c)](#1.2(c)).\n",
    "We will now attempt to reproduce the results of Leike by using weighted least squares regression. The motivation for this approach is that the measurement errors in the raw data are not constant. Weighted regression accounts for this by assigning more influence to data points with smaller uncertainties.\n",
    "\n",
    "To assign the weights ($w_i$) we can use $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting log-transformed values to $y = \\log (h(t) / h(0))$, and this will modify the errors.\n",
    "If you are familiar with [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that the error in $y$ ($\\sigma_y$) is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_y^2 = \\frac{\\sigma_h^2}{h^2} ,\n",
    "\\end{equation}\n",
    "\n",
    "which says that we can get the error in $y$ by dividing the measured error in the height ($\\sigma_h$) by the measured height ($h$).\n",
    "\n",
    "Do the following steps to perform the weighted least squares:\n",
    "\n",
    "1. Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "2. Calculate weights for your $y$ values as $1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "3. Perform a weighted least squares fitting (see the example below) using the calculated weights. Estimate $\\tau$, plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to run weighted least squares:\n",
    "\n",
    "# Set up the model:\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Create some weights (note: these are for illustration and not correct for 1.2(d))\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "\n",
    "# Handle potential division by zero by setting infinite weights to zero:\n",
    "weights[weights == float(\"inf\")] = 0\n",
    "\n",
    "# Perform the fit using the weights:\n",
    "model.fit(X, height, sample_weight=weights)\n",
    "\n",
    "# Calculate R² (ensuring the weights are considered in the calculation):\n",
    "r2 = model.score(X, height, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "\n",
    "# 1. Calculate errors for y:\n",
    "sigma_y_sq = height_error**2 / height**2\n",
    "\n",
    "# 2. Calculate weights for y:\n",
    "weights = 1.0 / sigma_y_sq\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "# 3. Do weighted least squares:\n",
    "model3.fit(X, y, sample_weight=weights)\n",
    "\n",
    "# Calculate R² (considering the weights).\n",
    "r2_model3 = model3.score(X, y, sample_weight=weights)\n",
    "y_hat_3 = model3.predict(X)\n",
    "\n",
    "# Recalculate the heights:\n",
    "height_hat_3 = height[0] * np.exp(y_hat_3)\n",
    "\n",
    "# Calculate MSE, based on heights. For this, we need the weights for the heights:\n",
    "weights_h = 1.0 / height_error**2\n",
    "weights_h[weights_h == float(\"inf\")] = 0\n",
    "# Normalize the weights (since scikit-learn is using normalized weights)\n",
    "weights_h /= sum(weights_h)\n",
    "\n",
    "mse_model3 = mean_squared_error(height, height_hat_3, sample_weight=weights_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_ = -1.0 / model3.coef_[0]\n",
    "print(f\"τ = {tau_:.4g} s\")\n",
    "\n",
    "model3_txt = f\"h(t) = h(0) exp(-t/{tau_:4g})\"\n",
    "model3_txt = f\"{model3_txt}\\n(R² = {r2_model3:.3g}, MSE = {mse_model3:.3g})\"\n",
    "print(model3_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",\n",
    "    capsize=4, \n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_3,\n",
    "    lw=3,\n",
    "    label=model3_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.scatter(height_hat_3, (height - height_hat_3) * np.sqrt(weights_h))\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Weighted residual, w × (y - ŷ)\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe90764",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(d): What value did you get for $\\tau$? How does it compare to Leike's result?\n",
    "With the weighted approach, we get $\\tau = 277$ s, which is very close to the $276$ s reported by Leike.\n",
    "The weighted residuals are smaller in magnitude, but there might still be a weak trend. The high R² value indicates that the model captures the general trend well.\n",
    "\n",
    "We will next quantify the uncertainty in our estimate of $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd117a24",
   "metadata": {},
   "source": [
    "### 1.2(e)\n",
    "We can use the measured errors to estimate the uncertainty in the $\\tau$ parameter. Adopt the example code below, using [statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/wls.html) to compute a 95% confidence interval for $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for obtaining a 95% confidence interval.\n",
    "\n",
    "# Use statsmodels:\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the data for statsmodels:\n",
    "X = time.reshape(-1, 1)\n",
    "\n",
    "# Obtain weights:\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "# Set infinite values to zero:\n",
    "weights[weights == float(\"inf\")] = 0\n",
    "\n",
    "# Create the model and fit it:\n",
    "model_wls = sm.WLS(height, X, weights=weights)\n",
    "results_wls = model_wls.fit()\n",
    "\n",
    "# Display the full statistical summary:\n",
    "# alpha=0.05 calculates a 95% confidence interval (100 * (1 - alpha)%).\n",
    "print(results_wls.summary(alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7668117",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in [Appendix B](#B.-The-summary-results-from-statsmodels). We only need this part:\n",
    "\n",
    "```code\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "x1             0.0452      0.015      2.997      0.010       0.013       0.077\n",
    "==============================================================================\n",
    "```\n",
    "\n",
    "where `coef` gives the fitted coefficient and the numbers below `[0.025      0.975]` is the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can get the confidence interval by:\n",
    "results_wls.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# We use the same X and we use the logarithmic y:\n",
    "X = time.reshape(-1, 1)\n",
    "y = np.log(height / height[0])\n",
    "\n",
    "# 1. Calculate errors for y (same as before):\n",
    "sigma_y_sq = height_error**2 / height**2\n",
    "\n",
    "# 2. Calculate weights for y (same as before):\n",
    "weights = 1.0 / sigma_y_sq\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "model_wls = sm.WLS(y, X, weights=weights)\n",
    "results_wls = model_wls.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the confidence interval we use:\n",
    "tau_statsmodels = -1.0 / results_wls.params[0]\n",
    "tau_ci = results_wls.conf_int()[0]\n",
    "lower_limit = min(-1.0 / tau_ci)\n",
    "upper_limit = max(-1.0 / tau_ci)\n",
    "print(f\"τ = {tau_statsmodels:.4g} s\")\n",
    "print(f\"Confidence interval: {lower_limit:.4g} to {upper_limit:.4g}\")\n",
    "\n",
    "# To get this as a ±\n",
    "uncertainty1 = upper_limit - tau_statsmodels\n",
    "uncertainty2 = tau_statsmodels - lower_limit\n",
    "uncertainty = max(uncertainty1, uncertainty2)\n",
    "print(f\"τ = {round(tau_statsmodels, 0):.4g} ± {round(uncertainty, 0):.4g} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707141e",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(e): What confidence interval did you get for $\\tau$?\n",
    "The confidence interval was 265.7 to 290.1 s. The uncertainty is 13 s, so τ = 277 ± 13 s. \n",
    "This compares well with the results of Leike: τ = 276 ± 14 s at 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be7fbe",
   "metadata": {},
   "source": [
    "## Exercise 1.3: Non-linear least squares\n",
    "\n",
    "In spectroscopy, it is often necessary to \"deconvolve\" a spectrum into a number of overlapping spectral peaks. The individual peaks can often be approximated as Gaussian functions of amplitude $\\beta$, peak wavelength $\\lambda$   and standard deviation $\\sigma$. Consequently, the total spectrum $S(\\lambda)$ may be written as, \n",
    "\n",
    "\\begin{equation}\n",
    "S(\\lambda) = \\sum_{i=1}^N \\beta_i \\exp \\left( -\\frac{(\\lambda - \\lambda_i)^2}{2\\sigma_i^2} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of overlapping peaks.\n",
    "\n",
    "In this exercise, we will use non-linear least squares to determine the parameters for a spectrum consisting of $N=3$ peaks. Specifically, we will use the parameters provided in Table 1 to generate a synthetic spectrum, add random noise, and then attempt to recover the original parameters through regression.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Parameters for spectral peaks</p>  \n",
    "\n",
    "|             | Peak 1   | Peak 2   | Peak 3   |\n",
    "|-------------|----------|----------|----------|\n",
    "| $\\beta_i$   | 0.2      | 0.4      | 0.3      |\n",
    "| $\\lambda_i$ | 4.0      | 5.5      | 7.2      |\n",
    "| $\\sigma_i$  | 0.5      | 0.8      | 0.9      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b013cf",
   "metadata": {},
   "source": [
    "### 1.3(a)\n",
    "\n",
    "We will first generate a synthetic spectrum using the parameters in Table 1 to use for the non-linear least-squares fitting. To create the noise-free absorption spectrum, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters from Table 1:\n",
    "betas = [0.2, 0.4, 0.3]\n",
    "lambdas = [4.0, 5.5, 7.2]\n",
    "sigmas = [0.5, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def gaussian(x, beta_i, lambda_i, sigma_i):\n",
    "    \"\"\"Evaluate a Gaussian function at points x.\"\"\"\n",
    "    return beta_i * np.exp(-((x - lambda_i) ** 2) / (2.0 * sigma_i ** 2))\n",
    "\n",
    "# Create the noise-free spectrum, sample it at 100 points between 0 and 10:\n",
    "x = np.linspace(0, 10, 100)\n",
    "spectrum_without_noise = np.zeros_like(x)\n",
    "\n",
    "# Sum the individual Gaussian peaks to create the synthetic spectrum:\n",
    "for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "    spectrum_without_noise += gaussian(x, beta_i, lambda_i, sigma_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fbc5f",
   "metadata": {},
   "source": [
    "Next, create a noisy spectrum by adding zero-mean Gaussian noise with a standard deviation $\\gamma=0.025$, using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.025\n",
    "\n",
    "# Since we will be using random numbers, we set the\n",
    "# seed to get reproducible numbers:\n",
    "np.random.seed(4175)\n",
    "\n",
    "# Generate Gaussian noise and add it to the original spectrum:\n",
    "noise = np.random.normal(loc=0, scale=gamma, size=spectrum_without_noise.shape)\n",
    "spectrum_with_noise = spectrum_without_noise + noise\n",
    "\n",
    "# Show the spectrum with and without noise:\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, spectrum_with_noise, label=\"With noise\")\n",
    "ax.plot(x, spectrum_without_noise, color=\"k\", label=\"Without noise\")\n",
    "ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c80022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will try different gamma values, we generate some\n",
    "# more signals with noise here:\n",
    "\n",
    "gammas = [0.025, 0.05, 0.075]\n",
    "\n",
    "spectra_with_noise = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    noise = np.random.normal(loc=0, scale=gamma, size=spectrum_without_noise.shape)\n",
    "    with_noise = spectrum_without_noise + noise\n",
    "    spectra_with_noise.append(with_noise)\n",
    "\n",
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "\n",
    "for ax, signal, gamma in zip(axes, spectra_with_noise, gammas):\n",
    "    ax.scatter(x, signal, label=\"With noise\")\n",
    "    ax.plot(x, spectrum_without_noise, color=\"k\", label=\"Without noise\")\n",
    "    ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "axes[0].legend(fontsize=\"xx-small\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e938f8",
   "metadata": {},
   "source": [
    "### 1.3(b)\n",
    "\n",
    "Use non-linear least squares to estimate the Gaussian parameters for the noisy spectrum you generated in [1.3(a)](#1.3(a)). Investigate how the solution depends on your initial guess and the noise level:\n",
    "\n",
    "- Initial guess: Compare \"good\" initial guesses (e.g., those from Table 1) with \"bad\" values (e.g. placing the peak centres far from their actual positions).\n",
    "- Noise level: Try running the fitting for a signal containing a higher level of noise, for instance, by setting $\\gamma=0.05$ in [1.3(a)](#1.3(a)).\n",
    "\n",
    "**Hint:** The code below outlines how to perform the fit using the noise-free spectrum.\n",
    "\n",
    "**Note:** We use [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) from [SciPy](https://scipy.org/) to run non-linear least squares. You can also use [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) from [SciPy](https://scipy.org/) for non-linear least squares: this is simpler and requires less setup, but you have less control over the optimisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the model we will fit:\n",
    "def model(x, params):\n",
    "    \"\"\"Calculate the total spectral signal using the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x: The independent variable (e.g., wavelength).\n",
    "        params: A flat list of parameters for N peaks.\n",
    "            The length must be a multiple of 3, ordered as:\n",
    "            [beta_1, lambda_1, sigma_1, beta_2, lambda_2, sigma_2, ...]\n",
    "\n",
    "    Returns:\n",
    "        The value of the function at each given x.\n",
    "    \"\"\"\n",
    "    betas = params[0::3]\n",
    "    lambdas = params[1::3]\n",
    "    sigmas = params[2::3]\n",
    "    \n",
    "    signal = np.zeros_like(x)\n",
    "\n",
    "    for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "        signal += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the objective function we will minimise:\n",
    "def objective(params, x, y):\n",
    "    \"\"\"Calculate sum of squared errors between the model and the data.\"\"\"\n",
    "    y_model = model(x, params)\n",
    "    residuals = y - y_model\n",
    "    # Return sum of squared errors:\n",
    "    return np.sum(residuals**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimisation:\n",
    "\n",
    "# Initial guess for the parameters:\n",
    "initial_guess = [\n",
    "    0.3, 4.0, 1.0,  # Peak 1\n",
    "    0.3, 6.0, 1.0,  # Peak 2\n",
    "    0.3, 8.0, 1.0,  # Peak 3\n",
    "]\n",
    "\n",
    "# Set up boundaries for the parameters, these\n",
    "# are in the form (min, max) for each parameter\n",
    "bounds = [\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 1\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 2\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 3\n",
    "]\n",
    "\n",
    "# Run the minimisation:\n",
    "result = minimize(\n",
    "    objective,\n",
    "    initial_guess,\n",
    "    args=(x, spectrum_without_noise),\n",
    "    bounds=bounds,\n",
    "    options={\n",
    "        \"maxiter\": 5000,\n",
    "    },\n",
    ")\n",
    "# Check if the optimisation converged successfully:\n",
    "print(result.message)\n",
    "\n",
    "# Extract the fitted parameters:\n",
    "fit_params = result.x\n",
    "print(f\"Fitted parameters: {fit_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate using the fitted parameters:\n",
    "y_model = model(x, fit_params) \n",
    "\n",
    "# Plot the fitted signal\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(x, spectrum_without_noise, label=\"True signal\", lw=5, alpha=0.4)\n",
    "ax.plot(x, y_model, label=\"Fitted model\", color=\"black\", lw=2, ls=\":\")\n",
    "ax.set(xlabel=r\"$\\lambda$\", ylabel=r\"$S(\\lambda)$\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first do the fitting to the two cases with noise,\n",
    "# and we use the guess from before:\n",
    "results_with_good_guess = []\n",
    "\n",
    "for spectrum in spectra_with_noise:\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_guess,\n",
    "        args=(x, spectrum),\n",
    "        bounds=bounds,\n",
    "        options={\"maxiter\": 5000},\n",
    "    )\n",
    "    print(result.message)\n",
    "    print(f\"Fitted parameters: {result.x}\")\n",
    "    results_with_good_guess.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "import pandas as pd\n",
    "\n",
    "# Make a function to create a table for the fitted parameters:\n",
    "def make_table(result):\n",
    "    table_data = {\n",
    "        \"Peak 1\": [round(i, ndigits=3) for i in result.x[:3]],\n",
    "        \"Peak 2\": [round(i, ndigits=3) for i in result.x[3:6]],\n",
    "        \"Peak 3\": [round(i, ndigits=3) for i in result.x[6:]],\n",
    "    }\n",
    "    tablei = pd.DataFrame(\n",
    "        table_data, index=[r\"$\\beta_i$\", r\"$\\lambda_i$\", r\"$\\sigma_i$\"]\n",
    "    )\n",
    "    return tablei\n",
    "\n",
    "# Print out the parameters:\n",
    "for result in results_with_good_guess:\n",
    "    tablei = make_table(result)\n",
    "    display_html(tablei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us show the results graphically to make them easier to compare:\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=3, sharex=True, sharey=True, figsize=(9,3)\n",
    ")\n",
    "\n",
    "for result, gamma, spectrum, ax in zip(results_with_good_guess, gammas, spectra_with_noise, axes):\n",
    "    fit_params = result.x\n",
    "    # Recalculate using the fitted parameters:\n",
    "    y_fit = model(x, fit_params)\n",
    "    ax.scatter(x, spectrum, label=\"Spectrum with noise\", alpha=0.3)\n",
    "    ax.plot(x, y_fit, color=\"k\", label=\"Fitted model\")\n",
    "    ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "    ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "axes[0].legend(fontsize=\"xx-small\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303494b2",
   "metadata": {},
   "source": [
    "We see that there are some differences in the fitted models. To quantify the accuracy, we compare them to the noise-free spectrum. We plot the fitted absorption values against the true values. Ideally, the points would fall on the line $y=x$, indicating a perfect fit. Deviations from this line reveal discrepancies between the model and the true spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=3, nrows=3, figsize=(12,8)\n",
    ")\n",
    "\n",
    "for i, (result, gamma, spectrum) in enumerate(zip(results_with_good_guess, gammas, spectra_with_noise)):\n",
    "    \n",
    "    fit_params = result.x\n",
    "    # Recalculate using the fitted parameters:\n",
    "    y_fit = model(x, fit_params)\n",
    "    r2 = r2_score(spectrum_without_noise, y_fit)\n",
    "    mse = mean_squared_error(spectrum_without_noise, y_fit)\n",
    "    residual = spectrum_without_noise - y_fit\n",
    "    \n",
    "    axes[i, 0].plot(x, spectrum_without_noise, label=f\"Without noise\", color=\"k\")\n",
    "    axes[i, 0].scatter(\n",
    "        x,\n",
    "        spectrum,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"k\",\n",
    "        facecolor=\"none\",\n",
    "        alpha=0.3,\n",
    "        label=\"With noise\"\n",
    "    )\n",
    "    axes[i, 0].plot(x, y_fit, label=f\"Fitted spectrum\", color=\"red\")\n",
    "    axes[i, 0].set_title(f\"γ = {gamma}, R² = {r2:.3f}, MSE = {mse:.2g}\", loc=\"left\")\n",
    "    \n",
    "    axes[i, 1].scatter(\n",
    "        spectrum_without_noise,\n",
    "        y_fit,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"red\",\n",
    "        facecolor='none',\n",
    "    )\n",
    "    axes[i, 2].axhline(y=0.0, ls=\"--\", color=\"k\")\n",
    "    axes[i, 2].scatter(\n",
    "        y_fit,\n",
    "        residual,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"red\",\n",
    "        facecolor='none',\n",
    "    )\n",
    "    \n",
    "    axes[i, 0].legend(fontsize=\"xx-small\")\n",
    "    axes[i, 0].set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    axes[i, 1].set(xlabel=\"Spectrum without noise (y)\", ylabel=\"Fitted model (ŷ)\")\n",
    "    axes[i, 2].set(xlabel=\"Fitted model (ŷ)\",ylabel=\"Residual, y - ŷ\")\n",
    "    \n",
    "    axes[i, 0].sharex(axes[0, 0])\n",
    "    axes[i, 0].sharey(axes[0, 0])\n",
    "    axes[i, 1].sharex(axes[0, 1])\n",
    "    axes[i, 1].sharey(axes[0, 1])\n",
    "    axes[i, 2].sharex(axes[0, 2])\n",
    "    axes[i, 2].sharey(axes[0, 2])\n",
    "    \n",
    "def add_xy_line(ax):\n",
    "    \"\"\"Add the x=y line to help reading plots.\"\"\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot(xlim, ylim, color=\"black\", ls=\"--\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "for ax in axes[:, 1]:\n",
    "    add_xy_line(ax)\n",
    "\n",
    "\n",
    "axes[0, 0].autoscale()\n",
    "axes[0, 1].autoscale()\n",
    "axes[0, 2].autoscale()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we try poorer initial guesses. For simplicity, we use the lowest noise level:\n",
    "initial_guesses = [\n",
    "    # First the \"good\" guess from before:\n",
    "    [0.3, 4.0, 1.0, 0.3, 6.0, 1.0, 0.3, 8.0, 1.0],\n",
    "    # Then, moving all locations to the left:\n",
    "    [0.3, 4.0, 1.0, 0.3, 3.0, 1.0, 0.3, 2.0, 1.0],\n",
    "    # Increase sigmas for all:\n",
    "    [0.3, 4.0, 1.5, 0.3, 6.0, 1.5, 0.3, 8.0, 1.5],\n",
    "    # Set everything to 1.0:\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    # With beta: \n",
    "    [0.8, 4.0, 1.0, 0.8, 6.0, 1.0, 0.8, 8.0, 1.0],\n",
    "    # With locations in the \"center\" of the range:\n",
    "    [0.1, 5.0, 1.0, 0.1, 5.0, 1.0, 0.1, 5.0, 1.0],\n",
    "]\n",
    "    \n",
    "results_with_different_guess = []\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=3, nrows=2, sharex=True, sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, guess in enumerate(initial_guesses):\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        guess,\n",
    "        args=(x, spectrum_with_noise),\n",
    "        bounds=bounds,\n",
    "        options={\"maxiter\": 5000},\n",
    "    )\n",
    "    print(result.message)\n",
    "    print(f\"Fitted parameters: {result.x}\")\n",
    "    results_with_different_guess.append(result)\n",
    "    \n",
    "    fit_params = result.x\n",
    "    tablei = make_table(result)\n",
    "    display_html(tablei)\n",
    "    y_fit = model(x, fit_params)\n",
    "    mse = mean_squared_error(spectrum_with_noise, y_fit)\n",
    "    r2 = r2_score(spectrum_with_noise, y_fit)\n",
    "    \n",
    "    axes[i].plot(x, spectrum_without_noise, label=f\"Without noise\", color=\"k\")\n",
    "    axes[i].scatter(x, spectrum_with_noise, marker=\"o\", edgecolor=\"k\",facecolor=\"none\", alpha=0.3, label=\"With noise\")\n",
    "    axes[i].plot(x, y_fit, label=f\"Fitted spectrum\", color=\"red\")\n",
    "    axes[i].set_title(f\"R² = {r2:.3f}, MSE = {mse:.2g}\", loc=\"left\", fontsize=\"xx-small\")\n",
    "\n",
    "axes[0].legend(fontsize=\"xx-small\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c4526",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(b): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise level?\n",
    "\n",
    "From the results above, we see that the non-linear least squares method finds a good overall fit to the noisy spectrum, as indicated by the high R² values and low mean squared errors in all cases. \n",
    "However, increasing the noise level leads to overfitting (we start to model more of the noise) and deviations from the true spectral shape (see for instance the difference between the lowest and highest noise levels). We also note that we have some patterns in the residuals in all cases, meaning that we make some systematic error.\n",
    "\n",
    "Inspecting the plots above, we also see that the fitted parameters can vary significantly depending on the initial guess. The initial guess for the peak position seems to be important for the quality of the fitted curve.\n",
    "\n",
    "The parameter values are given in the different tables above and while they vary with noise levels and the initial guesses, the method generally yields reasonable estimates in most cases. This suggests that while careful selection of initial guesses and noise reduction are important, the method exhibits some robustness to variations in these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1eb9e",
   "metadata": {},
   "source": [
    "### 1.3(c)\n",
    "\n",
    "Modify your code to use separable least squares (SLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We modify our set-up so that we can find the beta parameters\n",
    "# via least squares, and the other parameters via non-linear least squares:\n",
    "\n",
    "\n",
    "def model_sls(x, betas, lambdas, sigmas):\n",
    "    \"\"\"Calculate y using the given parameters.\"\"\"\n",
    "    signal = np.zeros_like(x)\n",
    "    for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "        signal += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def calculate_beta(x, y, lambdas, sigmas):\n",
    "    X = []\n",
    "    for lambda_i, sigma_i in zip(lambdas, sigmas):\n",
    "        X.append(gaussian(x, 1, lambda_i, sigma_i))\n",
    "    X = np.array(X).T\n",
    "    return np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "\n",
    "def sls_objective(nonlinear_params, x, y):\n",
    "    lambdas = nonlinear_params[0:3]\n",
    "    sigmas = nonlinear_params[3:]\n",
    "\n",
    "    betas = calculate_beta(x, y, lambdas, sigmas)\n",
    "    y_fit = model_sls(x, betas, lambdas, sigmas)\n",
    "    return np.sum((y - y_fit) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_sls = [\n",
    "    4.0, 6.0, 8.0,  # Lambda\n",
    "    1.0, 1.0, 1.0,  # Sigma\n",
    "]\n",
    "\n",
    "# Set up boundaries for the coefficients, these\n",
    "# are on form (min, max) for each parameter\n",
    "bounds_sls = [\n",
    "    (0.0, 10.0), (0.0, 10.0), (0.0, 10.0),  # Lambda\n",
    "    (0.01, 2.0), (0.01, 2.0), (0.01, 2.0),  # Sigma\n",
    "]\n",
    "\n",
    "\n",
    "spectrum_to_use = spectra_with_noise[1]\n",
    "gamma_selected = gammas[1]\n",
    "\n",
    "result = minimize(\n",
    "    sls_objective,\n",
    "    initial_guess_sls,\n",
    "    args=(x, spectrum_to_use),\n",
    "    bounds=bounds_sls,\n",
    "    options={\n",
    "        \"maxiter\": 5000,\n",
    "    },\n",
    ")\n",
    "\n",
    "lambdas = result.x[0:3]\n",
    "sigmas = result.x[3:]\n",
    "betas = calculate_beta(x, spectrum_to_use, lambdas, sigmas)\n",
    "\n",
    "\n",
    "table_data = {\n",
    "    \"Peak 1\": [round(i, ndigits=3) for i in [betas[0], lambdas[0], sigmas[0]]],\n",
    "    \"Peak 2\": [round(i, ndigits=3) for i in [betas[1], lambdas[1], sigmas[1]]],\n",
    "    \"Peak 3\": [round(i, ndigits=3) for i in [betas[2], lambdas[2], sigmas[2]]],\n",
    "}\n",
    "table = pd.DataFrame(\n",
    "    table_data, index=[r\"$\\beta_i$\", r\"$\\lambda_i$\", r\"$\\sigma_i$\"]\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = model_sls(x, betas, lambdas, sigmas)\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(x, spectrum_to_use, label=\"Spectrum with noise\", alpha=0.3)\n",
    "ax.plot(\n",
    "    x, spectrum_without_noise, label=\"Spectrum without noise\", alpha=0.7, lw=3\n",
    ")\n",
    "ax.plot(x, y_fit, label=\"Fitted model\", color=\"black\", alpha=0.5, lw=3)\n",
    "ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "ax.set_title(f\"γ = {gamma_selected}\", loc=\"left\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f36d",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(c): What values did you find for the parameters? Are they different from the ones found in [1.3(b)](#1.3(b))?\n",
    "\n",
    "The SLS method results in parameters that are similar to the ones found by non-linear least squares. The parameters depend on both the initial guess and the noise amplitude (it is easier to fit to lower noise levels). Varying the initial guess indicates that SLS is not particularly more robust than non-linear least squares. Nevertheless, SLS successfully recovers reasonable parameter estimates in many scenarios, even with high noise or less accurate initial guesses. Again, we find that careful selection of initial guesses and noise reduction are important factors to obtain accurate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71138c6c",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1159e",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimising the sum of squared errors ($S$). With $n$\n",
    "measurements of $y$ and $x$,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2 = \\sum_{i=1}^n r_i^2\n",
    "\\end{equation}\n",
    "\n",
    "To minimise $S$ we calculate the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimisation,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in \"normal\" least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9ef76",
   "metadata": {},
   "source": [
    "## B. The summary results from `statsmodels`\n",
    "\n",
    "The summary method in `statsmodels` prints out a lot of information.\n",
    "Here is an example where we have fitted a model $y=a + bx$ to 10 $(x, y)$ points with `statsmodels`\n",
    "and the resulting summary output is printed below. In the table below, the parameter `a` corresponds to `const` and `b` corresponds to `x1`. The output is described further in the\n",
    "sections below. \n",
    "\n",
    "```text\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                      y   R-squared:                       0.956\n",
    "Model:                            OLS   Adj. R-squared:                  0.951\n",
    "Method:                 Least Squares   F-statistic:                     175.6\n",
    "Date:                Tue, 14 Feb 2023   Prob (F-statistic):           1.00e-06\n",
    "Time:                        08:42:06   Log-Likelihood:                -16.957\n",
    "No. Observations:                  10   AIC:                             37.91\n",
    "Df Residuals:                       8   BIC:                             38.52\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          4.4248      0.931      4.754      0.001       2.278       6.571\n",
    "x1             1.9235      0.145     13.253      0.000       1.589       2.258\n",
    "==============================================================================\n",
    "Omnibus:                        3.674   Durbin-Watson:                   2.067\n",
    "Prob(Omnibus):                  0.159   Jarque-Bera (JB):                0.755\n",
    "Skew:                           0.464   Prob(JB):                        0.686\n",
    "Kurtosis:                       3.975   Cond. No.                         13.0\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be87c33",
   "metadata": {},
   "source": [
    "### B.1. Information about the model\n",
    "\n",
    "\n",
    "- **Dep. Variable:** The dependent variable (the variable we are predicting, $y$) in the model.\n",
    "- **Model:** The type of model we have created (OLS = Ordinary Least Squares).\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date & Time:** The date and time for when we created the model.\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 ($x$,$y$) values here).\n",
    "\n",
    "### B.2. Information about the calculation\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares). \n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of predictors (variables excluding the constant).\n",
    "  In our case: $n - k - 1 = 10 - 1 - 1 = 8$. If we did the\n",
    "  fitting without the constant term (for instance, by centring the data first), this\n",
    "  number would be $n-k = 10-1=9$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** Calculations of standard errors assume homoscedastic errors.\n",
    "  If this is not the case, then the standard error is not computed correctly. There\n",
    "  are alternative ways of calculating the standard error; this field tells you\n",
    "  if statsmodels used a more robust method.\n",
    "  \n",
    "### B.3. Information about the overall quality\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "- **Adj. R-squared:** The adjusted $R^2$ for the model. Useful for comparing\n",
    "  models as this one will only increase (when adding more variables) if the\n",
    "  increase in $R^2$ is more than one would expect by chance.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero! Effectively, this compares the model we\n",
    "  have just made to an alternative model equal to the constant intercept term. \n",
    "  To use this value, we would have to decide on a $\\alpha$ level and look up a critical F-value.\n",
    "  This is some extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at\n",
    "  least as extreme as the one above if all regression coefficients are zero. \n",
    "  It is also known as the $p$-value.\n",
    "  If we have selected $\\alpha$ value, we will reject the null hypothesis if \n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis: We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error.\n",
    "  This is equivalent (if the errors are normally distributed)\n",
    "  to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** The\n",
    "  [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n",
    "  [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "  These can be directly calculated from the Log-Likelihood and are useful for comparing alternative\n",
    "  models. Generally, we prefer models with lower AIC and BIC.\n",
    "  \n",
    "### B.4. Information about the coefficients\n",
    "\n",
    "- **coef:** The determined coefficients for the model.\n",
    "\n",
    "- **std err:** The standard error of the coefficients. This\n",
    "  is calculated from,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  s^2 = \\frac{SSE}{n - k - 1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "\n",
    "- **t, P>|t|, and [0.025 0.975]:** Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the $p$-value for such a $t$-test.\n",
    "  Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, these coefficients are indeed different from\n",
    "  zero.\n",
    "  Finally, the **[0.025 0.975]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not specify \n",
    "  $\\alpha$ here, but we can give it as a parameter. The default is $\\alpha=0.05$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
