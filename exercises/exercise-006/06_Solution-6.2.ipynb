{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY313,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44457c41",
   "metadata": {},
   "source": [
    "## Solution to Exercise 6.2\n",
    "\n",
    "[Schummer *et al.*](https://doi.org/10.1016/S0378-1119(99)00342-X) used microarray technology to analyze the expression of 1536 genes in ovarian cancer and non-cancer tissues. Their primary objective was to identify differentially expressed genes in ovarian cancer versus non-cancer tissues to discover genes with diagnosis potential.\n",
    "\n",
    "The data file [`ovo.csv`](ovo.csv) contains numerical gene expressions (for 1536 genes) for 54 tissue samples. Each column corresponds to a specific gene, named `X.1`, `X.2`, and so on. Each tissue sample has been classified as non-cancer (`N`) or cancer (`C`) tissue, and these labels can be found in the column `class`. The raw data has been preprocessed by centring each gene expression so that no further preprocessing is needed. The raw data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the data set.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "data_ovo = pd.read_csv(\"ovo.csv\")\n",
    "classes = data_ovo[\"class\"]  # Classification of samples.\n",
    "# Turn the class labels into numbers for numeric methods\n",
    "y_ovo = [1 if i == \"C\" else 0 for i in classes]\n",
    "X_ovo = data_ovo.filter(like=\"X.\", axis=1)  # Gene expressions for samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2574ba0",
   "metadata": {},
   "source": [
    "### 6.2(a)\n",
    "\n",
    "**Task: Explore the raw data. Do you find genes that appear to show significant differences in expression between non-cancer and cancer tissue?**\n",
    "\n",
    "**Hint:** You can, for instance, inspect the raw data by running a principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "comp = range(1, len(pca.explained_variance_ratio_) + 1)\n",
    "ax1.bar(comp, 100 * pca.explained_variance_ratio_)\n",
    "ax2.plot(comp, 100 * np.cumsum(pca.explained_variance_ratio_), marker=\"o\")\n",
    "ax1.set(xlabel=\"PC no.\", ylabel=\"Explained variance (%)\")\n",
    "ax2.set(xlabel=\"Number of PCs used\", ylabel=\"Explained variance (%)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d736ec",
   "metadata": {},
   "source": [
    "The explained variance indicates that a substantial number of principal components are required to capture a significant portion of the variance in the data. However, the scores of the first few principal components show interesting patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 1], hue=\"class\", ax=axes[0]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data_ovo, x=scores[:, 0], y=scores[:, 2], hue=\"class\", ax=axes[1]\n",
    ")\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "axes[0].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "axes[1].set(\n",
    "    xlabel=f\"Scores, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Scores, PC3 ({percent[2]:.2g}%)\",\n",
    ")\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "    ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee7718",
   "metadata": {},
   "source": [
    "The PC1 scores separate the two tissue types. To understand what variables are important for this separation, we inspect the loadings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ec358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "\n",
    "\n",
    "loadings = pca.components_\n",
    "pc1_loadings = loadings[0, :]\n",
    "pc2_loadings = loadings[1, :]\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "ax.set(\n",
    "    xlabel=f\"Loadings, PC1 ({percent[0]:.2g}%)\",\n",
    "    ylabel=f\"Loadings, PC2 ({percent[1]:.2g}%)\",\n",
    ")\n",
    "ax.axhline(y=0, ls=\":\", color=\"k\")\n",
    "ax.axvline(x=0, ls=\":\", color=\"k\")\n",
    "\n",
    "# Get the 10 largest loadings along PC1 for highlighting:\n",
    "highlighted_indices = np.argsort(abs(pc1_loadings))[-10:]\n",
    "print(f\"10 largest along PC1: {highlighted_indices}\")\n",
    "\n",
    "for i, (xi, yi) in enumerate(zip(pc1_loadings, pc2_loadings)):\n",
    "    if i in highlighted_indices:\n",
    "        txt = ax.text(xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\")\n",
    "        txt.set_path_effects(\n",
    "            [\n",
    "                pe.withStroke(linewidth=1.5, foreground=\"yellow\"),\n",
    "                pe.Normal(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        txt = ax.text(\n",
    "            xi, yi, i, fontsize=\"small\", ha=\"center\", va=\"center\", color=\"0.7\"\n",
    "        )\n",
    "\n",
    "\n",
    "ax.set_xlim(-0.15, 0.15)\n",
    "ax.set_ylim(-0.15, 0.15)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7beaa8",
   "metadata": {},
   "source": [
    "Finally, we create a scatterplot to show how two genes with high loadings (for PC1) separate the tissue samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene1 = 1490\n",
    "gene2 = 92\n",
    "grid = sns.jointplot(\n",
    "    data=data_ovo,\n",
    "    x=f\"X.{gene1 + 1}\",\n",
    "    y=f\"X.{gene2 + 1}\",\n",
    "    hue=\"class\",\n",
    ")\n",
    "ax = grid.fig.axes[0]\n",
    "ax.set_xlabel(f\"Gene expression for {gene1}\")\n",
    "ax.set_ylabel(f\"Gene expression for {gene2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c552259",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(a): Did you find any promising genes?\n",
    "Yes, genes with the highest absolute loadings on Principal Component 1 (PC1) appear promising for distinguishing between the samples. For example 1490 and 92 as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea56348",
   "metadata": {},
   "source": [
    "### 6.2(b)\n",
    "\n",
    "**Task: In the following task, you will develop a classifier to predict whether a tissue sample is cancerous or non-cancerous based on gene expression data. Which error type (false positive or false negative) should be minimised?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9cf63",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(b): Will you minimise false positives or negatives?\n",
    "\n",
    "While a false positive could result in unnecessary and potentially invasive medical procedures, a false negative may be a greater risk by delaying treatment. Assuming that starting medical treatment is not only based on our classifier and that secondary testing will be used, then a false positive made by our classifier would be identified in subsequent tests. Therefore, to minimise the risk of overlooking cancerous samples and delaying treatment, we prioritise the minimisation of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a9963",
   "metadata": {},
   "source": [
    "### 6.2(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify tissue type from the gene expressions. Optimize the tree depth using cross-validation on a training set. Report the optimal maximum depth of the resulting tree.**\n",
    "\n",
    "With reference to the previous problem:\n",
    "\n",
    "* If you prioritised minimising false positives, use the `precision` as your optimisation metric.\n",
    "* If you prioritised minimising false negatives, use the `recall` as your optimisation metric.\n",
    "* If you opted for a balanced approach, use the `balanced_accuracy` as your optimisation metric.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the decision tree can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c71485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ovo, y_ovo, stratify=classes, test_size=0.33, random_state=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d72be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Use recall to minimize false negatives\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b87d1",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(c): What depth did you get for your tree?\n",
    "The optimal depth was 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc51e4",
   "metadata": {},
   "source": [
    "### 6.2(d)\n",
    "\n",
    "**Task: Create a k-nearest neighbours classifier to classify tissue type from the gene expressions. Optimize the number of neighbours using cross-validation on a training set. Report the optimal number of neighbours.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the k-nearest neighbours classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 15)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3886e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 15)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # To minimize false negatives\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a116b6",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(d): What was the optimal number of neighbours?\n",
    "The optimal number of neighbours was 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a61c",
   "metadata": {},
   "source": [
    "### 6.2(e)\n",
    "\n",
    "**Task: Create a random forest classifier to classify tissue type from the gene expressions. Optimize the number of trees and levels using cross-validation on a training set. Report the optimal number of trees and levels.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the random forest classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 11),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_forest = grid.best_estimator_\n",
    "print(\"Best forest:\", best_forest)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7828d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 10),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=2025),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_forest = grid.best_estimator_\n",
    "print(\"Best forest:\", best_forest)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9512269",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(e): What was the optimal number of estimators and tree depth?\n",
    "The optimal number of estimators was 50 and the depth was 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894199b8",
   "metadata": {},
   "source": [
    "### 6.2(f)\n",
    "\n",
    "**Task: Compare the three optimised classifiers you have made by applying them to the test set and obtaining the corresponding confusion matrices. Also compute the [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), and the [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) for the test set. Which classifier performs best?**\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The metrics can be computed as follows:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "y_hat = best_tree.predict(X_test)\n",
    "recall_tree = recall_score(y_test, y_hat)\n",
    "precision_tree = precision_score(y_test, y_hat)\n",
    "bac_tree = balanced_accuracy_score(y_test, y_hat)\n",
    "print(f\"Recall: {recall_tree:.3f}\")\n",
    "print(f\"Precision: {precision_tree:.3f}\")\n",
    "print(f\"Balanced accuracy: {bac_tree:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9, 3))\n",
    "\n",
    "axes[0].set_title(\"Decision tree (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=False,\n",
    "    ax=axes[0],\n",
    ")\n",
    "fig.colorbar(axes[0].images[0], ax=axes[0], shrink=0.7)\n",
    "\n",
    "axes[1].set_title(\"kNN (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_knn, X_test, y_test, colorbar=False, ax=axes[1]\n",
    ")\n",
    "fig.colorbar(axes[1].images[0], ax=axes[1], shrink=0.7)\n",
    "\n",
    "axes[2].set_title(\"Random forest (test)\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_forest, X_test, y_test, colorbar=False, ax=axes[2]\n",
    ")\n",
    "fig.colorbar(axes[2].images[0], ax=axes[2], shrink=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83670868",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"Classifier\": [],\n",
    "    \"Recall (test)\": [],\n",
    "    \"Precision (test)\": [],\n",
    "    \"Balanced accuracy (test)\": [],\n",
    "}\n",
    "for cls in (best_tree, best_knn, best_forest):\n",
    "    name = str(cls)\n",
    "    print(name)\n",
    "    y_hat = cls.predict(X_test)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    precision = precision_score(y_test, y_hat)\n",
    "    bac = balanced_accuracy_score(y_test, y_hat)\n",
    "    table[\"Classifier\"].append(name)\n",
    "    table[\"Recall (test)\"].append(recall)\n",
    "    table[\"Precision (test)\"].append(precision)\n",
    "    table[\"Balanced accuracy (test)\"].append(bac)\n",
    "    print(f\"\\t-Recall: {recall:.3f}\")\n",
    "    print(f\"\\t-Precision: {precision:.3f}\")\n",
    "    print(f\"\\t-Balanced accuracy: {bac:.3f}\")\n",
    "table = pd.DataFrame(table)\n",
    "table.sort_values(by=\"Recall (test)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace6af5",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(f): Which classifier performs best?\n",
    "From these results, we see that the random forest classifier makes fewer false negative mistakes (resulting in a higher recall). However, it produces one additional false positive compared to the k-nearest neighbours classifier (resulting in a lower precision). Since we focus on minimizing the number of false negatives (as per our answer to 6.2(b)), the random forest classifier performs best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
