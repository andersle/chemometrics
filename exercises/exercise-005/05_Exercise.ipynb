{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe25a2c",
   "metadata": {},
   "source": [
    "# Exercise set 5: Partial Least squares and training and testing.\n",
    "\n",
    "The main goals of this exercise are to perform Partial Least Squares (PLS) regression and use training and testing sets. Using training and testing sets allows us to assess the model's ability to generalize to unseen data and avoid overfitting. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Create a PLS regression model.\n",
    "- Create and use training and test sets.\n",
    "- Assess your regression model by calculating root mean squared errors.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "* [5.1(a)](#5.1(a)), [5.1(b)](#5.1(b)), and [5.1(c)](#5.1(c)): To show that you can train a Partial Least Squares regression model and calculate RMSEC (Root Mean Squared Error of Calibration) and RMSEP (Root Mean Squared Error of Prediction).\n",
    "\n",
    "**Note:**\n",
    "Exercises [5.1(d)](#5.1(d)), [5.1(e)](#5.1(e)), [5.2(b)](#5.2(b)), [5.3(b)](#5.3(b)), [5.3(c)](#5.3(c)), [5.3(d)](#5.3(d)) involve programming of a complexity beyond what you are expected to handle independently, particularly under exam time pressure. Understanding the results and the underlying concepts are more important and to help you practice this (without focusing on the technical programming part), you can find partial solutions (without interpretation) in [appendix B](#B.-Partial-answers-to-some-of-the-exercises), specifically:\n",
    "\n",
    "* [5.1(e)](#5.1(e)): You can use the figure in [appendix B.1](#B.1-Results-for-5.1(e)) to compare the performance of the model for the training and test sets. Make sure that you understand what the figure is showing.\n",
    "\n",
    "* [5.3(b)](#5.3(b)): The X-scores and PLS X-rotations can be found in the figure in [appendix B.2](#B.2-Results-for-5.3(b)).\n",
    "\n",
    "* [5.3(c)](#5.3(c)): The Y-scores and PLS Y-rotations can be found in the figure in [appendix B.3](#B.3-Results-for-5.3(c)).\n",
    "\n",
    "* [5.3(d)](#5.3(d)): The PLS loadings for X and Y can be found in the figure in [appendix B.4](#B.4-Results-for-5.3(d)).\n",
    "\n",
    "For [5.1(d)](#5.1(d)) and [5.2(b)](#5.2(b)), ensure you understand the process and purpose of cross-validation. One way to check your understanding is to explain it to a classmate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3b183",
   "metadata": {},
   "source": [
    "## Exercise 5.1 Partial Least Squares with training and testing\n",
    "\n",
    "[Windig and Stephenson](https://doi.org/10.1021/ac00046a015) measured near-infrared spectra\n",
    "for 140 mixtures of the solvents methylene chloride, 2-butanol, methanol,\n",
    "dichloropropane, and acetone. Here, we will predict the compositions of the mixtures from the spectra.\n",
    "Each spectrum was sampled at 700 wavelengths\n",
    "between 1100 and 2500 nm. The file\n",
    "[`windig.csv`](windig.csv) contains the raw data:\n",
    "Each row in this file\n",
    "contains a spectrum (the columns starting with `wavelength.`) and the\n",
    "corresponding concentrations (the columns starting with `conc.`).\n",
    "\n",
    "**The goal of exercise 5.1 is to make a model for predicting the composition of a mixture from its spectrum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0916009c",
   "metadata": {},
   "source": [
    "You can inspect the raw data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"windig.csv\")\n",
    "X = data.filter(like=\"wavelength\", axis=1).values  # NIR spectra\n",
    "Y = data.filter(like=\"conc\", axis=1).values  # Concentrations\n",
    "print(f\"No. of spectra: {X.shape[0]}\")\n",
    "print(f\"No. of wavelengths: {X.shape[1]}\")\n",
    "print(f\"No of concentration samples: {Y.shape[0]}\")\n",
    "print(f\"No of species in each sample: {Y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bed7b6",
   "metadata": {},
   "source": [
    "And the individual spectra can be visualised with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the spectra:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "for spectrum in X:\n",
    "    ax.plot(spectrum)\n",
    "ax.set(xlabel=\"Wavelength (nm)\", ylabel=\"Absorbance\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc070e12",
   "metadata": {},
   "source": [
    "**Note:** The spectra have been processed so you can use the directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0cd7f",
   "metadata": {},
   "source": [
    "### 5.1(a)\n",
    "\n",
    "To develop and assess your model, you will create and make use of a training and testing data set.\n",
    "\n",
    "**Explain what the purposes of these two sets are and how they can be created.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963b6df",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(a): What is the purpose of the training and testing data sets, and how are they created?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45e9d9",
   "metadata": {},
   "source": [
    "### 5.1(b)\n",
    "\n",
    "**Split the raw data into a training set and a test set. Use 33% of the data for the test set. How many samples do you have in the training set and the test set?**\n",
    "\n",
    "**Hint:** With scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), splitting the data can be done with\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.33,  # Use 33 % of the data (one-third) for the test set.\n",
    "    shuffle=True,  # Randomly shuffle the data\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1faa24",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(b): How many samples do you have in the training set and the test set?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a78abe",
   "metadata": {},
   "source": [
    "### 5.1(c)\n",
    "\n",
    "**Task: Create a Partial Least Squares (PLS) regression model for predicting the concentrations from the spectra. Use 2 latent variables for the PLS model and evaluate your model by calculating the RMSEC (root mean squared error of calibration) and RMSEP (root mean squared error of prediction) for each of the five concentrations.**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1.  **Create a PLS regression model:**\n",
    "    ```python\n",
    "    from sklearn.cross_decomposition import PLSRegression\n",
    "    # Set up a PLS model:\n",
    "    model = PLSRegression(\n",
    "        n_components=2,  # Use two components (latent variables)\n",
    "        scale=False,  # Do not scale X and Y (we will do this separately, if needed)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "2.  **Fit the model to the training data:**\n",
    "    ```python\n",
    "    model.fit(X_train, Y_train)  # Fit/make the model\n",
    "    ```\n",
    "\n",
    "3.  **Calculate the RMSEC (root mean squared error of calibration):**\n",
    "    * When we use the training set to create our model, we are doing a *calibration*. If we calculate the RMSE (root mean squared error) based on the training set, we refer to this as the RMSEC. This quantifies the error we get in connection with making (calibrating) the model.\n",
    "    ```python\n",
    "    from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    rmsec = root_mean_squared_error(Y_train, y_hat_train)\n",
    "    ```\n",
    "\n",
    "4.  **Calculate the RMSEP (root mean squared error of prediction):**\n",
    "    * When we use the test set to test our model, we are checking how well our model *predicts* \"new\" samples (that is, samples not used when making the model). If we calculate RMSE based on the test set, we refer to this as the RMSEP. This quantifies the error we can expect to make when using our model for predicting new samples.\n",
    "    ```python\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    rmsep = root_mean_squared_error(Y_test, y_hat_test)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604150f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec63e3",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(c): What values did you get for RMSEC and RMSEP.\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6c9b4",
   "metadata": {},
   "source": [
    "### 5.1(d)\n",
    "\n",
    "**Task: Optimize the number of PLS components by performing cross-validation on a grid where you vary the number of components. Calculate RMSEC, RMSECV (root mean squared error of cross-validation), and RMSEP for your new model. Report the optimal number of components.**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. **Optimize the number of components by using cross-validation on a grid of possible parameters, for instance, by using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for scikit-learn:**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\"n_components\": range(1, 11)}  # Test 1 through 10 components\n",
    "# Set up a search over the parameter space:\n",
    "grid_search = GridSearchCV(\n",
    "    PLSRegression(scale=False),  # The base model\n",
    "    parameters,  # The parameters we will consider,\n",
    "    cv=5,  # The number of splits for the cross-validation\n",
    "    scoring=\"neg_mean_squared_error\",  # How we score how well the model is performing\n",
    "    refit=True,  # Refit using the best-found parameters on the whole training set.\n",
    ")\n",
    "# Run the cross-validation\n",
    "grid_search.fit(X_train, Y_train)\n",
    "# Get the best number of components:\n",
    "best_components = grid_search.best_params_[\"n_components\"]\n",
    "# Get the best-performing model:\n",
    "best_model = grid_search.best_estimator_\n",
    "# Get the results per parameter considered:\n",
    "mean_score = grid_search.cv_results_[\"mean_test_score\"]  # The mean score\n",
    "error_score = grid_search.cv_results_[\n",
    "    \"std_test_score\"\n",
    "]  # The uncertainty in the score\n",
    "# These two can be plotted as a function of the number of parameters considered;\n",
    "# this can help us see the best parameters while considering the error.\n",
    "```\n",
    "\n",
    "2. **Recalculate RMSECV using the optimized model, for instance, by using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) from scikit-learn:**\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cvscore = cross_val_score(\n",
    "    model,  # Select the model we are going to score\n",
    "    X_train,  # Give the X-training set\n",
    "    Y_train,  # Give the y-training set\n",
    "    scoring=\"neg_mean_squared_error\",  # select scoring method\n",
    "    cv=5,  # Number of splits to make\n",
    ")\n",
    "cvscore = np.sqrt(-cvscore)  # Account for the negative sign.\n",
    "rmsecv = cvscore.mean()\n",
    "rmsecv_std = np.std(cvscore)\n",
    "print(f\"\\nRMSECV: {rmsecv} ± {rmsecv_std}\")\n",
    "```\n",
    "\n",
    "**Note:** We use a *negative* mean squared error for the grid search and for calculating RMSECV. This is because the methods `cross_val_score` and `GridSearchCV` are often used in connection with optimization where we want to *maximize* something. If we *maximize the negative* of the mean squared error, we can *minimize the error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f42304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7a493",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(d): What is the optimal number of components?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1963d89",
   "metadata": {},
   "source": [
    "### 5.1(e)\n",
    "\n",
    "**Task: Show the results for the training data and the testing data graphically by plotting the predicted vs. the observed values for all 5 chemical components for the optimized model. Include RMSEC, R² for the training set, RMSEP and R² for the test set as labels in your figure (calculate these for each component separately). Further, report the metrics you calculated in a table.**\n",
    "\n",
    "**Hint:** The R² can be calculated using:\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "component_number = 1  # To select a column (one component) from Y:\n",
    "r_squared_train = r2_score(Y_train[:,component_number], y_predicted_train[:,component_number])\n",
    "r_squared_test = r2_score(Y_test[:,component_number], y_predicted_test[:,component_number])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41039d3",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(e): Report the metrics you found:\n",
    "*Double click here*\n",
    "\n",
    "| Component                        | RMSEC | RMSEP | R² (train) | R² (test) |\n",
    "|----------------------------------|-------|-------|------------|-----------|\n",
    "| methylene chloride (component 1) |       |       |            |           |\n",
    "| 2-butanol (component 2)          |       |       |            |           |\n",
    "| methanol (component 3)           |       |       |            |           |\n",
    "| dichloropropane (component 4)    |       |       |            |           |\n",
    "| acetone (component 5)            |       |       |            |           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febcce0",
   "metadata": {},
   "source": [
    "## Exercise 5.2 Use of cross-validation when we have few samples\n",
    "\n",
    "It is not always feasible to do the split into training and test sets when we have few samples. Another option then is to use something called **Leave-one-out cross-validation** (LOOCV). LOOCV involves training the model on all but one data point and using the remaining point for testing, repeating this process for each data point. We will use that method in this exercise\n",
    "\n",
    "We will use the data of [Forbes](https://doi.org/10.1017/S0080456800032075) who investigated the\n",
    "relationship between the boiling point of water and the atmospheric pressure, and collected data in the Alps and Scotland. Forbes' goal was to estimate altitudes from the boiling point alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119b5e6",
   "metadata": {},
   "source": [
    "### 5.2(a)\n",
    "\n",
    "**Task: Load the data from Forbes (data file [forbes.csv](forbes.csv)), plot it, and create a linear regression model\n",
    "that predicts the atmospheric pressure from the temperature. Report the R² and [root mean\n",
    "squared error (RMSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html) for your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a15fc",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(a): What value did you get for R² and the RMSE?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a29e4a",
   "metadata": {},
   "source": [
    "### 5.2(b)\n",
    "\n",
    "**Task: Estimate the error you can expect to make if you use your model for predicting the pressure.\n",
    "Do this by LOOCV and calculate the root mean squared error of cross-validation (RMSECV)**\n",
    "\n",
    "**Note:** LOOCV is a special case of **training** and **testing**, and you can find a short description of it\n",
    "in [appendix A](#A.-Leave-one-out-cross-validation) with example code for running LOOCV. The code example for LOOCV is concise, so make sure you understand what goes on here (that is, what LOOCV is doing). If you are working with someone, try explaining testing/training and how LOOCV works to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c5e02",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(b): What value did you get for RMSECV?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d263f",
   "metadata": {},
   "source": [
    "## Exercise 5.3 Partial Least Squares and interpretation of scores and loadings\n",
    "\n",
    "The file [elements.csv](elements.csv) contains information about the elements of the periodic table. This dataset includes various physical and chemical properties, allowing us to explore the underlying relationships between these elements. The columns in the file are as follows:\n",
    "\n",
    "\n",
    "| **Column**                      | **Description**                                         | **Unit** |\n",
    "|:--------------------------------|:--------------------------------------------------------|:---------|\n",
    "| name                            | The name of the element                                 |          |\n",
    "| symbol                          | The symbol for the element (e.g. H, He, etc.)           |          |\n",
    "| atomic_radius                   | Atomic radius                                           | Å        |\n",
    "| atomic_weight                   | Atomic weight                                           | u        |\n",
    "| covalent_radius                 | Covalent radius                                         | pm       |\n",
    "| density                         | Density at 295 K                                        | g/cm³    |\n",
    "| dipole_polarizability           | Dipole polarizability                                   | bohr³    |\n",
    "| electrons                       | The number of electrons in the element                  |          |\n",
    "| mass_number                     | Mass number of the most abundant isotope                |          |\n",
    "| neutrons                        | The number of neutrons in the element                   |          |\n",
    "| protons                         | The number of protons in the element                    |          |\n",
    "| zeff                            | Effective nuclear charge                                |          |\n",
    "| vdw_radius                      | Van der Waals radius                                    | pm       |\n",
    "| first_ionization                | First ionization energy                                 | eV       |\n",
    "| electronegativity allred-rochow | Allred and Rochow’s scale of electronegativity          | e²/pm²   |\n",
    "| electronegativity gordy         | Gordy’s scale of electronegativity                      | e/pm     | \n",
    "| atomic_radius_wikipedia         | Atomic radius from [Wikipedia](https://en.wikipedia.org/wiki/Atomic_radii_of_the_elements_(data_page)) | pm |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905e637",
   "metadata": {},
   "source": [
    "### 5.3(a)\n",
    "\n",
    "**Task: Create a PLS regression model for predicting `first_ionization`, `density`, `protons`, and `atomic_radius` from the other variables.**\n",
    "\n",
    "**Notes:**:\n",
    "\n",
    "1. Remove the non-numeric columns like 'name' and 'symbol' before creating the PLS model.\n",
    "\n",
    "2. Use two components for the PLS regression model. Do not do a split into a training and test set in this exercise (this is not so crucial here since we will focus on interpreting scores and loadings).\n",
    "\n",
    "3. Consider if you should normalize (scale) the data using a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) before performing the PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d23c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53024d8e",
   "metadata": {},
   "source": [
    "### 5.3(b)\n",
    "\n",
    "**Task: Inspect the scores and rotations for X by creating 2D scatter plots. Are there any trends/groupings or outliers in the scores? What of the original X-variables can, if groups/trends are present, be used to interpret these trends?**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. Assuming that `pls` is the fitted object containing the PLS model, and `X` is our raw data, we can get the scores and rotations by:\n",
    "```python\n",
    "x_scores = pls.transform(X)\n",
    "x_rotations = pls.x_rotations_\n",
    "```\n",
    "\n",
    "2. For the scatter plots, you have two options. To plot the scores and rotations for the two PLS components in two different plots, or in the same plot (as a biplot). The biplot can sometimes help interpretation, but can be crowded if there are many samples and features.\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "1. We use the rotations here instead of the loadings (or weights). This is because the X-rotations apply directly to the X data to create the scores. Thus, the rotations tell use more directly the relation between the original variables and the calculated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676b1f8",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(b): Are there any trends/groupings or outliers in the scores and what variables can be used to interpret them?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae076b5",
   "metadata": {},
   "source": [
    "### 5.3(c)\n",
    "\n",
    "**Task: Inspect the scores and rotations for Y by creating 2D scatter plots. Are there any trends/groupings or outliers in the scores? What of the original Y-variables can, if groups/trends are present, be used to interpret these trends?**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. Assuming that `pls` is the fitted object containing the PLS model, and `X` and `Y` are our raw data, we can get the scores and rotations by:\n",
    "```python\n",
    "x_scores, y_scores = pls.transform(X, y=Y)\n",
    "y_rotations = pls.y_rotations_\n",
    "```\n",
    "\n",
    "2. For the scatter plots, you have two options. To plot the scores and rotations for the two PLS components in two different plots, or in the same plot (as a biplot). The biplot can sometimes help interpretation, but can be crowded if there are many samples and features.\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "1. We use the rotations here instead of the loadings (or weights). This is because the Y-rotations apply directly to the Y data to create the scores. Thus, the rotations tell use more directly the relation between the original variables and the calculated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be552ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95524a4b",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(c): Are there any trends/groupings or outliers in the scores and what variables can be used to interpret them?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cca899",
   "metadata": {},
   "source": [
    "### 5.3(d)\n",
    "\n",
    "**Task: Inspect the X-rotations and Y-loadings together. Which of the X-variables could be important for predicting the different Y-variables (answer this by exploring the correlations between the X-variables and the Y-variables).**\n",
    "\n",
    "**Hints:** \n",
    "\n",
    "1. Assuming that `pls` is the fitted object containing the PLS model, we can get the scores for Y by:\n",
    "```python\n",
    "y_loadings = pls.y_loadings_\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "1. We use the X-rotations ($R$) and Y-loadings ($Q$) because they help us understand the relationships between the X-variables and the Y-variables in the PLS model. The PLS model $Y = XB = XRQ^T$ shows that the regression coefficients ($B$) can be decomposed into the product of $R$ and $Q^T$. Therefore, plotting $R$ and $Q$ together helps us investigate correlations captured by the PLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ad5bb",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(d): What X-variables seem important for predicting Y and what variables are correlated?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9611c6",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a31c4b",
   "metadata": {},
   "source": [
    "## A. Leave-one-out cross-validation\n",
    "\n",
    "In Leave-one-out cross-validation (LOOCV), we first pick one sample,\n",
    "measurement number $j$, and we fit the model using the $n-1$ other points\n",
    "(all points except $j$). After the fitting, we check how well the model can predict\n",
    "measurement $j$ by calculating the difference between the\n",
    "measured ($y_j$) and predicted ($\\tilde{y}_j$) value. This difference, $r_j = y_{j} - \\tilde{y}_j$, is\n",
    "called the predicted residual, and it tells us the error we just made.\n",
    "\n",
    "There is nothing special about picking point $j$, and we can try all possibilities\n",
    "of leaving one point out, fitting the model using the remaining $n-1$\n",
    "measurements, and predicting the value we left out.\n",
    "After doing this for all possibilities, we have fitted the model\n",
    "$n$ times and calculated $n$ predicted residuals. The mean squared error (obtained from the squared\n",
    "residuals), $\\mathrm{MSE}_{\\mathrm{CV}}$, can then be used\n",
    "to estimate the error in the model,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the measured $y$ in experiment $i$, and $\\tilde{y}_i$ is the\n",
    "predicted $y$, using a model which was fitted using all points *except* $y_i$.\n",
    "\n",
    "For a polynomial fitting, there is an alternative to refitting the model $n$ times. In fact,\n",
    "we can show that for polynomial fitting, the mean squared error can\n",
    "be obtained by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2 =\n",
    "\\frac{1}{n}\\sum_{i=1}^{m} \\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{y}_i$'s are predicted values using the\n",
    "model fitted with *all data points*,\n",
    "and $h_{ii}$ is the $i$'th diagonal element of the\n",
    "$\\mathbf{H}$ matrix (the projection matrix,\n",
    "see Eq.(4.49) on page 49 in our textbook),\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H} =\n",
    "\\mathbf{X} \n",
    "\\left( \n",
    "  \\mathbf{X}^\\mathrm{T} \\mathbf{X}\n",
    "\\right)^{-1}\n",
    "\\mathbf{X}^\\mathrm{T} = \\mathbf{X} \\mathbf{X}^+,\n",
    "\\end{equation}\n",
    "\n",
    "Note the difference between $\\hat{y}_i$ and $\\tilde{y}_i$, and the\n",
    "fact that we  do not have to do the\n",
    "refitting(!) to obtain the $\\mathrm{MSE}_{\\mathrm{CV}}$.\n",
    "\n",
    "When you calculate $\\mathrm{MSE}_{\\mathrm{CV}}$, use one of the two approaches above or both\n",
    "if you want to see if they give the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbddb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The examples below assume that the matrix X is called X_temp\n",
    "# and that y is stored in the variable pressure.\n",
    "\n",
    "# Example 1 of LOOCV:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# scikit-learn has a method to pick out samples for leave-one-out:\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "# Split the X-data in X_temp into training and testing:\n",
    "for train_index, test_index in loo.split(X_temp):\n",
    "    # train_index = index of samples to use for training\n",
    "    # test_index = index of samples to use for testing\n",
    "    # Pick out samples (for training and testing):\n",
    "    X_train, X_test = X_temp[train_index], X_temp[test_index]\n",
    "    y_train, y_test = pressure[train_index], pressure[test_index]\n",
    "    # Fit a new model with the training set:\n",
    "    model = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    # Predict y for the test set:\n",
    "    y_hat = model.predict(X_test)\n",
    "    # Compare the predicted y values in the test set with the measured ones:\n",
    "    error.append((y_test - y_hat) ** 2)\n",
    "rmsecv_1 = np.sqrt(np.mean(error))\n",
    "print(f\"RMSECV = {rmsecv_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aaa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 of LOOCV:\n",
    "\n",
    "# scikit-learn has a method for leave-one-out selection, and a method for\n",
    "# cross-validation. And these two can be combined:\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# Create \"empty\" model for fitting:\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "# Run cross-validation, where we select testing and training with LeaveOneOut:\n",
    "scores = cross_val_score(\n",
    "    model, X_temp, pressure, scoring=\"neg_mean_squared_error\", cv=LeaveOneOut()\n",
    ")\n",
    "rmsecv_2 = np.sqrt(np.mean(-scores))\n",
    "print(f\"RMSECV = {rmsecv_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 of LOOCV:\n",
    "\n",
    "# We calculate the H matrix and use that:\n",
    "# OBS! First, a detail that is easy to miss; The X used for H includes the column of ones!\n",
    "X_matrix = np.column_stack((np.ones_like(temperature), temperature))\n",
    "H = X_matrix @ np.linalg.pinv(X_matrix)\n",
    "hii = np.diagonal(H)\n",
    "residuals_loo = (pressure - pressure_hat) / (1 - hii)\n",
    "rmsecv_3 = np.sqrt(np.mean(residuals_loo**2))\n",
    "print(f\"RMSECV = {rmsecv_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa064d",
   "metadata": {},
   "source": [
    "## B. Partial answers to some of the exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e5ee6",
   "metadata": {},
   "source": [
    "### B.1 Results for 5.1(e)\n",
    "\n",
    "![Exercise 5.1(e)](5.1.e.png)\n",
    "\n",
    "|    | Component          |    RMSEC |    RMSEP |   R² (train) |   R² (test) |\n",
    "|---:|:-------------------|---------:|---------:|-------------:|------------:|\n",
    "|  0 | methylene chloride | 0.774702 | 0.913428 |     0.996022 |    0.994106 |\n",
    "|  1 | 2-butanol          | 0.56474  | 0.666846 |     0.997418 |    0.997723 |\n",
    "|  2 | methanol           | 0.686723 | 0.871563 |     0.996762 |    0.994905 |\n",
    "|  3 | dichloropropane    | 0.829081 | 0.954528 |     0.99573  |    0.992305 |\n",
    "|  4 | acetone            | 0.416455 | 0.541473 |     0.998559 |    0.99849  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae38d2",
   "metadata": {},
   "source": [
    "### B.2 Results for 5.3(b)\n",
    "\n",
    "![Exercise 5.3(b)](5.3.b.png)\n",
    "\n",
    "**Note:** The elements have been colored according to their period to help the interpretation. The text labels for `electrons`, `neutrons`, `atomic_weight`, and `mass_number` are overlapping, making it difficult to see the individual labels. This is because their rotations are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37906b",
   "metadata": {},
   "source": [
    "### B.3 Results for 5.3(c)\n",
    "\n",
    "![Exercise 5.3(c)](5.3.c.png)\n",
    "\n",
    "**Note:** The elements have been colored according to their period to help the interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da273d0d",
   "metadata": {},
   "source": [
    "### B.4 Results for 5.3(d)\n",
    "\n",
    "![Exercise 5.3(d)](5.3.d.png)\n",
    "\n",
    "**Note:** In the plot, the text labels for `electrons`, `neutrons`, `atomic_weight`, and `mass_number` are overlapping, making it difficult to see the individual labels. This is because their rotations are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bdc90",
   "metadata": {},
   "source": [
    "## Your feedback for Exercise 5\n",
    "\n",
    "1. **Time & Difficulty:**\n",
    "* Length (1=too short, 5=too long): 1  2  3  4  5\n",
    "* Difficulty (1=too easy, 5=too difficult): 1  2  3  4  5\n",
    "* Most challenging part: ________________________\n",
    "\n",
    "2. **Code Examples:**\n",
    "* More or less example code?  More  Less  About Right\n",
    "* Areas where more examples would be helpful: ________________________\n",
    "\n",
    "3. **Errors/Inconsistencies:** Did you encounter any?  Yes  No  If yes, please describe: ________________________\n",
    "    \n",
    "4. **Suggestions:** How could this exercise be improved? ________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
