{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801b2291",
   "metadata": {},
   "source": [
    "# Exercise set 7: Classification & Signal processing\n",
    "\n",
    "There are two main goals for this exercise:\n",
    "\n",
    "1. To develop optimised classifiers (e.g., a decision tree) using cross-validation, and gain experience with the assessment of classifiers.\n",
    "2. To gain practical experience with signal processing techniques used for preprocessing, for instance, of Near-Infrared (NIR) spectra. Preprocessing methods are important for improving the signal-to-noise ratio, correcting for scattering effects (variations in light path due to particle size, etc.), and enhancing spectral features, which can lead to more reliable analysis and development of robust predictive models.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Develop optimised classifiers and assess them.\n",
    "- Preprocess spectra by normalisation, multiplicative scatter correction, or taking a derivative.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [7.1(a)](#7.1(a)), [7.1(b)](#7.1(b)) and [7.1(c)](#7.1(c)): To show that you can create a optimised classifier.\n",
    "- [7.2(a)](#7.2(a)) and at least one of [7.2(b)](#7.2(b)), [7.2(c)](#7.2(c)) or [7.2(d)](#7.2(d)): To show that you can apply preprocessing to NIR spectra.\n",
    "\n",
    "**Files required for this exercise:**\n",
    "* [Exercise 7.1](#Exercise-7.1-Developing-optimised-classifiers): [bace-small.csv](bace-small.csv)\n",
    "* [Exercise 7.2](#Exercise-7.2-Preprocessing-NIR-spectra): [nir.csv](nir.csv)\n",
    "\n",
    "Please ensure that these files are saved in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe0cd0",
   "metadata": {},
   "source": [
    "## Exercise 7.1 Developing optimised classifiers\n",
    "\n",
    "We will here consider a version of the **BACE** dataset from [MoleculeNet](https://moleculenet.org) (site containing benchmark data for molecular machine learning).\n",
    "\n",
    "This data set contains 1513 molecules that have been labelled by their binding affinity to BACE-1 (1 for active, 0 for inactive). Active binders of BACE-1 could potentially be used as treatments for Alzheimer’s Disease.\n",
    "\n",
    "The version we consider contains a subset (9) of all features in the original data (around 590):\n",
    "* `MW`: The total mass (molecular weight) of the molecule.\n",
    "* `AlogP`: The partition coefficient. Measures the lipophilicity (how much the molecule prefers oil over water).\n",
    "* `HBA`: Hydrogen Bond Acceptors, the number of atoms that can receive a hydrogen bond.\n",
    "* `HBD`: Hydrogen Bond Donors, the number of atoms that can be \"donated\" to form a hydrogen bond.\n",
    "* `PSA`: Polar Surface Area, the total surface area contributed by polar atoms.\n",
    "* `RB`: Rotatable Bonds, A count of single bonds that can rotate freely.\n",
    "* `HeavyAtomCount`: The total number of atoms in the molecule, excluding hydrogen.\n",
    "* `ChiralCenterCount`: The number of chiral centres in the molecule.\n",
    "* `RingCount`: The number of cyclic structures (like benzene rings) in the molecule.\n",
    "\n",
    "The data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "data = pd.read_csv(\"bace-small.csv\")\n",
    "skip = (\"mol\", \"Class\")\n",
    "y = data[\"Class\"]  # Classification of samples.\n",
    "class_names = [\"Inactive\", \"Active\"]\n",
    "features = [i for i in data.columns if i not in skip]\n",
    "print(\"Features:\", features)\n",
    "X = data[features].to_numpy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a7cd3",
   "metadata": {},
   "source": [
    "### 7.1(a)\n",
    "\n",
    "**Task: In the following task, you will develop a classifier to predict whether a small molecule is an active (positive) or inactive (negative) binder. Which error type (false positive or false negative) should be minimised?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14386fb",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(a): Will you minimise false positives or negatives?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982acd17",
   "metadata": {},
   "source": [
    "### 7.1(b)\n",
    "\n",
    "**Task: Prepare the data by splitting it into train/test sets and standardise the features. Use the code provided below and note the order of operations.**\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. Use the code provided in the cell below to create the training and test sets. We use something called **stratification** here. This makes sure that the train/test split maintains the same proportion of classes as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the training set use:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train0, X_test0, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.33, random_state=2026\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d141617",
   "metadata": {},
   "source": [
    "2. Preprocess the data using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train0)\n",
    "\n",
    "X_train = scaler.transform(X_train0)\n",
    "X_test = scaler.transform(X_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860bc37",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(b): Can you give a reason why we fit the `StandardScaler` to the training data and not to the full data set?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09546aea",
   "metadata": {},
   "source": [
    "### 7.1(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify molecules. Optimise the tree depth using cross-validation on a training set. Report the optimal maximum depth of the resulting tree.**\n",
    "\n",
    "With reference to the previous problem:\n",
    "\n",
    "* If you prioritised minimising false positives, use the `precision` as your optimisation metric.\n",
    "* If you prioritised minimising false negatives, use the `recall` as your optimisation metric.\n",
    "* If you opted for a balanced approach, use the `balanced_accuracy` as your optimisation metric.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "1. The optimisation of the decision tree can be done as follows (assuming that you have already split into the training and test sets):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid_t = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Swap this with the metric you prefer\n",
    "    refit=True,\n",
    ")\n",
    "# Run the grid search:\n",
    "grid_t.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid_t.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid_t.best_score_)\n",
    "print(\"Best parameters\", grid_t.best_params_)\n",
    "print(\"Actual depth of the tree:\", best_tree.get_depth())\n",
    "# Note: The max_depth variables is just the maximum depth,\n",
    "# the resulting tree can be shorter if adding more levels\n",
    "# does not improve the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196f0a7",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(c): What depth did you get for your tree?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b268e",
   "metadata": {},
   "source": [
    "### 7.1(d)\n",
    "\n",
    "**Task: Visualise your decision tree and use this to describe how the classification is made.**\n",
    "\n",
    "**Hint:** The decision tree can be visualised using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) or [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html),\n",
    "\n",
    "1. Easiest: Using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html):\n",
    "\n",
    "```python\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "tree.plot_tree(\n",
    "    best_tree,  # The tree to plot\n",
    "    filled=True,  # Add colour to the boxes.\n",
    "    feature_names=features,  # Get name for features.\n",
    "    class_names=class_names,  # Get the name of the different classes.\n",
    ")\n",
    "```\n",
    "\n",
    "2. Looks nicer: Using [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html):\n",
    "\n",
    "```python\n",
    "from sklearn.tree import export_graphviz  # To create the tree.\n",
    "import graphviz  # To turn the three into a graph, you may need to install this (pip install graphviz).\n",
    "from IPython.display import display  # To show the graph.\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    best_tree,  # The tree to plot.\n",
    "    out_file=None,  # Do not write to file.\n",
    "    feature_names=features,  # Get name for features.\n",
    "    class_names=class_names,  # Get the name of the different classes.\n",
    "    rounded=True,  # Show the boxes in the tree with rounded corners.\n",
    "    filled=True,  # Add colour to the boxes.\n",
    ")\n",
    "display(graphviz.Source(dot_data))  # Show the tree in a notebook.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a3836",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(d): What features is the best decision tree using?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7740969",
   "metadata": {},
   "source": [
    "### 7.1(e)\n",
    "\n",
    "**Task: Create a k-nearest neighbours classifier to classify the molecules. Optimise the number of neighbours using cross-validation on a training set. Report the optimal number of neighbours.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the k-nearest neighbours classifier can be done as follows (assuming that you have already split into the training and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 20)}\n",
    "grid_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid_knn.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid_knn.best_score_)\n",
    "print(\"Best parameters\", grid_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e0bca",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(e): What was the optimal number of neighbours?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64831c",
   "metadata": {},
   "source": [
    "### 7.1(f)\n",
    "\n",
    "**Task: Create a random forest classifier to classify molecules. Optimise the number of trees and levels using cross-validation on a training set. Report the optimal number of trees and levels.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the random forest classifier can be done similarly to what you did in [7.1(c)](#7.1(c)) and [7.1(e)](#7.1(e)). You just have to make use of the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and optimise the parameters `n_estimators` and `max_depth`:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 11),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "\n",
    "# ... rest of the optimisation code ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf13c6",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(f): What was the optimal number of estimators and tree depth?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644daa3",
   "metadata": {},
   "source": [
    "### 7.1(g)\n",
    "\n",
    "**Task: Compare the three optimised classifiers you have made by applying them to the test set and obtaining the corresponding confusion matrices. Also compute the [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), and the [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) for the test set. Which classifier performs best?**\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The metrics can be computed as follows:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "y_hat = best_tree.predict(X_test)\n",
    "recall_tree = recall_score(y_test, y_hat)\n",
    "precision_tree = precision_score(y_test, y_hat)\n",
    "bac_tree = balanced_accuracy_score(y_test, y_hat)\n",
    "print(f\"Recall: {recall_tree:.3f}\")\n",
    "print(f\"Precision: {precision_tree:.3f}\")\n",
    "print(f\"Balanced accuracy: {bac_tree:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815245ff",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(g): Which classifier performs best?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880c9d8",
   "metadata": {},
   "source": [
    "### 7.1(h)\n",
    "\n",
    "Explore if you can create an even better classifier using a gradient boosting method (e.g., [XGBoost](https://xgboost.readthedocs.io)) or a foundation model (e.g., [TabPFN](https://github.com/PriorLabs/TabPFN); note that this requires some extra steps for the installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c49438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db893d",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.1(h): Do you get better performance?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b4748",
   "metadata": {},
   "source": [
    "## Exercise 7.2 Preprocessing NIR spectra\n",
    "\n",
    "We will analyse NIR spectra from two distinct Ethiopian [sorghum](https://en.wikipedia.org/wiki/Sorghum) cultivars to determine if they can be differentiated. Specifically, we will examine how different preprocessing techniques impact the outcome of a principal component analysis (PCA) applied to the spectra. \n",
    "\n",
    "**Note:**\n",
    "\n",
    "1. The dataset used in this exercise is derived from [Kosmowski and Worku\n",
    "](https://doi.org/10.1371/journal.pone.0193620) who used a miniaturised NIR spectrometer to identify Ethiopian crop cultivars. To simplify the analysis, we focus on measurements from only two of the ten sorghum cultivars studied in the original work\n",
    "\n",
    "2. This exercise will mainly ask you to run and observe results from already implemented code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ace50",
   "metadata": {},
   "source": [
    "### 7.2(a)\n",
    "\n",
    "The following code performs these steps:\n",
    "\n",
    "1. Load the NIR spectra from the data file [nir.csv](./nir.csv).\n",
    "2. Extracts wavelengths, spectra, and cultivar names.\n",
    "3. Defines colours for plotting cultivars.\n",
    "4. Creates a function to plot spectra by cultivar.\n",
    "5. Creates a function to run a PCA on provided spectra and plot the scores of the first two principal components.\n",
    "6. Initialises a figure for results.\n",
    "7. Plots the original spectra and the PCA results.\n",
    "\n",
    "**Task: Execute the code and observe the generated plot. In the PCA scores plot, are there any noticeable groupings that suggest cultivar separation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f161cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# Load the raw data:\n",
    "data = pd.read_csv(\"nir.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the data\n",
    "variables = [i for i in data.columns if i != \"Cultivator\"]\n",
    "# Wavelengths as numbers:\n",
    "wavelengths = np.array([float(i) for i in variables])\n",
    "print(f\"Number of wavelengths: {len(wavelengths)}\")\n",
    "# All spectra as a data matrix:\n",
    "all_spectra = data[variables].to_numpy()\n",
    "print(f\"Size of data matrix: {all_spectra.shape}\")\n",
    "# Name of the two cultivators:\n",
    "cultivators = data[\"Cultivator\"].unique()\n",
    "print(f\"Cultivators: {cultivators}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d51d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colour mapping for the two cultivators:\n",
    "colors = sns.color_palette(\"colorblind\", n_colors=len(cultivators))\n",
    "color_mapping = {key: colori for key, colori in zip(cultivators, colors)}\n",
    "# Show the two colors\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7974b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectra(data, X, wavelengths, color_mapping, axi, legend=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Plots NIR spectra from the given data matrix X, colour-coded by cultivar.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame containing cultivar information.\n",
    "        X (numpy.ndarray): Matrix of NIR spectra, where each row is a spectrum.\n",
    "        wavelengths (numpy.ndarray): Array of corresponding wavelengths for the spectra.\n",
    "        color_mapping (dict): Dictionary mapping cultivar names to colours.\n",
    "        axi (matplotlib.axes.Axes): Matplotlib Axes object for plotting.\n",
    "        legend (bool, optional): Whether to include a legend. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None (plots directly to the provided Axes object).\n",
    "    \"\"\"\n",
    "    # Initialise empty lists to store legend handles and labels:\n",
    "    handles, labels = [], []\n",
    "    for cultivator in color_mapping.keys():\n",
    "        # Filter spectra belonging to the current cultivar\n",
    "        spectra_cult = X[data[\"Cultivator\"] == cultivator]\n",
    "        color = color_mapping[cultivator]\n",
    "        for spectrum in spectra_cult:\n",
    "            # Plot each spectrum with the assigned color\n",
    "            (linei,) = axi.plot(wavelengths, spectrum, color=color)\n",
    "        # Append the line handle and cultivar label for the legend\n",
    "        handles.append(linei)\n",
    "        labels.append(cultivator)\n",
    "    if legend:\n",
    "        # Add a legend to the plot if 'legend' is True\n",
    "        legend = axi.legend(handles, labels, title=\"Cultivator:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c32b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_plot_scores(data, X, color_mapping, axi):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on the input spectra and plots the scores (colour-coded).\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame containing cultivar information.\n",
    "        X (numpy.ndarray): Matrix of NIR spectra, where each row is a spectrum.\n",
    "        color_mapping (dict): Dictionary mapping cultivar names to colours.\n",
    "        axi (matplotlib.axes.Axes): Matplotlib Axes object for plotting.\n",
    "\n",
    "    Returns:\n",
    "        None (plots directly to the provided Axes object).\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2)  # Initialize PCA with 2 components\n",
    "    scores = pca.fit_transform(X)  # Perform PCA and get the scores\n",
    "    sns.scatterplot(\n",
    "        data=data,\n",
    "        x=scores[:, 0],\n",
    "        y=scores[:, 1],\n",
    "        hue=\"Cultivator\",\n",
    "        palette=color_mapping,\n",
    "        legend=False,\n",
    "        ax=axi,\n",
    "    )\n",
    "    # Calculate explained variance ratios\n",
    "    perc = pca.explained_variance_ratio_ * 100\n",
    "    # Set axis labels with explained variance percentages\n",
    "    axi.set_xlabel(f\"Scores PC1 ({perc[0]:.2f}%)\")\n",
    "    axi.set_ylabel(f\"Scores PC2 ({perc[1]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1, axes1 = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "plot_spectra(\n",
    "    data,\n",
    "    all_spectra,\n",
    "    wavelengths,\n",
    "    color_mapping,\n",
    "    axes1[0],\n",
    "    legend=True,\n",
    ")\n",
    "run_pca_plot_scores(data, all_spectra, color_mapping, axes1[1])\n",
    "\n",
    "axes1[0].set_xlabel(\"Wavelength (nm)\")\n",
    "axes1[0].set_ylabel(\"Absorbance\")\n",
    "axes1[0].set_title(\"Original spectra\", loc=\"left\")\n",
    "axes1[1].set_title(\"PCA, Original spectra\", loc=\"left\")\n",
    "sns.despine(fig=figure1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a696a08",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(a): Is there a clear cultivar separation in the scores plot?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dad47d",
   "metadata": {},
   "source": [
    "### 7.2(b)\n",
    "\n",
    "**Task: Observe the impact of normalisation on the spectra and PCA results. In the PCA scores plot, are there any noticeable groupings that suggest cultivar separation?**\n",
    "\n",
    "**Hint:**\n",
    "1. Apply one of the provided normalisations to scale the spectra, for instance\n",
    "```python\n",
    "normed = normalise_spectra(all_spectra)\n",
    "```\n",
    "2. Plot the normalised spectra and the corresponding PCA results side-by-side. For instance,\n",
    "```python\n",
    "figure2, axes2 = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "plot_spectra(data, normed, wavelengths, color_mapping, axes2[0], legend=True)\n",
    "run_pca_plot_scores(data, normed, color_mapping, axes2[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "def normalise_spectra(spectra):\n",
    "    \"\"\"Normalise the given spectra to the range [-1, 1].\"\"\"\n",
    "    s_min = spectra.min(axis=1, keepdims=True)\n",
    "    s_max = spectra.max(axis=1, keepdims=True)\n",
    "    return 2 * (spectra - s_min) / (s_max - s_min) - 1\n",
    "\n",
    "\n",
    "def vector_norm(spectra):\n",
    "    \"\"\"Norm each row to a length of 1.\"\"\"\n",
    "    scaler = Normalizer(norm=\"l2\")\n",
    "    return scaler.fit_transform(spectra)\n",
    "\n",
    "\n",
    "def snv(spectra):\n",
    "    \"\"\"Normalise by standardising each row: (x - mean) / std, per row.\"\"\"\n",
    "    return (spectra - np.mean(spectra, axis=1, keepdims=True)) / np.std(\n",
    "        spectra, axis=1, keepdims=True\n",
    "    )\n",
    "\n",
    "\n",
    "def max_peak_normalisation(spectra):\n",
    "    \"\"\"Normalise spectra so that the max for each spectrum is at 1.\"\"\"\n",
    "    return spectra / np.max(spectra, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80184e",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(b): Is there a clear cultivar separation in the scores plot?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82fb73",
   "metadata": {},
   "source": [
    "### 7.2(c)\n",
    "\n",
    "**Task: Observe the impact of multiplicative scatter correction (MSC) on the spectra and PCA results. In the PCA scores plot, are there any noticeable groupings that suggest cultivar separation?**\n",
    "\n",
    "**Hint:**\n",
    "1. Apply the provided MSC function to correct the spectra, for instance,\n",
    "```python\n",
    "corrected = multiplicative_scatter_correction(all_spectra)\n",
    "```\n",
    "2. Plot the corrected spectra and the corresponding PCA results side-by-side. For instance,\n",
    "```python\n",
    "figure3, axes3 = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "plot_spectra(data, corrected, wavelengths, color_mapping, axes3[0], legend=True)\n",
    "run_pca_plot_scores(data, corrected, color_mapping, axes3[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplicative_scatter_correction(spectra):\n",
    "    \"\"\"\n",
    "    Applies Multiplicative Scatter Correction (MSC) to the input spectra.\n",
    "\n",
    "    MSC is a preprocessing technique used to reduce the effects of scatter in spectral data.\n",
    "    It corrects for variations in path length and particle size, which can affect the\n",
    "    baseline and slope of the spectra.\n",
    "\n",
    "    Args:\n",
    "        spectra (numpy.ndarray): Matrix of spectra, where each row is a spectrum.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: MSC-corrected spectra matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.mean(spectra, axis=0)  # Calculate the mean spectrum\n",
    "    msc_spectra = np.zeros_like(\n",
    "        spectra\n",
    "    )  # Initialise an array to store MSC-corrected spectra\n",
    "    for i, spectrum in enumerate(spectra):\n",
    "        # Fit a linear regression model to each spectrum against the mean spectrum\n",
    "        param = np.polyfit(mean, spectrum, 1)\n",
    "        # Apply the MSC correction: (spectrum - intercept) / slope\n",
    "        msc_spectra[i] = (spectrum - param[1]) / (param[0])\n",
    "    return msc_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62871c9",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(c): Is there a clear cultivar separation in the scores plot?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab6e77",
   "metadata": {},
   "source": [
    "### 7.2(d)\n",
    "\n",
    "**Task: Investigate the impact of applying a second derivative transformation on the spectra and PCA results. In the PCA scores plot, are there any noticeable groupings that suggest cultivar separation?**\n",
    "\n",
    "**Hint:**\n",
    "1. Use the provided code to calculate the second derivative of the original spectra, for instance,\n",
    "\n",
    "```python\n",
    "dspectra = derivative(wavelengths, all_spectra, deriv=2)\n",
    "```\n",
    "2. Plot the resulting second derivative spectra and the corresponding PCA results side-by-side. For instance,\n",
    "```python\n",
    "figure4, axes4 = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "plot_spectra(data, dspectra, wavelengths, color_mapping, axes4[0], legend=True)\n",
    "run_pca_plot_scores(data, dspectra, color_mapping, axes4[1])\n",
    "```\n",
    "\n",
    "**Note:** The derivative is computed using the [Savitzky-Golay filter](https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter). This method smooths the data by fitting a polynomial to a moving window of points and then calculating the derivative of that fitted polynomial. The method, as implemented here, assumes evenly spaced data points. It may produce inaccurate results if your wavelengths are unevenly spaced. In such cases, alternative methods like B-spline derivatives or other interpolation-based approaches might be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a283d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def derivative(wavelengths, spectra, window_length=21, polyorder=3, deriv=2):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of the input spectra using the Savitzky-Golay filter.\n",
    "\n",
    "    This function applies the Savitzky-Golay filter to smooth and differentiate the\n",
    "    input spectra. The filter is used to reduce noise and enhance spectral features.\n",
    "\n",
    "    Args:\n",
    "        wavelengths (numpy.ndarray): Array of wavelengths corresponding to the spectra.\n",
    "        spectra (numpy.ndarray): Matrix of spectra, where each row is a spectrum.\n",
    "        window_length (int): The length of the filter window (must be odd).\n",
    "        polyorder (int): The order of the polynomial used to fit the samples.\n",
    "        deriv (int, optional): The order of the derivative to compute. Defaults to 2 (second derivative).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Matrix of derivative spectra.\n",
    "    \"\"\"\n",
    "    # Apply Savitzky-Golay filter to calculate the derivative:\n",
    "    delta_w = wavelengths[1] - wavelengths[0]\n",
    "    dspectra = savgol_filter(\n",
    "        spectra,\n",
    "        window_length,\n",
    "        polyorder,\n",
    "        deriv=deriv,\n",
    "        delta=delta_w,  # Wavelength spacing\n",
    "        mode=\"nearest\",  # Extrapolation mode at the edges\n",
    "        axis=1,  # Process each row\n",
    "    )\n",
    "    return dspectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a567",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(d): Is there a clear cultivar separation in the scores plot?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6112e1",
   "metadata": {},
   "source": [
    "### 7.2(e)\n",
    "\n",
    "**Task: Explain how the Savitzky-Golay filter uses polynomial fitting to smooth data and compute derivatives.**\n",
    "\n",
    "**Hint:** See page 149 in our textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d4a78",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(e): Your explanation for Savitzky-Golay filtering?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f980a",
   "metadata": {},
   "source": [
    "### 7.2(f)\n",
    "\n",
    "**Task: The figure below displays the results of the preprocessing steps from exercise [7.2(a)](#7.2(a)) to [7.2(d)](#7.2(d)). Based on these results, which preprocessing method appears most promising for building a classifier?**\n",
    "\n",
    "![Preprocessing NIR results](results7.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff977844",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(f): Which preprocessing step appears most promising?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e71aa6",
   "metadata": {},
   "source": [
    "### 7.2(g)\n",
    "\n",
    "**Task: Assuming that the largest variation in the raw data is due to scattering effects, we could assume that PCA should pick up on this in the first (and perhaps second) principal component. If you go back to [7.2(a)](#7.2(a)), will the scores plot look more promising if you use principal components 2 and 3?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fbef5",
   "metadata": {},
   "source": [
    "#### Your answer to question 7.2(g): Will using other principal components improve the separation in the scores plot?\n",
    "*Double click here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
