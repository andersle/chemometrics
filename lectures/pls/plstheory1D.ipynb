{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e2b5f9",
   "metadata": {},
   "source": [
    "# Partial least squares regression \"by hand\" for 1D $\\mathbf{y}$ = PLS1\n",
    "\n",
    "Here, we will find the latent variables in partial least squares (PLS) regression by hand. As an\n",
    "example, we will consider the solubility data again, but we will use:\n",
    "\n",
    "- the number of hydrogen atoms and carbon atoms as the X-variables\n",
    "\n",
    "- the sum of hydrogen atoms and carbon atoms as the y-variable.\n",
    "\n",
    "In this example, y consists of a single variable. PLS performed when y is a single variable is also\n",
    "referred to as PLS1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af7e74",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d24b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import svd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61976b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"solubility_descriptors.csv.zip\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nC + nH\"] = data[\"nC\"] + data[\"nH\"]\n",
    "\n",
    "xvars = [\n",
    "    \"nC\",\n",
    "    \"nH\",\n",
    "]\n",
    "yvars = [\"nC + nH\"]\n",
    "\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "\n",
    "y = scaler_y.fit_transform(data[yvars].to_numpy())\n",
    "X = scaler_x.fit_transform(data[xvars].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a6675",
   "metadata": {},
   "source": [
    "### Plotting X- and Y-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(X[:, 0], X[:, 1], y)\n",
    "ax.set(xlabel=xvars[0], ylabel=xvars[1], zlabel=yvars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a688800",
   "metadata": {},
   "source": [
    "## The PLS method\n",
    "\n",
    "In PLS, we are looking for scores $\\mathbf{t}$ (for X) so that the covariance with $\\mathbf{y}$\n",
    "is maximised. The main idea is that the scores should explain the variance in X and the\n",
    "covariance between X and y.\n",
    "\n",
    "The covariance can be calculated by $\\mathbf{t}^\\top \\mathbf{y}$.\n",
    "We shall see later how we can find the directions in X and Y to maximise the covariance, but let us assume that we have found them. In PLS, these directions are referred to as the *weights* $\\mathbf{w}$ (for X).\n",
    "We can use these *weights* to calculate the scores:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{t} = \\mathbf{X} \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "Assume that we will predict $\\mathbf{y}$ from some measured $\\mathbf{X}$. \n",
    "Ideally, we would have a perfect correlation between the scores so\n",
    "that $\\mathbf{y} = g \\mathbf{t}$ where $g$ is some number.\n",
    "But in general, this is not the case. Instead, we will approximate $\\mathbf{y}$ from $\\mathbf{t}$ using\n",
    "this relation. The least squares approximation for $g$ is,\n",
    "\n",
    "\\begin{equation}\n",
    "g = \\frac{\\mathbf{t}^\\top \\mathbf{y}}{\\mathbf{t}^\\top \\mathbf{t}},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "so that the y approximated from the X-scores are $\\hat{\\mathbf{y}} = g \\mathbf{t}$.\n",
    "We can rewrite this as,\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = g \\mathbf{t} = g \\mathbf{X} \\mathbf{w},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and comparing with a linear equation on the form $\\mathbf{y} = \\mathbf{X} \\mathbf{b}_\\text{PLS}$ with regression\n",
    "coefficients $b_\\text{PLS}$ we see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b}_\\text{PLS} = g \\mathbf{w} .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81cd53",
   "metadata": {},
   "source": [
    "## A short example\n",
    "\n",
    "We set set $\\mathbf{w} = (0.0, 1.0)^\\top$, then \n",
    "the product $\\mathbf{X} \\mathbf{w}$ will just pick out\n",
    "the second column of $\\mathbf{X}$. Then a plot of\n",
    "$\\mathbf{t}$ vs $\\mathbf{y}$ will show how the sum of carbon and hydrogen atoms is correlated with the number of hydrogen atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(X, y, w):\n",
    "    \"\"\"Plot X and t vs. y (calculated using w)\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        constrained_layout=True,\n",
    "        ncols=2,\n",
    "        figsize=(8, 4),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    axes[0].scatter(X[:, 0], X[:, 1])\n",
    "    axes[0].set(xlabel=xvars[0], ylabel=xvars[1], title=\"X\")\n",
    "\n",
    "    vecx = w.flatten()\n",
    "\n",
    "    axes[0].quiver(\n",
    "        0,\n",
    "        0,\n",
    "        vecx[0],\n",
    "        vecx[1],\n",
    "        color=\"black\",\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.2,\n",
    "        width=0.015,\n",
    "    )\n",
    "\n",
    "    t = X @ (w / norm(w))\n",
    "\n",
    "    cov = t.T @ y\n",
    "\n",
    "    axes[1].scatter(t[:, 0], y[:, 0])\n",
    "    axes[1].set(\n",
    "        xlabel=\"X-scores (t)\", ylabel=\"y\", title=f\"Covariance: {cov[0][0]:.4g}\"\n",
    "    )\n",
    "    sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights by hand:\n",
    "w = np.array([0.0, 1.0])\n",
    "w = w.reshape(2, -1)  # Make it a column vector\n",
    "print(f\"w.T = {w.T}, shape of w: {w.shape}\")\n",
    "\n",
    "make_plot(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78880d",
   "metadata": {},
   "source": [
    "The X-data is projected onto the black vector in the plot above, giving the X-scores. The rightmost plot shows the X-scores plotted against y. Let us calculate the regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_coefficients(X, y, w):\n",
    "    \"\"\"Calculates the regression coefficients given w.\"\"\"\n",
    "    t = X @ w\n",
    "    # Find g (approximate u from t):\n",
    "    g = t.T @ y / (t.T @ t)\n",
    "    # Find b:\n",
    "    B = g * w\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, y, w)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68425e3c",
   "metadata": {},
   "source": [
    "In the regression coefficients above, we see that the first row is just zero. Effectively this means that we\n",
    "are predicting y using only the number of hydrogens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=f\"R² = {r2_score(y, y_hat):.3f}\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"y\", ylabel=\"ŷ\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc471b3",
   "metadata": {},
   "source": [
    "## Maximizing the covariance\n",
    "\n",
    "The covariance between $\\mathbf{t}$ and $\\mathbf{y}$ is\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{t}^\\top \\mathbf{y} = (\\mathbf{X} \\mathbf{w})^\\top (\\mathbf{y})\n",
    "= \\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "We set $\\mathbf{v} = \\mathbf{X}^\\top \\mathbf{y}$. What vector $\\mathbf{w}$ will\n",
    "maximize the dot product $\\mathbf{w}^\\top \\mathbf{v}$? If we make $\\mathbf{w}$ parallel\n",
    "to $\\mathbf{v}$, then the dot product will be maximized! So we can set: $\\mathbf{w} = \\lambda \\mathbf{v} =\n",
    "\\lambda \\mathbf{X}^\\top \\mathbf{y}$ for some number $\\lambda$. If we simultaneously normalize $\\mathbf{w}$,\n",
    "then we don't need to find $\\lambda$ (it will drop out in the normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5455b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = X.T @ y\n",
    "w /= norm(w)\n",
    "make_plot(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, y, w)\n",
    "print(B)\n",
    "y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=f\"R² = {r2_score(y, y_hat):.3f}\")\n",
    "ax.set(xlabel=\"y\", ylabel=\"ŷ\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acdc76",
   "metadata": {},
   "source": [
    "## Comparing with `PLSRegression` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLSRegression(scale=False, n_components=1)\n",
    "model.fit(X, y)\n",
    "make_plot(X, y, model.x_weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B / model.coef_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64ef5a",
   "metadata": {},
   "source": [
    "## Finding the next latent variables\n",
    "\n",
    "So far, we have found one latent variable. The strategy to find the next PLS components is\n",
    "to essentially repeat the process, but we first \"subtract\" the latent variable we just found\n",
    "from $\\mathbf{X}^T$ and $\\mathbf{y}$. This is referred to as deflation.\n",
    "In addition, we make sure that the next latent variable\n",
    "we find will have scores orthogonal to the previous one. A consequence of this is\n",
    "that loadings and scores we find in the next steps of the method do not\n",
    "directly operate on $\\mathbf{X}$, but on the deflated versions of this one.\n",
    "\n",
    "It would be nice to have loadings that we could apply directly \n",
    "to $\\mathbf{X}$ (and not the deflated version).\n",
    "PLS methods will typically calculate these as well, and in `sklearn`, they\n",
    "are called *rotations*. We will use these later for the interpretation of correlations between\n",
    "variables.\n",
    "\n",
    "Finally, here is an example of an implementation of the PLS regression method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pls1(X, y, n_components=2):\n",
    "    \"\"\"PLS1 regression given X and y.\"\"\"\n",
    "    Xk, yk = np.copy(X), np.copy(y)\n",
    "\n",
    "    W = np.zeros((Xk.shape[1], n_components))\n",
    "    P = np.zeros((Xk.shape[1], n_components))\n",
    "    T = np.zeros((Xk.shape[0], n_components))\n",
    "    G = np.zeros((1, n_components))\n",
    "\n",
    "    for i in range(n_components):\n",
    "        wxi = Xk.T @ yk\n",
    "        wxi /= norm(wxi)\n",
    "        # For convention, make largest value in wxi positive:\n",
    "        idx = np.argmax(abs(wxi))\n",
    "        sign = np.sign(wxi[idx])\n",
    "        wxi *= sign\n",
    "        # Store X-weights\n",
    "        W[:, i] = wxi.flatten()\n",
    "\n",
    "        # Calculate scores:\n",
    "        t = Xk @ wxi\n",
    "        # Calculate the predicted y:\n",
    "        g = t.T @ yk / (t.T @ t)\n",
    "        # Store regression coefficients\n",
    "        G[:, i] = g\n",
    "        # Subtract the predicted y:\n",
    "        yk = yk - g * t\n",
    "        # Calculate loadings to remove the part of\n",
    "        # X we have described with t:\n",
    "        p = (Xk.T @ t) / (t.T @ t)\n",
    "        Xk = Xk - t @ p.T\n",
    "        # Store X-loadings and X-scores:\n",
    "        P[:, i] = p.flatten()\n",
    "        T[:, i] = t.flatten()\n",
    "\n",
    "    R = W @ np.linalg.pinv(P.T @ W)  # Rotations, can be used for T = X @ R\n",
    "    B = R @ G.T  # Regression coefficients, for Ŷ = X @ B\n",
    "    return W, P, T, R, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test the method above by comparing with sklearn:\n",
    "xvars = [\"MolLogP\", \"nAtom\", \"Polar Surface Area\", \"MW\"]\n",
    "yvars = [\"Molecular Weight\"]\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "\n",
    "y = scaler_y.fit_transform(data[yvars].to_numpy())\n",
    "X = scaler_x.fit_transform(data[xvars].to_numpy())\n",
    "\n",
    "model = PLSRegression(scale=False, n_components=4)\n",
    "model.fit(X, y)\n",
    "W, P, T, R, B = run_pls1(X, y, n_components=model.n_components)\n",
    "print(np.allclose(R, model.x_rotations_))\n",
    "print(np.allclose(W, model.x_weights_))\n",
    "print(np.allclose(P, model.x_loadings_))\n",
    "print(np.allclose(T, model.x_scores_))\n",
    "print(np.allclose(B, model.coef_.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
