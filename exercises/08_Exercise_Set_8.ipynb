{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise set 8**\n",
    "==============\n",
    "\n",
    "> The goal of this exercise is to perform **principal component analysis**,\n",
    "> **clustering**, and **classification** on a data set with many variables.\n",
    "\n",
    "\n",
    "**Exercise 8.1**\n",
    "\n",
    "In this exercise, we will explore the \"wine dataset\" which is a common example\n",
    "dataset used for classification. The dataset contains the results of\n",
    "a chemical analysis of wines from the same region in Italy, using grapes grown\n",
    "by three different cultivators. In this first exercise, we will explore this\n",
    "dataset using principal component analysis.\n",
    "\n",
    "\n",
    "**(a)** Begin by exploring the raw data. Here, you should choose\n",
    "the method yourself. You can, for instance, look at histograms of the\n",
    "different measured quantities, correlations between the quantities,\n",
    "or other plots of the raw data. It can also be useful to explore\n",
    "statistical properties like averages and standard deviations. The Python\n",
    "code in the following cells can be used to load the data set, \n",
    "and it will print out some summaries of the raw data which you may find\n",
    "helpfull for your exploration\n",
    "\n",
    "After looking at the raw data, are there some of the variables that seem to be able to distinguish between the wines produced by the different cultivators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and print some info.\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_set = load_wine()\n",
    "# Print out some information about the data set:\n",
    "print('Variables in the data set:')\n",
    "for i in data_set['feature_names']:\n",
    "    print(i)\n",
    "print('\\nClasses in the data set (cultivators):')\n",
    "for i in data_set['target_names']:\n",
    "    print(i)\n",
    "# Convert the data set into a pandas DataFrame:\n",
    "data = pd.DataFrame(data_set['data'], columns=data_set['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a table with a summary for each variable:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the class information:\n",
    "class_data = data_set['target']\n",
    "class_names = dict(enumerate(data_set['target_names']))\n",
    "variable = 'color_intensity'\n",
    "for class_id, class_name in class_names.items():\n",
    "    print(f'\\nInformation about \"{variable}\" for \"{class_name}\"')\n",
    "    idx = np.where(class_data == class_id)[0]\n",
    "    data_class = data.loc[idx, variable]\n",
    "    print(data_class.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.1(a)**: *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Perform a PCA on the data set and plot the explained variance as a function\n",
    "of the number of principal components. Do you need to scale your data before performing\n",
    "PCA in this case (why/why not)? How many principal components are needed to explain 95%\n",
    "of the variance in the data? The following code cell can be used to run the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and run PCA.\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_set = load_wine()\n",
    "data = pd.DataFrame(data_set['data'], columns=data_set['feature_names'])\n",
    "X = data\n",
    "# Uncomment the following line to scale your data:\n",
    "#X = scale(data)\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)\n",
    "# Print out the percentage of variance explained by each component:\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.1(b)**: *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Rerun the PCA with\n",
    "the number of components you found in the previous question.\n",
    "This can be done by defining argument `n_components` to \n",
    "`PCA()`, e.g.: `pca = PCA(n_components=13)`.\n",
    "\n",
    "Obtain the scores, and make a plot of the scores for\n",
    "principal component 1 (on the x-axis) and principal component 2 (on the y-axis).\n",
    "\n",
    "Do you see any grouping(s) (\"clusters\") in your scores plot?\n",
    "Here, you can choose to color the scores according\n",
    "to the class they belong to (i.e. by using the class\n",
    "data available in the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.1(c):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** \n",
    "Explore the loadings for your PCA model by plotting the\n",
    "loadings for the variables (on principal component 1 and\n",
    "principal component 2). Do any of the variables seem to be correlated?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.1(d):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** \n",
    "Save the scores you have obtained to a new file.\n",
    "We will use this information in the next part\n",
    "of the exercise, where we will try to find clusters in our data.\n",
    "\n",
    "Saving the scores can be done with `pandas` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set, run PCA and save scores.\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_set = load_wine()\n",
    "data = pd.DataFrame(data_set['data'], columns=data_set['feature_names'])\n",
    "X = data\n",
    "# Uncomment the following line to scale your data:\n",
    "#X = scale(data)\n",
    "pca = PCA(n_components=5)\n",
    "scores = pca.fit_transform(X)\n",
    "# Create variable names for the principal components:\n",
    "pc_name = [f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "# Create a DataFrame from the scores:\n",
    "scores_data = pd.DataFrame(scores, columns=pc_name)\n",
    "# Save the scores to a comma separated values-file:\n",
    "scores_data.to_csv('scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8.2**\n",
    "We will continue exploring the \"wine dataset\". We will pretend that we do not\n",
    "know that there are 3 classes in the dataset, and we will investigate\n",
    "what the `KMeans` clustering method can tell us about it. For this\n",
    "exercise, it is a good idea to read through all points below before\n",
    "starting, as you will find a link to a specific example you can use \n",
    "to answer most of the questions.\n",
    "\n",
    "**(a)** Explain the steps in the `KMeans` clustering algorithm.\n",
    "How can we use this algorithm without knowing how many clusters\n",
    "there are in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.2(a):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "Run `KMeans` clustering on the wine dataset. Here, you will have to\n",
    "select a set of numbers of clusters to look for. (Limit yourself to\n",
    "a maximum of 10 clusters) After running the clustering for your \n",
    "data, plot the sum of squared distances of samples to their closest\n",
    "cluster center, as a function of the number of clusters considered. \n",
    "\n",
    "Explain briefly how this plot can be used to identify the \"correct\"\n",
    "number of clusters. \n",
    "\n",
    "How many clusters would you say there are in the\n",
    "dataset, based on this plot alone?\n",
    "      \n",
    "To get you started, the cell below has some Python code that can be used to run the\n",
    "clustering and store the results (see also the [silhouette example](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html))\n",
    "\n",
    "Note that the `cluster_km` object contains the following results as attributes:\n",
    " * `cluster_centers_`: Coordinates of cluster centers.\n",
    " * `labels_`: Labels of each point.\n",
    " * `inertia_`: Sum of squared distances of samples to their closest cluster center.\n",
    " * `n_iter_`: Number of iterations run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and run KMeans.\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_set = load_wine()\n",
    "data = pd.DataFrame(data_set['data'], columns=data_set['feature_names'])\n",
    "X = scale(data)\n",
    "# Define a set of numbers of clusters to run KMeans for:\n",
    "number_of_clusters = [2, 5]\n",
    "# Set up variables for storing the results\n",
    "results = []  # Results for the clustering\n",
    "yfit = []  # Predicted clusters for data points in X\n",
    "for i in number_of_clusters:\n",
    "    cluster_km = KMeans(\n",
    "        n_clusters=i,\n",
    "        init='k-means++',\n",
    "    )\n",
    "    y = cluster_km.fit_predict(X)\n",
    "    results.append(cluster_km)\n",
    "    yfit.append(y)\n",
    "# Print out some results:\n",
    "print('Sum of squared distances of samples to their closest cluster center:')\n",
    "for i, result in zip(number_of_clusters, results):\n",
    "    print('Clusters: {}: {}'.format(i, result.inertia_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.2(b):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** \n",
    "A general method that can be used to assess the clustering, \n",
    "is the silhouette method. This method calculates a silhouette \n",
    "value for each object which is a measure of how similar the \n",
    "object is to the cluster it belongs to (cohesion) compared to\n",
    "other clusters (separation). This is rather easy to calculate \n",
    "with `sklearn` as there is a method to do just so: \n",
    "[`silhouette_samples`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html) from the module `sklearn.metrics`.\n",
    "\n",
    "Do the following:\n",
    " * For each clustering you have considered, i.e. for each number of clusters you tried,\n",
    "   calculate the silhouette values.\n",
    " * Plot the average silhouette value as a function of the number of clusters considered.\n",
    " * For each clustering, plot the silhouette values grouped into clusters. Say, if you,\n",
    "   for instance, considered 4 clusters in one of your clusterings, plot the silhouette\n",
    "   values for each of these 4 clusters. An example of how to do this is available on\n",
    "   the website\n",
    "   of [`sklearn`](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
    "   \n",
    "Using these results (average silhouette values) and the plots of silhouette values, what\n",
    "is the best number of clusters to use? How does this compare with what we already know -\n",
    "that the samples come from 3 different cultivators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.2(c):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** \n",
    "Rerun your analysis on the scores you stored in the last point of the PCA part.\n",
    "But use only the scores from principal components 1 and 2.\n",
    "Do the results from this analysis differ from the cluster analysis on the full data set?\n",
    "\n",
    "**Note:** As we only consider two of the principal components here, we have 2D-data. This\n",
    "means that we can plot the clusters more easily. If you are curious, plot the\n",
    "scores for principal components 1 and 2 and color the points according to the\n",
    "clustering results you have obtained. Here, you can also show the centers of the\n",
    "clusters by using the `cluster_centers_` attribute of the `KMeans` object you have\n",
    "used. This part of the exercise also shows that PCA can be used as an initial \n",
    "method to reduce the dimensionality of the original problem. We have here \n",
    "combined PCA and KMeans to solve a clustering problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.2(d):** *Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8.3: LDA Example**\n",
    "\n",
    "Both PCA and KMeans are examples of unsupervised methods - we did not use the class information available to us to\n",
    "find clusters in our data.\n",
    "[Linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "on the other hand, is a supervised method that uses the class\n",
    "information for *classification*. In your own words, how would you\n",
    "describe the difference between *classification* and\n",
    "*clustering*?\n",
    "\n",
    "LDA is similar to PCA, but rather than looking for latent variables that maximize\n",
    "the covariance in our data, we rather look for latent variables that maximize the \n",
    "*class separation*.\n",
    "Below, you will find a small script that will run LDA on the\n",
    "wine data set. Run this script and observe the results.\n",
    "\n",
    "Note here the difference when we train the LDA\n",
    "model: `X_trans = lda.fit_transform(X, y)`.\n",
    "We are supplying the \"y\" values (i.e. the classes) which is what we expect\n",
    "for a supervised method.\n",
    "\n",
    "For the curious student: Apply LDA to the 2D-example dataset from exercise 7,\n",
    "where we investigated classification by PCA. Does this classification differ from\n",
    "the simple rule we found there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and run LDA.\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.cm import tab10\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.style.use('seaborn-talk')\n",
    "\n",
    "\n",
    "data_set = load_wine()\n",
    "data = pd.DataFrame(data_set['data'], columns=data_set['feature_names'])\n",
    "X = scale(data)\n",
    "y = data_set['target']  # load the class information\n",
    "# Run LDA:\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_trans = lda.fit_transform(X, y)\n",
    "print('Number of classes:', len(lda.classes_))\n",
    "# Predict classes for our original points:\n",
    "y_hat = lda.predict(X)\n",
    "\n",
    "# Plot the explained variance:\n",
    "fig1, ax1 = plt.subplots(constrained_layout=True)\n",
    "comp = list(range(1, len(lda.explained_variance_ratio_) + 1))\n",
    "ax1.bar(\n",
    "    comp,\n",
    "    lda.explained_variance_ratio_,\n",
    "    label='Variance explained by component'\n",
    ")\n",
    "ax1.plot(\n",
    "    [0] + comp,\n",
    "    [0] + list(np.cumsum(lda.explained_variance_ratio_)),\n",
    "    color='black',\n",
    "    marker='o',\n",
    "    label='Cumulative variance explained'\n",
    ")\n",
    "ax1.set_xticks([0] + comp)\n",
    "ax1.set(xlabel='LDA component', ylabel='Ratio of variance explained')\n",
    "ax1.legend()\n",
    "ax1.axhline(y=1, ls=':', color='black', alpha=0.8)\n",
    "\n",
    "# Plot the transformed X, this is similar to the scores found in PCA:\n",
    "fig2, ax2 = plt.subplots(constrained_layout=True)\n",
    "for i in np.unique(y_hat):\n",
    "    ax2.scatter(\n",
    "        X_trans[y_hat == i, 0],\n",
    "        X_trans[y_hat == i, 1],\n",
    "        color=tab10.colors[i],\n",
    "        s=150\n",
    "    )\n",
    "ax2.set(xlabel='LDA component 1', ylabel='LDA component 2')\n",
    "# Plot the centers of the clusters found:\n",
    "for center in lda.transform(lda.means_):\n",
    "    ax2.scatter(\n",
    "        center[0],\n",
    "        center[1],\n",
    "        s=250,\n",
    "        color='black',\n",
    "        marker='X',\n",
    "        edgecolor='white'\n",
    "    )\n",
    "\n",
    "# Now, in order to plot the regions, we would like to have 2D data.\n",
    "# The classification we have right now, expects 13 variables to\n",
    "# classify samples. We therefore run a second LDA on the LDA we\n",
    "# already have performed:\n",
    "lda2 = LinearDiscriminantAnalysis()\n",
    "X_trans2 = lda2.fit_transform(X_trans, y_hat)\n",
    "y_hat2 = lda2.predict(X_trans)\n",
    "# Show the regions:\n",
    "fig3, ax3 = plt.subplots(constrained_layout=True)\n",
    "X_set = X_trans\n",
    "X1, X2 = np.meshgrid(\n",
    "    np.linspace(X_trans[:, 0].min() - 1, X_trans[:, 0].max() + 1, 500),\n",
    "    np.linspace(X_trans[:, 1].min() - 1, X_trans[:, 1].max() + 1, 500)\n",
    ")\n",
    "Z = lda2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
    "ax3.contourf(X1, X2, Z, alpha = 0.3,\n",
    "             cmap=ListedColormap(tab10.colors[:3]))\n",
    "# Add the original samples:\n",
    "for i in np.unique(y_hat2):\n",
    "    ax3.scatter(\n",
    "        X_trans2[y_hat2 == i, 0],\n",
    "        X_trans2[y_hat2 == i, 1],\n",
    "        color=tab10.colors[i],\n",
    "        s=150\n",
    "    )\n",
    "ax3.set(xlabel='LDA component 1', ylabel='LDA component 2')\n",
    "# Plot the centers of the clusters found:\n",
    "for center in lda2.transform(lda2.means_):\n",
    "    ax3.scatter(\n",
    "        center[0],\n",
    "        center[1],\n",
    "        s=250,\n",
    "        color='black',\n",
    "        marker='X',\n",
    "        edgecolor='white'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 8.3:** *(Double click here)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
