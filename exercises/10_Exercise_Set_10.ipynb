{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 10\n",
    "\n",
    "\n",
    ">The goal of this exercise is to gain familiarity with some\n",
    "classification methods and the different ways we can assess and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.1\n",
    "\n",
    "\n",
    "In this exercise, we will consider the\n",
    "[UCI ML Breast Cancer Wisconsin (Diagnostic) dataset](https://goo.gl/U2Uwz2).\n",
    "\n",
    "This data set contains 569 tumours classified\n",
    "as malignant or benign. In addition, 30 variables have been\n",
    "measured, and our goal is to make a predictive model that\n",
    "can classify new tumours as malignant or benign.\n",
    "\n",
    "An overview of the different variables can be found\n",
    "on the \n",
    "[scikit-learn website](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset).\n",
    "In the following, we are going to label the two classes as:\n",
    "\n",
    "* `benign` as a negative ($-1$), and\n",
    "* `malignant` as a positive ($+1$). \n",
    "\n",
    "\n",
    "In the lectures, we mentioned categorical variables and that we might have to\n",
    "transform these to use them in practice.\n",
    "[Dummy variables](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) and\n",
    "[one-hot encoding](https://en.wikipedia.org/wiki/One-hot) are examples of such transformations.\n",
    "In scikit-learn, we do normally not have to worry about this for the y-values we use in classification.\n",
    "For instance, the\n",
    "[scikit-learn documentation for decision trees](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "says that a decision tree \n",
    "> is capable of both binary (where the labels are $[-1, 1]$) classification and multiclass (where the labels are \n",
    "$[0, \\ldots, K-1]$) classification\n",
    "\n",
    "so we use the values $-1$ and $+1$ to represent the two classes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(a) \n",
    "\n",
    "Begin by loading the raw data and creating\n",
    "a test set using $33$\\% of the available data points for the test set.\n",
    "The example code below can be used to load the data set\n",
    "and create training/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data[\"data\"]\n",
    "# \"Rename\" y so that -1 = benign and 1 = malignant:\n",
    "y = [-1 if i == 1 else 1 for i in data[\"target\"]]\n",
    "class_names = [\"benign\", \"malignant\"]\n",
    "print(\"Classes:\")\n",
    "print(class_names)\n",
    "\n",
    "print(\"Variables:\", data[\"feature_names\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    # stratify=y, # Uncomment if you are using stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the training/test sets we use the method\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "from the module [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). One of the input parameters to `train_test_split` is\n",
    "`stratify`:\n",
    "\n",
    "\n",
    "* (i) Reading the documentation for\n",
    "  [stratification](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)\n",
    "  (or the [Wikipedia entry on stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling))\n",
    "  can you explain what `stratify` does?\n",
    "\n",
    "\n",
    "* (ii)  Should we use `stratify` here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(a):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(b)\n",
    "In this case, we have to determine what\n",
    "metric we are going to use\n",
    "to judge the performance of the classifiers we make. Before selecting\n",
    "a metric, we should consider what false positives and false negatives\n",
    "mean for our current problem: How would you define these two terms in our present\n",
    "case, and would you say that false positives are a more serious mistake than false negatives here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(b):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(c)\n",
    "Following up on the previous question, here\n",
    "are some [metrics](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) we could use to assess the performance of a\n",
    "classifier model we make:\n",
    "\n",
    "\n",
    "* **Precision**: The ratio of true positives to the sum of\n",
    "  true positives and false positives.\n",
    "\n",
    "\n",
    "* **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "\n",
    "* **F1**: The (harmonic) mean of the precision and recall.\n",
    "\n",
    "\n",
    "In addition, we can summarize the performance using the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "\n",
    "(Note: There are many [other possibilities](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "as well! If you are curious, you can, for instance, include the\n",
    "*accuracy* (the ratio of correct predictions\n",
    "to the number of total predictions).)\n",
    "\n",
    "\n",
    "The choice of the metric for assessing a classifier will lead to different results.\n",
    "For instance, if we choose to use precision as our metric, we will maximize it\n",
    "during the optimization of our model. This means that we will *minimize* the\n",
    "number of *false positives*. If we choose to use the recall, on the other hand,\n",
    "we will *minimize* the number of *false negatives*.\n",
    "\n",
    "\n",
    "In the following, we will calculate all these metrics for the\n",
    "different classification methods we consider. At the end of the\n",
    "exercise, you will be asked to compare the different classifiers\n",
    "using them. But before we do that: \n",
    "Which of the\n",
    "metrics mentioned above is most important for\n",
    "our classification task?\n",
    "\n",
    "(Note: There is no single correct answer here: it depends on how you judge the seriousness of false positives vs false negatives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(c):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(d)\n",
    "Create a [$k$-nearest neighbour classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " with 3 neighbours and\n",
    "fit it using your training set. Evaluate (with the test set) the classifier using the\n",
    "precision, recall, and F1 metrics, and plot the confusion matrix.\n",
    "\n",
    "An\n",
    "example of how this can be done is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a classifier:\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier on the training set:\n",
    "knn3.fit(X_train, y_train)\n",
    "\n",
    "# Use classifier for prediction for the test set:\n",
    "y_hat = knn3.predict(X_test)\n",
    "\n",
    "# Calculate the precision etc. for the test set:\n",
    "precision = precision_score(y_test, y_hat)\n",
    "recall = recall_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print(f\"precision = {precision}\")\n",
    "print(f\"recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    knn3,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Name of class one\", \"Name of class two\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many false positives and false negatives do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(d):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(e)\n",
    "We will now try to optimize the $k$ for a $k$-nearest neighbour classifier.\n",
    "This can be done using the method [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "One of the inputs to this method is the `scoring` parameter, which\n",
    "selects the metric to use for finding the best $k$. Use the metric\n",
    "you deemed most important in question [10.1(c)](#10.1(c)) \n",
    "and use $k$-values in the range $1 \\leq k \\leq 10$ in your search for the best $k$.\n",
    "\n",
    "An example\n",
    "of how this can be done is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 11)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Select scoring here!\n",
    ")\n",
    "\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "\n",
    "# Use the best classifier for the test set:\n",
    "y_hat = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate the precision etc. for the test set:\n",
    "precision = precision_score(y_test, y_hat)\n",
    "recall = recall_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print(f\"precision = {precision}\")\n",
    "print(f\"recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_knn,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Name of class one\", \"Name of class two\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the optimised classifier using\n",
    "the metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "What value for $k$ did you find? And did the number of false\n",
    "positives and false negatives change compared to the non-optimised $k$-nearest neighbour classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(e):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(f)\n",
    "Create a [decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "and fit it using your training set. Limit the tree to $3$ levels by setting\n",
    "the parameter `max_depth=3`.\n",
    "Evaluate the classifier using the\n",
    "metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "\n",
    "Note, the example below question [10.1(h)](#10.1(h)) shows how you can \n",
    "create the decision tree and optimise it. The example will plot the decision tree.\n",
    "You can use this code as\n",
    "inspiration for solving [10.1(f)](#10.1(f)) and the following two questions (maybe you\n",
    "prefer to do them all at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(f):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(g)\n",
    "We will also\n",
    "try to tune the `DecisionTreeClassifier`\n",
    "by determining the maximum depth\n",
    "we should use for the tree. Use the method\n",
    "`GridSearchCV` to optimize the parameter\n",
    "`max_depth` for the `DecisionTreeClassifier`.\n",
    "Use the metric you deemed most important\n",
    "in question [10.1(c)](#10.1(c)). Limit the depth to the range `max_depth = range(1, 21)`, but also\n",
    "include a depth where you set `max_depth = None` (this lets the\n",
    "tree expand as far down as possible).\n",
    "\n",
    "Evaluate the classifier with the best `max_depth` using the\n",
    "metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "\n",
    "What is the best `max_depth` you find in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(g):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(h)\n",
    "Visualise the best decision tree you found. This\n",
    "can be done using the\n",
    "method [export_graphviz from sklearn.tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html),\n",
    "or the method [plot_tree from sklearn.tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)\n",
    "\n",
    "An example using `export_graphviz` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# First grab the variable names, these can be used to label\n",
    "# variables in the plot of the decision tree:\n",
    "variables = data[\"feature_names\"]\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"max_depth\": list(range(1, 21)) + [None]}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Select scoring here!\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_clf = grid.best_estimator_\n",
    "print(\"Best tree:\", best_clf)\n",
    "\n",
    "# Show the decision tree:\n",
    "dot_data = export_graphviz(\n",
    "    best_clf,  # The decision tree we want to draw\n",
    "    out_file=None,  # We will set the file name later\n",
    "    feature_names=variables,  # Name of variables\n",
    "    class_names=[\n",
    "        \"Name of first class\",\n",
    "        \"Name of second class\",\n",
    "    ],  # Class names, update these!\n",
    "    rounded=True,  # Use rounded boxes\n",
    "    filled=True,  # Use colors\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"tree\", view=True)  # Show the tree, and create a tree.pdf for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If the code above executed successfully, a file named [tree.pdf](./tree.pdf) should have been created.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The plot tree method is used as follows:\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(32, 8))\n",
    "# If the three is too small, you can change the figsize above^ or adjust the fontsize below\n",
    "plot_tree(\n",
    "    best_clf,  # The decision tree we want to draw\n",
    "    feature_names=variables,  # Name of variables\n",
    "    class_names=[\"Name of first class\", \"Name of second class\"],  # Class names\n",
    "    rounded=True,  # Use rounded boxes\n",
    "    filled=True,  # Use colors\n",
    "    ax=ax,  # Use this axes for plotting\n",
    "    fontsize=10,\n",
    ")\n",
    "fig.savefig(\"tree2.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If the code above executed successfully, a file named [tree2.pdf](tree2.pdf) should have been created.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(h):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(i)\n",
    "Compare the precision, recall, and F1 scores for the classifiers you have considered.\n",
    "If you were to select one\n",
    "classifier to put into real-life use, which one would you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(i):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(j)\n",
    "Extra task for the curious student: Create an alternative classifier, for instance,\n",
    "using a so-called [support vector machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). We will not go into the details about how\n",
    "this classifier works in our lectures, but with `sckikit-learn` it is rather easy\n",
    "to try\n",
    "it and see what it can do for us.\n",
    "\n",
    "In the scikit-learn documentation, there is also [an example that compares several classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). Maybe you can find one\n",
    "that outperforms those we have considered in this exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(j):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.2\n",
    "\n",
    "Consider again the data set for ovarian cancer and the measured gene expressions (see exercise 9).\n",
    "Create a decision tree classifier for this data set. Limit the depth of the decision\n",
    "tree to 2, and visualise the decision tree. How do the \"rules\" the decision tree uses\n",
    "for its classification compare to what you found from the PCA analysis?\n",
    "Does it consider the same genes?\n",
    "\n",
    "Note: There is some \"randomness\" in decision trees, so\n",
    "the tree you now\n",
    "create will likely\n",
    "use different genes from the ones you\n",
    "found in exercise 9. You can rerun your code a few times to see how the randomness influences\n",
    "things or you can also change the depth of the tree to see if it picks out\n",
    "more genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.2:\n",
    "*Double click here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
