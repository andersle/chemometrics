{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1:** \n",
    "\n",
    "In this exercise, we will make a least squares model for a case where we have several\n",
    "variables which may be correlated.\n",
    "The file [bloodpress.txt](Data/bloodpress.txt) ('Data/bloodpress.txt') contains data about 20 individuals with high blood pressure.\n",
    "The data columns present in the file are given in table 1.\n",
    "\n",
    "\n",
    "| Label  | Description              |             Unit |\n",
    "|:-------|:-------------------------|-----------------:|\n",
    "| PT     | Label for person         |              --- |\n",
    "| BP     | Blood pressure           |             mmHg |\n",
    "| Age    | Age                      |            years |\n",
    "| Weight | Weight                   |               kg |\n",
    "| BSA    | Body surface area        |            m$^2$ |\n",
    "| DUR    | Duration of hypertension |            years |\n",
    "| Pulse  | Basal heart rate         | beats per minute |\n",
    "| Stress | Stress index             |              --- |\n",
    "||**Table 1:** *Data columns present in the file [bloodpress.txt](Data/bloodpress.txt)*|\n",
    "\n",
    "\n",
    "**(a)** Begin by exploring the data. Plot pairs of variables against each other\n",
    "to visually inspect which variables might be correlated. Based on your plots, which pairs\n",
    "of variables seem to be correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.1(a):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** To quantitatively investigate possible correlations, we can calculate\n",
    "the [Pearson correlation\n",
    "coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) ($\\rho_{X,Y}$),\n",
    "\\begin{equation*}\n",
    "\\rho_{X,Y} = \\frac{\\mathrm{cov}(X,Y)}{S_X S_Y},\n",
    "\\end{equation*}\n",
    "where $\\mathrm{cov}(X,Y)$ is the covariance between the variables $X$ and $Y$, and\n",
    "$S_i$ is the estimated standard deviation for variable $i$.\n",
    "In `scipy`, this coefficient is a part of the [`stats` package](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) \n",
    "and can be imported by:\n",
    "```Python\n",
    "from scipy.stats import pearsonr\n",
    "```\n",
    "This function returns two values:\n",
    " * The correlation coefficient itself. It is close to $1$ for positively correlated distributions\n",
    "   and close to $-1$ for negatively correlated distributions.\n",
    " * A \"p-value\" which (to quote the documentation of `scipy`):\n",
    "\n",
    "> roughly indicates the probability of an uncorrelated system\n",
    "> producing datasets that have a Pearson correlation at least as extreme\n",
    "> as the one computed from these datasets.\n",
    "\n",
    "\n",
    "\n",
    "Calculate this coefficient for each of the pairs of variables and\n",
    "use it to judge if the variables\n",
    "are correlated. Does this seem reasonable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.1(b):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Create a linear model in which you predict the blood pressure ($y$) from\n",
    "all 6 available variables (Age, Weight, BSA, DUR, Pulse, and Stress).\n",
    "For this, make use of the `statsmodels` package and the\n",
    "ordinary least squares (OLS) estimation. (For an example of the usage, see: [here](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html))\n",
    "Here, it might be beneficial to scale your variables so that they have a\n",
    "mean of $0$ and a variance of $1$. This can, for instance, be done using the `sklearn` package.(Please see the [`scale` method](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html))\n",
    "Compare the results of the fitting using both scaled and non-scaled variables.\n",
    "\n",
    "**Note:** One motivation for using the `statsmodels` package is that it will do\n",
    "some additional statistics in connection with the fitting. To print out\n",
    "the results of a fitting, we can do the following:\n",
    "```Python\n",
    "import statsmodels.api as sm\n",
    "# load X data...\n",
    "# load y data...\n",
    "# Do fitting:\n",
    "model = sm.OLS(y, X).fit()\n",
    "# Print out statistics:\n",
    "print(model.summary())\n",
    "```\n",
    "Some information on the interpretation of this summary can be found [here](https://blog.datarobot.com/ordinary-least-squares-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.1(c):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Your results so far should indicate that some of the variables\n",
    "are correlated. Create a new model where you remove variables\n",
    "that are correlated. That is, if a variable, say $u$, is correlated with\n",
    "another variable, say $v$, pick either $u$ or $v$ for your new model.\n",
    "\n",
    "Fit your new model to the blood pressure data\n",
    "and compare it with the model where you included all variables.\n",
    "How many variables can you remove and still get a good model?\n",
    "Which variables seem\n",
    "to be more important for predicting if a person will have high blood pressure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.1(d):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) (Optional exercise)** One strategy to assess the quality of a model is to split the raw data\n",
    "into a **training set** and a **test set**. Then the **training set**\n",
    "is used to fit the data and the **test set** is used to test the quality of\n",
    "the model.\n",
    "\n",
    "Consider a data set containing $n$ samples. If the data set is\n",
    "relatively small, we can use the \"Leave-one-out cross-validation (LOOCV)\"\n",
    "approach. Here, we pick one sample, say measurement number $j$, which we\n",
    "keep out of the fitting and we fit the model using the $n-1$ other points.\n",
    "Now, we can *predict* the value of measurement $j$ using the fitted model,\n",
    "let us call the predicted value $\\tilde{y}_j$. The difference between the predicted\n",
    "value from the model and the measured value $y_j$ can then be taken as a measure\n",
    "of the error,\n",
    "\\begin{equation*}\n",
    "r_j = y_{j} - \\tilde{y}_j,\n",
    "\\end{equation*}\n",
    "where $r_j$ is the so-called predicted residual.\n",
    "There is nothing special about picking point $j$, and we can try all possibilities\n",
    "of leaving one point out, and refitting the model using the remaining $n-1$\n",
    "measurements. After doing this for all possibilities, we have fitted the model\n",
    "$n$ times and the mean squared error (obtained from the squared\n",
    "residuals), $\\mathrm{MSE}_{\\mathrm{CV}}$, can then be used\n",
    "to estimate the error in the model,\n",
    "\\begin{equation*}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2,\n",
    "\\end{equation*}\n",
    "where $y_i$ is the measured $y$ in experiment $i$, and $\\tilde{y}_i$ is the\n",
    "predicted $y$, using a model which was fitted using all point \\emph{except} $y_i$.\n",
    "\n",
    "*(i)* Implement the LOOCV approach and calculate the mean squared error for a linear fit\n",
    "using all variables, and for a linear fit using just the weight and age as\n",
    "variables.\n",
    "\n",
    "**Hint:** The splitting into training and test sets can be done with the method \\textsf{LeaveOneOut}\n",
    "from `sklearn`:\n",
    "```Python\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "# load X data...\n",
    "# load y data...\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Do fitting using X_train and y_train.\n",
    "    # Calculate error using y_test and predicted y from the model.\n",
    "```\n",
    "\n",
    "*(ii)* One can show that for polynomial fitting, the mean squared error can\n",
    "be obtained in a simpler way,\n",
    "\\begin{equation*}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2 =\n",
    "\\frac{1}{n}\\sum_{i=1}^{m} \\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2,\n",
    "\\end{equation*}\n",
    "where the $\\hat{y}_i$'s are predicted values using the\n",
    "model fitted with \\emph{all data points},\n",
    "and $h_{ii}$ is the $i$'th diagonal element of the\n",
    "$\\mathbf{H}$ matrix (the projection matrix,\n",
    "see Eq. (2) in exercise 2),\n",
    "\\begin{equation}\n",
    "\\mathbf{H} =\n",
    "\\mathbf{X} \n",
    "\\left( \n",
    "  \\mathbf{X}^\\mathrm{T} \\mathbf{X}\n",
    "\\right)^{-1}\n",
    "\\mathbf{X}^\\mathrm{T}\n",
    "\\end{equation}\n",
    "Using the equations given above, recalculate $\\mathrm{MSE}_{\\mathrm{CV}}$ and compare\n",
    "with your previous answers.\n",
    "Note the difference between $\\hat{y}_i$ and $\\tilde{y}_i$, and the\n",
    "fact that we actually don't have to do the\n",
    "refitting(!) in order to obtain the $\\mathrm{MSE}_{\\mathrm{CV}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Your answer to 4.1(e):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2**\n",
    "\n",
    "In this exercise, we will investigate a data set using PCA and\n",
    "our aim is to classify objects.\n",
    "The data set is contained in the file [\"Data/data_exercise4.txt\"](Data/data_exercise4.txt)\n",
    "where you will find columns as described in table 2.\n",
    "\n",
    "| Label | Description                       |    Unit |\n",
    "|:------|:----------------------------------|--------:|\n",
    "| $x$   | Values of a measured quantity $x$ | Unknown |\n",
    "| $y$   | Values of a measured quantity $y$ | Unknown |\n",
    "| class | Classification of objects         |     --- |\n",
    "||**Table 2:** *Data columns present in the file [data_exercise4.txt](Data/data_exercise4.txt)\n",
    "\n",
    "Here, we have done a series of measurements of the variables\n",
    "$x$ and $y$ for some objects, and we have labeled these objects as\n",
    "belonging to one of two possible classes: \"foo\" or \"bar\".\n",
    "Your task is now to investigate if you can separate\n",
    "data points into these two classes, by performing a principal component analysis.\n",
    "\n",
    "**(a)** As stated above, we are attempting to separate the data into two\n",
    "classes. Should you include the labels (i.e. the \"class\" column)\n",
    "in the data matrix you will run PCA on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(a):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Begin by plotting the raw data.\n",
    "Below you will find some Python\n",
    "code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt # Plotting\n",
    "import numpy as np # Matrix operations\n",
    "import pandas as pd # Reading the data set\n",
    "from sklearn.preprocessing import StandardScaler # Scaling (if needed )\n",
    "from sklearn.decomposition import PCA # Performing PCA\n",
    "# Load data:\n",
    "data = pd.read_csv('Data/data_exercise4.txt', delim_whitespace =True)\n",
    "# Extract the two classes for plotting :\n",
    "class1 = data[data['class'] == 'foo']\n",
    "class2 = data[data['class'] == 'bar']\n",
    "# We can plot the raw data as follows :\n",
    "# Note: Here you could style the plot to your liking .\n",
    "# Adding x- labels and y- labels is , for instance , a good idea.\n",
    "plt.scatter(class1['x'], class1['y'], label='Class foo')\n",
    "plt.scatter(class2['x'], class2['y'], label='Class bar')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Run the principal component analysis. Should you scale the data in this case?\n",
    "Below you will find some Python\n",
    "code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "plt.style.use('seaborn-talk')\n",
    "# Load data:\n",
    "data = pd.read_csv('Data/data_exercise4.txt', delim_whitespace =True)\n",
    "variables = ['x','y'] # Insert the variables you will use here\n",
    "X = data[variables]\n",
    "# Uncomment the following line in order to scale the data:\n",
    "# scaler = StandardScaler() # Initialise the scaler\n",
    "# scaler.fit(X) # \"fit\" the scaller with the X data\n",
    "# X = scaler.transform(X) # Transform X data with the fit\n",
    "# To run PCA:\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(c):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** The `pca` object defined above contains the\n",
    "results of the principal component analysis.\n",
    "\n",
    "**(i)** How many principal components were\n",
    "used here? Hint: Inspect `pca.n_components_`\n",
    "\n",
    "**(ii)** Plot the explained variance by these components.\n",
    "Hint: Inspect `pca.explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(d):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** The principal components themselves are contained as row vectors in\n",
    "the matrix `pca.components_`:\n",
    "\n",
    "*(i)* Obtain the principal component vectors.\n",
    "\n",
    "*(ii)* Verify that they are normalized (Hint: dot products can be done with `np.dot`)\n",
    "\n",
    "*(iii)* Verify that they are orthogonal to each other.\n",
    "\n",
    "*(iv)* Plot them together with the raw data.\n",
    "Do they point in the directions you would expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(e):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Next, we will investigate the contributions from the\n",
    "original variables to the principal components. The contribution from\n",
    "a variable can be found as a column vector in the matrix `pca.components_`.\n",
    "Make a plot where you show the contributions from each of the original variables to principal components 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Plot the scores of the data points for\n",
    "principal components 1 and 2. Color these points by the\n",
    "class information.\n",
    "\n",
    "*(i)* Which of the principal components seems more important for\n",
    "separating between the two classes?\n",
    "\n",
    "*(ii)*\n",
    "For the following two points,\n",
    " * Point 1: $x = 4$, $y=6$\n",
    " * Point 2: $x = 6$, $y=4$\n",
    "which class would you\n",
    "predict?\n",
    "\n",
    "*(iii)* Can you, based on this plot, make a simple\n",
    "rule for determining if a point will belong to class \"foo\"  or class \"bar\"?\n",
    "What would your rule be?\n",
    "\n",
    "*(iv)* Can you \"translate\" your rule from the principal\n",
    "component space to the original ($x$, $y$) space? What would the corresponding\n",
    "rule be in the original variable space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(g):** (double click here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** PCA belongs to a class of methods that are called latent variable methods.\n",
    "Latent variable methods typically discover new\n",
    "variables using the original ones, intending to uncover\n",
    "\"hidden\" relations in the data.\n",
    "In PCA, we find\n",
    "such latent variables by taking linear combinations of our\n",
    "original variables, and we make the new latent variables point\n",
    "in the directions of the largest variance in our data.\n",
    "\n",
    "Another example of a latent variable method is \n",
    "Linear discriminant analysis (LDA).\n",
    "LDA is similar to PCA, and in LDA we also\n",
    "find new variables as linear combinations of our original variables.\n",
    "However, the way we find them is different. In LDA, we do not look for\n",
    "directions in which the variance is largest, but we rather look for\n",
    "directions that *separate* the classes best. This means that class information\n",
    "is needed as input when training the model.\n",
    "\n",
    "Based on this would you say that PCA is a supervised or unsupervised method?\n",
    "What about LDA? Is LDA a supervised or unsupervised method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer to 4.2(h):** (double click here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
