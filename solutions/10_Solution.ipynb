{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 10\n",
    "\n",
    "\n",
    ">The goal of this exercise is to gain familiarity with some\n",
    "classification methods and the different ways we can assess and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.1\n",
    "\n",
    "\n",
    "In this exercise, we will consider the\n",
    "[UCI ML Breast Cancer Wisconsin (Diagnostic) dataset](https://goo.gl/U2Uwz2).\n",
    "\n",
    "This data set contains 569 tumours classified\n",
    "as malignant or benign. In addition, 30 variables have been\n",
    "measured, and our goal is to make a predictive model that\n",
    "can classify new tumours as malignant or benign.\n",
    "\n",
    "An overview of the different variables can be found\n",
    "on the \n",
    "[scikit-learn website](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset).\n",
    "In the following, we are going to label the two classes as:\n",
    "\n",
    "* `benign` as a negative ($-1$), and\n",
    "* `malignant` as a positive ($+1$). \n",
    "\n",
    "\n",
    "In the lectures, we mentioned categorical variables and that we might have to\n",
    "transform these to use them in practice.\n",
    "[Dummy variables](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) and\n",
    "[one-hot encoding](https://en.wikipedia.org/wiki/One-hot) are examples of such transformations.\n",
    "In scikit-learn, we do normally not have to worry about this for the y-values we use in classification.\n",
    "For instance, the\n",
    "[scikit-learn documentation for decision trees](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "says that a decision tree \n",
    "> is capable of both binary (where the labels are $[-1, 1]$) classification and multiclass (where the labels are \n",
    "$[0, \\ldots, K-1]$) classification\n",
    "\n",
    "so we use the values $-1$ and $+1$ to represent the two classes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(a) \n",
    "\n",
    "Begin by loading the raw data and creating\n",
    "a test set using $33$\\% of the available data points for the test set.\n",
    "The example code below can be used to load the data set\n",
    "and create training/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY310,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data[\"data\"]\n",
    "# \"Rename\" y so that -1 = benign and 1 = malignant:\n",
    "y = [-1 if i == 1 else 1 for i in data[\"target\"]]\n",
    "class_names = [\"benign\", \"malignant\"]\n",
    "print(\"Classes:\")\n",
    "print(class_names)\n",
    "\n",
    "print(\"Variables:\", data[\"feature_names\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    # stratify=y, # Uncomment if you are using stratification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the training/test sets we use the method\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "from the module [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). One of the input parameters to `train_test_split` is\n",
    "`stratify`:\n",
    "\n",
    "\n",
    "* (i) Reading the documentation for\n",
    "  [stratification](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)\n",
    "  (or the [Wikipedia entry on stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling))\n",
    "  can you explain what `stratify` does?\n",
    "\n",
    "\n",
    "* (ii)  Should we use `stratify` here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data[\"data\"]\n",
    "X = scale(X)\n",
    "# \"Rename\" y so that -1 = benign and 1 = malignant:\n",
    "y = np.array([-1 if i else 1 for i in data[\"target\"]])\n",
    "class_names = [\"benign\", \"malignant\"]\n",
    "print(\"Classes:\")\n",
    "print(class_names)\n",
    "\n",
    "print(\"Variables:\")\n",
    "print(data[\"feature_names\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"\\nItems in training set: {len(y_train)}\")\n",
    "print(f\"Items in test set: {len(y_test)}\")\n",
    "print(f\"fraction malignant total: {len(y[y==-1])/len(y)}\")\n",
    "print(f\"fraction malignant train: {len(y_train[y_train==-1])/len(y_train)}\")\n",
    "print(f\"fraction malignant test: {len(y_test[y_test==-1])/len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(a):\n",
    "\n",
    "(i) and (ii):\n",
    "\n",
    "Stratification ensures that the distribution of the different classes is the same in the test and training set as in the original data set. This is important if the classes in the data set are unevenly distributed. In this case, the number of benign and malignant tumours are not too different, but to be safe, we tell train_test_split to stratify according to the y-values (the class information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(b)\n",
    "In this case, we have to determine what\n",
    "metric we are going to use\n",
    "to judge the performance of the classifiers we make. Before selecting\n",
    "a metric, we should consider what false positives and false negatives\n",
    "mean for our current problem: How would you define these two terms in our present\n",
    "case, and would you say that false positives are a more serious mistake than false negatives here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(b):\n",
    "\n",
    "Here we can make the following interpretations:\n",
    "\n",
    "\n",
    "- False positive: This is a tumour classified as malignant, but it is, in fact, benign.\n",
    "  The seriousness of mistakes like this depends on what we do with samples we believe to be malignant.\n",
    "  If we, for instance, start treatment based on these results, we would, in this case, treat something\n",
    "  that we did not need to treat. Depending on the treatment, this can be a grave mistake to make.\n",
    "\n",
    "\n",
    "- False negative: This is a tumour classified as benign, but it is malignant! Here, we are\n",
    "  missing potentially dangerous tumours (we think they are benign!). This is a grave\n",
    "  mistake since it could delay the start of treatment.\n",
    "\n",
    "\n",
    "Here, I will assume that the outcome of the classifier is used to screen for malignant tumours. A positive outcome is followed up by further tests to determine if the tumour is indeed malignant. In such a case, we would rather have someone tested extra due to a false positive than delay diagnosis due to a false negative. I will thus focus on minimizing the false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(c)\n",
    "Following up on the previous question, here\n",
    "are some [metrics](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) we could use to assess the performance of a\n",
    "classifier model we make:\n",
    "\n",
    "\n",
    "* **Precision**: The ratio of true positives to the sum of\n",
    "  true positives and false positives.\n",
    "\n",
    "\n",
    "* **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "\n",
    "* **F1**: The (harmonic) mean of the precision and recall.\n",
    "\n",
    "\n",
    "In addition, we can summarize the performance using the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "\n",
    "(Note: There are many [other possibilities](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "as well! If you are curious, you can, for instance, include the\n",
    "*accuracy* (the ratio of correct predictions\n",
    "to the number of total predictions).)\n",
    "\n",
    "\n",
    "The choice of the metric for assessing a classifier will lead to different results.\n",
    "For instance, if we choose to use precision as our metric, we will maximize it\n",
    "during the optimization of our model. This means that we will *minimize* the\n",
    "number of *false positives*. If we choose to use the recall, on the other hand,\n",
    "we will *minimize* the number of *false negatives*.\n",
    "\n",
    "\n",
    "In the following, we will calculate all these metrics for the\n",
    "different classification methods we consider. At the end of the\n",
    "exercise, you will be asked to compare the different classifiers\n",
    "using them. But before we do that: \n",
    "Which of the\n",
    "metrics mentioned above is most important for\n",
    "our classification task?\n",
    "\n",
    "(Note: There is no single correct answer here: it depends on how you judge the seriousness of false positives vs false negatives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(c):\n",
    "\n",
    "As stated in the previous question, I focus on minimising the number of false negatives. I will pick the **recall** since it will tell us how many malignant breast cancer tumours we correctly identified, and optimising it will minimise the number of false negatives.\n",
    "\n",
    "**Note:** We can always make the number of false negatives go to zero by just classifying everything as positive. However, this would make the classification pretty useless (we do not even need a classifier to say that everything is positive!). Therefore, we will monitor the other scores (precision and F1) to ensure that the number of false positives is manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(d)\n",
    "Create a [$k$-nearest neighbour classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    " with 3 neighbours and\n",
    "fit it using your training set. Evaluate (with the test set) the classifier using the\n",
    "precision, recall, and F1 metrics, and plot the confusion matrix.\n",
    "\n",
    "An\n",
    "example of how this can be done is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Make a table to store all results\n",
    "table = {\n",
    "    \"classifier\": [],\n",
    "    \"recall\": [],\n",
    "    \"precision\": [],\n",
    "    \"f1\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def add_results_to_table(table, name, recall, precision, f1):\n",
    "    \"\"\"Add results to the given table.\"\"\"\n",
    "    table[\"classifier\"].append(name)\n",
    "    table[\"recall\"].append(recall)\n",
    "    table[\"precision\"].append(precision)\n",
    "    table[\"f1\"].append(f1)\n",
    "\n",
    "\n",
    "# Create a classifier:\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier on the training set:\n",
    "knn3.fit(X_train, y_train)\n",
    "\n",
    "# Use classifier for prediction for the test set:\n",
    "y_hat = knn3.predict(X_test)\n",
    "\n",
    "# Calculate the precision etc. for the test set:\n",
    "precision = precision_score(y_test, y_hat)\n",
    "recall = recall_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print(f\"precision = {precision}\")\n",
    "print(f\"recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "\n",
    "add_results_to_table(table, \"knn_with_3_neighbours\", recall, precision, f1)\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    knn3,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Benign\", \"Malignant\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many false positives and false negatives do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(d):\n",
    "Here (see the confusion matrix above) we got 0 false positives and 9 false negatives. The test set contains 61 + 9 = 70 malignant tumors, and the classifier found 61 of these correctly. But it is not performing great, missing about 12,8% (9 out of 70) of the positive (malignant) tumors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(e)\n",
    "We will now try to optimize the $k$ for a $k$-nearest neighbour classifier.\n",
    "This can be done using the method [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "One of the inputs to this method is the `scoring` parameter, which\n",
    "selects the metric to use for finding the best $k$. Use the metric\n",
    "you deemed most important in question [10.1(c)](#10.1(c)) \n",
    "and use $k$-values in the range $1 \\leq k \\leq 10$ in your search for the best $k$.\n",
    "\n",
    "An example\n",
    "of how this can be done is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 11)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",  # Select scoring here!\n",
    ")\n",
    "\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "\n",
    "# Use the best classifier for the test set:\n",
    "y_hat = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate the precision etc. for the test set:\n",
    "precision = precision_score(y_test, y_hat)\n",
    "recall = recall_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print(f\"precision = {precision}\")\n",
    "print(f\"recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_knn,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Name of class one\", \"Name of class two\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the optimised classifier using\n",
    "the metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "What value for $k$ did you find? And did the number of false\n",
    "positives and false negatives change compared to the non-optimised $k$-nearest neighbour classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 11)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best recall score (train): {grid.best_score_}\")\n",
    "print(f\"Best k: {grid.best_params_['n_neighbors']}\")\n",
    "y_hat = best_knn.predict(X_test)\n",
    "\n",
    "recall = recall_score(y_test, y_hat)\n",
    "precision = precision_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print(\"Best recall score (test):\", recall)\n",
    "print(\"Best precision score (test):\", precision)\n",
    "print(\"Best F1 score (test):\", f1)\n",
    "\n",
    "\n",
    "add_results_to_table(table, \"knn_optimized\", recall, precision, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_knn,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Benign\", \"Malignant\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(e):\n",
    "\n",
    "We already guessed the optimal number of k (3), and the number of false positives and negatives did not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(f)\n",
    "Create a [decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "and fit it using your training set. Limit the tree to $3$ levels by setting\n",
    "the parameter `max_depth=3`.\n",
    "Evaluate the classifier using the\n",
    "metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "\n",
    "Note, the example below question [10.1(h)](#10.1(h)) shows how you can \n",
    "create the decision tree and optimise it. The example will plot the decision tree.\n",
    "You can use this code as\n",
    "inspiration for solving [10.1(f)](#10.1(f)) and the following two questions (maybe you\n",
    "prefer to do them all at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree3 = DecisionTreeClassifier(max_depth=3)\n",
    "tree3.fit(X_train, y_train)\n",
    "y_hat = tree3.predict(X_test)\n",
    "\n",
    "\n",
    "recall = recall_score(y_test, y_hat)\n",
    "precision = precision_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "\n",
    "print(\"Tree recall score (test):\", recall)\n",
    "print(\"Tree precision score (test):\", precision)\n",
    "print(\"Tree F1 score (test):\", f1)\n",
    "\n",
    "add_results_to_table(table, \"tree_depth_3\", recall, precision, f1)\n",
    "\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    tree3,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Benign\", \"Malignant\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(f):\n",
    "It performs worse than the k-nearest neighbours classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(g)\n",
    "We will also\n",
    "try to tune the `DecisionTreeClassifier`\n",
    "by determining the maximum depth\n",
    "we should use for the tree. Use the method\n",
    "`GridSearchCV` to optimize the parameter\n",
    "`max_depth` for the `DecisionTreeClassifier`.\n",
    "Use the metric you deemed most important\n",
    "in question [10.1(c)](#10.1(c)). Limit the depth to the range `max_depth = range(1, 21)`, but also\n",
    "include a depth where you set `max_depth = None` (this lets the\n",
    "tree expand as far down as possible).\n",
    "\n",
    "Evaluate the classifier with the best `max_depth` using the\n",
    "metrics mentioned above (with the test set) and plot the confusion matrix.\n",
    "\n",
    "What is the best `max_depth` you find in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=123)\n",
    "parameters = [{\"max_depth\": list(range(1, 21)) + [None]}]\n",
    "grid = GridSearchCV(\n",
    "    classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"recall\",\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best recall score (train):\", grid.best_score_)\n",
    "print(\"Best depth:\", grid.best_params_[\"max_depth\"])\n",
    "best = grid.best_estimator_\n",
    "y_hat = best.predict(X_test)\n",
    "\n",
    "recall = recall_score(y_test, y_hat)\n",
    "precision = precision_score(y_test, y_hat)\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "\n",
    "print(\"Best recall score (test):\", recall)\n",
    "print(\"Best precision score (test):\", precision)\n",
    "print(\"Best F1 score (test):\", f1)\n",
    "\n",
    "add_results_to_table(table, \"tree_optimized\", recall, precision, f1)\n",
    "\n",
    "\n",
    "# Make confusion matrix:\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Benign\", \"Malignant\"],\n",
    "    ax=ax,  # Use the figure we created above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(g):\n",
    "Six seems to be the optimal depth of the tree. There is some randomness used in the algorithm for making the decision tree. Above, the random state is set to a specified number to get the same decision tree each time we run the code. If we remove this, we will get a different number for the best depth, which changes a bit. This indicates that the recall scores are maybe not too different for different depts. Let us inspect how much the recall is changing as a function of the depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean = grid.cv_results_[\"mean_test_score\"]\n",
    "score_std = grid.cv_results_[\"std_test_score\"]\n",
    "depths = [i[\"max_depth\"] for i in grid.cv_results_[\"params\"]]\n",
    "# Deal with the \"None\" for plotting\n",
    "maxdepth = max([i for i in depths if i != None])\n",
    "depths = [\n",
    "    i if i != None else maxdepth + 1 for i in depths\n",
    "]  # \"Rename\" None to 21, that is, 21 is now max_depth = None\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.errorbar(depths, score_mean, yerr=score_std, marker=\"o\")\n",
    "ax.set_xticks(depths)\n",
    "labels = [i if i != 21 else \"None\" for i in depths]\n",
    "ax.set_xticklabels(labels)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that the recall score varies a lot; here, it indicates that the tree might have difficulties \"generalizing\" the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(h)\n",
    "Visualise the best decision tree you found. This\n",
    "can be done using the\n",
    "method [export_graphviz from sklearn.tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html),\n",
    "or the method [plot_tree from sklearn.tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import display\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    best,\n",
    "    out_file=None,\n",
    "    feature_names=data[\"feature_names\"],\n",
    "    class_names=[\"benign\", \"malignant\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "display(graphviz.Source(dot_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(h):\n",
    "(See the three above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(i)\n",
    "Compare the precision, recall, and F1 scores for the classifiers you have considered.\n",
    "If you were to select one\n",
    "classifier to put into real-life use, which one would you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(table[\"recall\"])\n",
    "best_recall = table[\"recall\"][idx]\n",
    "best_name = table[\"classifier\"][idx]\n",
    "print(f\"Best classifier was {best_name} with a recall = {best_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(i):\n",
    "Of the classifiers we have considered here, k-nearest neighbours with three neighbours seem to perform best (it has better scores for precision, recall, and F1). We would therefore use this one, but we are not completely satisfied (see the question below)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1(j)\n",
    "Extra task for the curious student: Create an alternative classifier, for instance,\n",
    "using a so-called [support vector machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). We will not go into the details about how\n",
    "this classifier works in our lectures, but with `sckikit-learn` it is rather easy\n",
    "to try\n",
    "it and see what it can do for us.\n",
    "\n",
    "In the scikit-learn documentation, there is also [an example that compares several classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). Maybe you can find one\n",
    "that outperforms those we have considered in this exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "names = [\n",
    "    \"Naive bayes\",\n",
    "    \"LDA\",\n",
    "    \"Support vector machine\",\n",
    "    \"CatBoost\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    SVC(),\n",
    "    CatBoostClassifier(verbose=0),\n",
    "]\n",
    "for clf, name in zip(classifiers, names):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    precision = precision_score(y_test, y_hat)\n",
    "    f1 = f1_score(y_test, y_hat)\n",
    "\n",
    "    add_results_to_table(table, name, recall, precision, f1)\n",
    "    print(f\"\\nResults for classifier: {name}\")\n",
    "    print(\"Recall score (test):\", recall)\n",
    "    print(\"Precision score (test):\", precision)\n",
    "    print(\"F1 score (test):\", f1)\n",
    "    if recall > best_recall:\n",
    "        print(f\"** Recall score is better than {best_name}! **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.DataFrame(table)\n",
    "table2.sort_values(by=\"recall\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.1(j):\n",
    "Here, we see that some of the classifiers we tried are performing better. We could now\n",
    "expand our study and try to find optimized parameters for them to see if we can do even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.2\n",
    "\n",
    "Consider again the data set for ovarian cancer and the measured gene expressions (see exercise 9).\n",
    "Create a decision tree classifier for this data set. Limit the depth of the decision\n",
    "tree to 2, and visualise the decision tree. How do the \"rules\" the decision tree uses\n",
    "for its classification compare to what you found from the PCA analysis?\n",
    "Does it consider the same genes?\n",
    "\n",
    "Note: There is some \"randomness\" in decision trees, so\n",
    "the tree you now\n",
    "create will likely\n",
    "use different genes from the ones you\n",
    "found in exercise 9. You can rerun your code a few times to see how the randomness influences\n",
    "things or you can also change the depth of the tree to see if it picks out\n",
    "more genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "data_ovo = pd.read_csv(\"Data/ovo.csv\")\n",
    "classes_ovo = data_ovo[\"class\"]\n",
    "X_ovo = data_ovo.filter(like=\"X.\", axis=1).values\n",
    "X_ovo = scale(X_ovo, with_std=False)\n",
    "\n",
    "feature_names_ovo = [\"gene_{}\".format(i) for i in range(X_ovo.shape[1])]\n",
    "\n",
    "y_ovo = [1 if i == \"C\" else 0 for i in classes_ovo]\n",
    "# Create a test and training set:\n",
    "X_train_ovo, X_test_ovo, y_train_ovo, y_test_ovo = train_test_split(\n",
    "    X_ovo,\n",
    "    y_ovo,\n",
    "    test_size=0.2,\n",
    "    stratify=y_ovo,\n",
    ")\n",
    "tree_ovo = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "tree_ovo.fit(X_train_ovo, y_train_ovo)\n",
    "dot_data_ovo = export_graphviz(\n",
    "    tree_ovo,\n",
    "    out_file=None,\n",
    "    feature_names=feature_names_ovo,\n",
    "    class_names=[\"N\", \"C\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "display(graphviz.Source(dot_data_ovo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score this classifier:\n",
    "y_hat_ovo = tree_ovo.predict(X_test_ovo)\n",
    "print(\"Recall score (test):\", recall_score(y_test_ovo, y_hat_ovo))\n",
    "print(\"Precision score (test):\", precision_score(y_test_ovo, y_hat_ovo))\n",
    "print(\"F1 score (test):\", f1_score(y_test_ovo, y_hat_ovo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 10.2:\n",
    "In this case, what genes you get will vary due to the randomness in the algorithm making the decision tree classifier. However, it will typically pick out some of the genes we found in the previous exercise. Here, we could try many trees and see the most frequently picked genes. Let us use a random forest classifier and plot the importance of the different genes for making the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_tree = RandomForestClassifier(\n",
    "    max_depth=2, n_estimators=500, random_state=42\n",
    ")  # max depth is 2, and we use 500(!) trees.\n",
    "rnd_tree.fit(X_train_ovo, y_train_ovo)\n",
    "# Plot the importance of variables (genes):\n",
    "importance = rnd_tree.feature_importances_\n",
    "idx = np.argsort(importance)\n",
    "y_pos = []\n",
    "y_label = []\n",
    "figi, axi = plt.subplots(constrained_layout=True)\n",
    "\n",
    "colors = sns.color_palette(\"husl\", 20)\n",
    "\n",
    "for i, idxi in enumerate(\n",
    "    idx[-20:]\n",
    "):  # 20 is here to plot the 20 most important ones:\n",
    "    y_pos.append(i)\n",
    "    y_label.append(feature_names_ovo[idxi])\n",
    "    axi.barh(i, importance[idxi], align=\"center\", color=colors[i])\n",
    "axi.set_yticks(y_pos)\n",
    "axi.set_yticklabels(y_label)\n",
    "axi.set_xlabel(\"Variable importance for a random forest classifier\")\n",
    "sns.despine(fig=figi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this analysis picks out many of the genes we found using PCA, for instance, gene 92 or 1490."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
