{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeef0fd",
   "metadata": {},
   "source": [
    "# Least squares for a more difficult case\n",
    "Here, we will try to predict the compressive strength of concrete. \n",
    "The data is taken from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength) and parts of it was used in\n",
    "[this article](https://doi.org/10.1016/S0008-8846(98)00165-3).\n",
    "\n",
    "The data set contains 1030 samples where the strength has been measured as a function of\n",
    "the amounts of several components:\n",
    "\n",
    "* *Cement*\n",
    "* *Blast Furnace Slag*\n",
    "* *Fly Ash*\n",
    "* *Water*\n",
    "* *Superplasticizer*\n",
    "* *Coarse Aggregate*\n",
    "* *Fine Aggregate*\n",
    "\n",
    "and the *Age* measured in days. On the UCI Machine Learning Repository page it says that the strength is\n",
    "a \"highly nonlinear function of age and ingredients\", but we will see how well linear models can do in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc132fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('concrete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeaec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a57d8",
   "metadata": {},
   "source": [
    "## Initial exploration - Scatter Plot Matrix & Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.pairplot(data, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d730953",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "corr.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fabf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.heatmap(corr,  cmap='PiYG', vmin=-1, vmax=1, annot=True, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa3fcd",
   "metadata": {},
   "source": [
    "### Model 1: Least squares using all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "# We prepare the data: Here we scale y and  X:\n",
    "y = scale(data['Strength'].to_numpy())\n",
    "variables = [i for i in data.columns if i != 'Strength']\n",
    "X = scale(data[variables].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5db041",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression(fit_intercept=False)\n",
    "model1.fit(X, y)\n",
    "y_hat = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0802bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, X, y_true):\n",
    "    \"\"\"Caclulate some metrics for a model and plot predicted values and residuals.\"\"\"\n",
    "    y_predict = model.predict(X)\n",
    "    fig, (ax1, ax2) = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4), sharex=True)\n",
    "    r2 = r2_score(y_true, y_predict)\n",
    "    mse = mean_squared_error(y_true, y_predict)\n",
    "    ax1.scatter(y_predict, y_true)\n",
    "    ax1.set_title(f'R² = {r2:.3g}, MSE = {mse:.3g}')\n",
    "    ax1.set(xlabel='ŷ', ylabel='y')\n",
    "    ax2.scatter(y_predict, y_true-y_predict)\n",
    "    ax2.axhline(y=0, ls=':', color='k')\n",
    "    ax2.set(xlabel='ŷ', ylabel='y - ŷ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1dae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coefficients(model, variables=None):\n",
    "    \"\"\"Display coefficients for a linear model.\"\"\"\n",
    "    figi, axi = plt.subplots(constrained_layout=True)\n",
    "    try:\n",
    "        coefficients = model.coef_\n",
    "    except:\n",
    "        reg = model.named_steps['regression']\n",
    "        coefficients = reg.coef_\n",
    "        # Attempt to generate variable names:\n",
    "        poly = model.named_steps['polynomial']\n",
    "        variables = poly.get_feature_names_out(input_features=variables)\n",
    "\n",
    "    pos = list(range(len(variables)))\n",
    "    axi.bar(pos, coefficients)\n",
    "    axi.axhline(y=0, ls=':', color='k')\n",
    "    axi.set_xticks(pos)\n",
    "    axi.set_xticklabels(variables, rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model1, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45001f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coefficients(model1, variables=variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b0e4e",
   "metadata": {},
   "source": [
    "### Model 2: Adding higher order terms\n",
    "The first linear model is not too impressive. We shall now try to add higher order terms and interactions.\n",
    "Interactions are terms of the tyoe (as an example) \"age × water\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5562621",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),  # Add all second order terms and interactions\n",
    "    ('regression', LinearRegression(fit_intercept=False)),\n",
    "]\n",
    "model2 = Pipeline(steps=steps)\n",
    "model2.fit(X, y)\n",
    "score_model(model2, X, y)\n",
    "show_coefficients(model2, variables=variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78312390",
   "metadata": {},
   "source": [
    "### Checking the performance by using a training and test set.\n",
    "We have certainly added many variables now. But the R² value did not improve that much. When adding variables,\n",
    "we might overfit our model. One way to check for this is to use a strategy with training and tests sets. The main\n",
    "idea is: we make our model on one part of the data (the training set), and test it on another (the test set).\n",
    "The test set is not used when creating the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e12098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_train_test(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Do some scoring for models made with a test and training set.\"\"\"\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    r2_train = r2_score(y_train, y_train_predict)\n",
    "    r2_test = r2_score(y_test, y_test_predict)\n",
    "    mse_train = mean_squared_error(y_train, y_train_predict)\n",
    "    mse_test = mean_squared_error(y_test, y_test_predict)\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=2, constrained_layout=True, sharex=True)\n",
    "    \n",
    "    axes[0, 0].scatter(y_train_predict, y_train)\n",
    "    axes[0, 0].set_title(f'Training: R² = {r2_train:.3g}, MSE = {mse_train:.3g}')\n",
    "    \n",
    "    axes[0, 1].scatter(y_test_predict, y_test)\n",
    "    axes[0, 1].set_title(f'Test: R² = {r2_test:.3g}, MSE = {mse_test:.3g}')\n",
    "    \n",
    "    axes[0, 0].set(xlabel='ŷ', ylabel='y')\n",
    "    axes[0, 1].set(xlabel='ŷ', ylabel='y')\n",
    "    \n",
    "    axes[1, 0].scatter(y_train_predict, y_train - y_train_predict)\n",
    "    axes[1, 1].scatter(y_test_predict, y_test - y_test_predict)\n",
    "    \n",
    "    axes[1, 0].set(xlabel='ŷ', ylabel='y-ŷ')\n",
    "    axes[1, 1].set(xlabel='ŷ', ylabel='y-ŷ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression(fit_intercept=False)\n",
    "model1.fit(X_train, y_train)\n",
    "score_train_test(model1, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('leastsquares', LinearRegression(fit_intercept=False)),\n",
    "]\n",
    "model2 = Pipeline(steps=steps)\n",
    "model2.fit(X_train, y_train)\n",
    "score_train_test(model2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44073994",
   "metadata": {},
   "source": [
    "### Can LASSO help us?\n",
    "Let us try another method to see if all the variables we have added are needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df00418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "steps = [\n",
    "    ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('regression', Lasso(alpha=0.04, fit_intercept=False)),\n",
    "]\n",
    "model3 = Pipeline(steps=steps)\n",
    "model3.fit(X_train, y_train)\n",
    "score_train_test(model3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ced390",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coefficients(model3, variables=variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc767a41",
   "metadata": {},
   "source": [
    "Inspired by the results above, we try another least squares model, but with fewer variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca63fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[['Age', 'Cement', 'Slag',]].copy()  # Make a selection of variables here!\n",
    "data2['Age²'] = data['Age']**2  # Maybe the Age² should be used?\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = scale(data2.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LinearRegression(fit_intercept=False)\n",
    "model4.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac02c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model(model4, X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e931d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coefficients(model4, variables=data2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0d04c",
   "metadata": {},
   "source": [
    "### Concluding remarks\n",
    "OK, we do not have super impressive results. Maybe we should try something completely different?\n",
    "\n",
    "What we have done with the training and test set is completely general. If we try other supervised\n",
    "learning methods, we can still calculate $R^2$, the mean squared error, and use the training/testing strategy.\n",
    "Here are some tests for three extra methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR  # Support Vector Machine\n",
    "model5 = SVR()\n",
    "model5.fit(X_train, y_train)\n",
    "score_train_test(model5, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  # A decision tree\n",
    "model6 = DecisionTreeRegressor(max_depth=8)\n",
    "model6.fit(X_train, y_train)\n",
    "score_train_test(model6, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor  # A multi-layer Perceptron\n",
    "model7 = MLPRegressor(max_iter=1000)\n",
    "model7.fit(X_train, y_train)\n",
    "score_train_test(model7, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
