{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6439eb9a",
   "metadata": {},
   "source": [
    "# Dealing with unusual points\n",
    "\n",
    "This notebook shows how we can calculate the [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance) for\n",
    "the points we use in a least squares regression.\n",
    "\n",
    "The Cook's distance estimates the influence of the data points\n",
    "for the regression, and the particularly influential points are points that we can think of as important\n",
    "for determining the parameters of the model. These are points that we should check more closely for\n",
    "validity (they can potentially be outliers). The Cook's distance can be calculated using the projection matrix\n",
    "$\\textbf{H}$ (see exercise 3 for the definition of this one).\n",
    "\n",
    "The data file [outliers.csv](./outliers.csv) contain some x-values and 4 corresponding y values labeled\n",
    "\"y1\", \"y2\", \"y3\", and \"y4\". For the x-values, there is nothing special, except for one point that is\n",
    "far away from the others (this point is \"unusual\" compared to the other x-values). Further:\n",
    "\n",
    "* y1: These are points from the equation y1 = 1 + 2x with some random noise.\n",
    "* y2: These points are similar to y1, but one y value (approximately at x=-3) has been multiplied by 4.\n",
    "* y3: These points are similar to y1, but one y value (approximately at x=0) has been\n",
    "  multiplied by 4.\n",
    "* y4: These points are similar to y1, but one y value (the one corresponding to the unusual x-value) has\n",
    "  been multiplied by 4.\n",
    "  \n",
    "  \n",
    "We begin by loading this data, making some plots, and creating least squares models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf234f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the data and plot them:\n",
    "data = pd.read_csv(\"outliers.csv\")\n",
    "X = data[\"x\"].to_numpy().reshape(-1, 1)\n",
    "y1 = data[\"y1\"].to_numpy()\n",
    "y2 = data[\"y2\"].to_numpy()\n",
    "y3 = data[\"y3\"].to_numpy()\n",
    "y4 = data[\"y4\"].to_numpy()\n",
    "\n",
    "all_y = [y1, y2, y3, y4]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    ncols=2,\n",
    "    nrows=2,\n",
    "    figsize=(8, 8),\n",
    ")\n",
    "axes = axes.flatten()\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[2].set_ylabel(\"y\")\n",
    "for i, axi in enumerate(axes):\n",
    "    axi.scatter(X, all_y[i])\n",
    "    axi.set(xlabel=\"x\", title=f\"Data set: {i+1}\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac884d9",
   "metadata": {},
   "source": [
    "## Creating some linear models\n",
    "\n",
    "We will now create least squares models for the 4 data sets to see how the unusual points\n",
    "influence the models we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "predicted = []\n",
    "r2_scores = []\n",
    "\n",
    "for y in all_y:\n",
    "    new_model = LinearRegression(fit_intercept=True)\n",
    "    new_model.fit(X, y)\n",
    "    y_hat = new_model.predict(X)\n",
    "    models.append(new_model)\n",
    "    predicted.append(y_hat)\n",
    "    r2_scores.append(r2_score(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b06ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equation(model):\n",
    "    \"\"\"Return a string with the parameters for a linear model.\"\"\"\n",
    "    return f\"y = {model.intercept_:.3g} + {model.coef_[0]:.3g} * x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values for the different data sets\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    ncols=2,\n",
    "    nrows=2,\n",
    "    figsize=(8, 8),\n",
    ")\n",
    "axes = axes.flatten()\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[2].set_ylabel(\"y\")\n",
    "\n",
    "for i, axi in enumerate(axes):\n",
    "    axi.scatter(X, all_y[i])\n",
    "    axi.plot(\n",
    "        X,\n",
    "        predicted[i],\n",
    "        label=f\"R² = {r2_scores[i]:.3g}\\n{model_equation(models[i])}\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "    axi.legend()\n",
    "    axi.set(xlabel=\"x\", title=f\"Data set: {i+1}\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e110d58",
   "metadata": {},
   "source": [
    "Here, we see that the influence of the unusual points differs in the different data sets.\n",
    "Essentially, there are two contributions: how unusual the $y$-value is and how unusual the $x$-value is.\n",
    "We will now look for the unusual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce05af6",
   "metadata": {},
   "source": [
    "## Finding unusual $x$-values\n",
    "\n",
    "We will first focus on finding points that are unusual along the $x$-values. The $x$-values are\n",
    "common for all 4 data sets, so we will only do it for data set number 1.\n",
    "\n",
    "To do this,\n",
    "we will calculate the so-called **leverage score** which can be found from\n",
    "the diagonal elements of the projection matrix. The leverage scores will help us locate\n",
    "unusual $x$-values. The motivation is as follows:\n",
    "\n",
    "If we know the projection matrix, $\\textbf{H}$, then we can get the y-values\n",
    "estimated by the model ($\\hat{\\textbf{y}}$) directly from the measured y-values ($\\textbf{y}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\textbf{y}} = \\textbf{H} \\textbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Let us say that we have $m$ $y$-values and that the elements of the matrix $\\textbf{H}$ are $h_{ij}$.\n",
    "If we write out the esimation of $\\hat{y}_i$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}_i = h_{i1} y_1 + h_{i2} y_2 + \\ldots + h_{ii} y_i + \\ldots + h_{im} y_m\n",
    "\\end{equation}\n",
    "\n",
    "and we see here that $h_{ii} \\times y_i$ gives the contribution of point $y_i$ to the estimation of point\n",
    "$\\hat{y}_i$. One can show the following properties for $h_{ii}$:\n",
    "\n",
    "* the sum of $h_{ii}$ equals the number of coefficients in the linear model\n",
    "* $0 \\leq h_{ii} \\leq 1$\n",
    "* $h_{ii}$ measures the distance between $x_i$ and the mean of all $x$-values\n",
    "\n",
    "So, if $h_{ii}$ is \"large\" this means that obervation no. $i$ is very important for predicting $\\hat{y}$.\n",
    "Let us see what these $h_{ii}$ elements look like for our current $\\textbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate projection matrix\n",
    "H = X @ np.linalg.pinv(X)\n",
    "h = np.diagonal(H)\n",
    "# Check the sum:\n",
    "print(\"Sum of diagonal elements (should be 1):\", h.sum())\n",
    "print(\"Smallest diagonal element:\", h.min())\n",
    "print(\"Largest diagonal element:\", h.max())\n",
    "print(\"Second largest diagonal element:\", np.sort(h)[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96b219",
   "metadata": {},
   "source": [
    "Here, we see that the largest diagonal element is 4 times greater than the second largest. So this can potentially\n",
    "be an unusual point. Different progams use different rules-of-thumb to flag \"large\" $h_{ii}$'s. Two common\n",
    "choices are:\n",
    "\n",
    "* $h_{ii} > 3 \\times \\langle h \\rangle = 3 \\times \\frac{k}{m}$\n",
    "* $h_{ii} > 2 \\times \\langle h \\rangle = 2 \\times \\frac{k}{m}$\n",
    "\n",
    "where $\\langle h \\rangle$ denotes the average of the diagonal elements, $k$ is the number of\n",
    "coefficients, and $m$ is the number of observations. Let us plot all $h_{ii}$'s and add the thresholds above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "pos = range(len(h))\n",
    "ax.bar(pos, h)\n",
    "ax.set(xlabel=\"Observation number\", ylabel=\"Leverage score\")\n",
    "threshold_3 = 3.0 * h.mean()\n",
    "threshold_2 = 2.0 * h.mean()\n",
    "ax.axhline(y=threshold_3, ls=\":\", color=\"k\", label=\"Threshold (3 × mean)\")\n",
    "ax.axhline(y=threshold_2, ls=\"--\", color=\"k\", label=\"Threshold (2 × mean)\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ce6a3",
   "metadata": {},
   "source": [
    "In general, it can be difficult to use the thresholds above. Another approach is to look for points in the\n",
    "figure above that seem to be significantly different from the others. Here, it seems to be point no. 31.\n",
    "Let us label this in the original data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\", title=\"Data set 1\")\n",
    "ax.scatter(X, y1, label=\"All points\")  # Draw all points\n",
    "\n",
    "idx = np.where(h > threshold_3)[0]  # Select points with h > threshold_3\n",
    "# You can swap the threshold_3 to threshold_2 to see if there is any difference\n",
    "\n",
    "ax.scatter(X[idx], y1[idx], alpha=0.5, label=\"Unusual point(s)!\", s=100)\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d65ccc",
   "metadata": {},
   "source": [
    "This is sort of what we expect: one of the $x$-values is significantly different from the other points, and\n",
    "now we have found which one it is. We should check this point in more detail to see if there is anything unusual about it and if we should keep it when training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfb523",
   "metadata": {},
   "source": [
    "## Finding unusual $y$-values\n",
    "\n",
    "Next, we will look for points that are important for the calculation of regression parameters. We do this by\n",
    "calculating the [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance).\n",
    "For each observation ($i$), we calculate the Cook's distance $D_i$ by:\n",
    "\n",
    "\\begin{equation}\n",
    "D_i = \\frac{(y_i - \\hat{y}_i)^2}{k s^2} \\left( \\frac{h_{ii}}{(1 - h_{ii})^2} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $s^2 = \\frac{\\sum_i^m (y_i - \\hat{y})^2}{m - k}$ is the mean squared error. We could do\n",
    "the same for observation number $i$ by training a new least squares model on a data set with observation number $i$\n",
    "removed, and comparing the new model with the old. This is a lot of work to do for all data points, so we\n",
    "prefer to use the simple formula above.\n",
    "\n",
    "Again, the Cook's distance is just a number, and we need some way of determining if a distance is \"large\".\n",
    "That is, we need some way of saying that a point influences the parameters a lot. A rule-of-thumb is to use the\n",
    "value $4/m$ as a cut-off. Different programs might use different threshold values, so we can also\n",
    "trust ourselves and look for distances that \"look\" unusual. Let us see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cedcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cook's distance:\n",
    "rank = np.linalg.matrix_rank(X)\n",
    "dof = X.shape[0] - rank\n",
    "k = X.shape[1]\n",
    "\n",
    "cook = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_hat = predicted[i]\n",
    "    y = all_y[i]\n",
    "    residual = y - y_hat\n",
    "    mse = np.dot(residual, residual) / dof  # this is s^2\n",
    "    # Calculate all distances:\n",
    "    dist = (residual**2 / (k * mse)) * (h / (1 - h) ** 2)\n",
    "    cook.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, nrows=2, figsize=(8, 8), sharex=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "axes[0].set_ylabel(\"Cook's distance\")\n",
    "axes[2].set_ylabel(\"Cook's distance\")\n",
    "axes[2].set_xlabel(\"Observation number\")\n",
    "axes[3].set_xlabel(\"Observation number\")\n",
    "\n",
    "for i, axi in enumerate(axes):\n",
    "    dist = cook[i]\n",
    "    pos = range(len(dist))\n",
    "    axi.bar(pos, dist)\n",
    "    threshold_cook = 4.0 / len(dist)\n",
    "    axi.axhline(y=threshold_cook, color=\"k\", ls=\":\", label=\"Threshold\")\n",
    "    axi.set(title=f\"Data set {i+1}\")\n",
    "    if i == 0:\n",
    "        axi.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121a9a1",
   "metadata": {},
   "source": [
    "From the figure above, we conclude that data set 1 and 3 does not contain any very influential points, while\n",
    "data set 2 and 4 does! Let us label these points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values for the different data sets\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    ncols=2,\n",
    "    nrows=2,\n",
    "    figsize=(8, 8),\n",
    ")\n",
    "axes = axes.flatten()\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[2].set_ylabel(\"y\")\n",
    "\n",
    "for i, axi in enumerate(axes):\n",
    "    axi.scatter(X, all_y[i])\n",
    "    axi.plot(\n",
    "        X,\n",
    "        predicted[i],\n",
    "        label=f\"R² = {r2_scores[i]:.3g}\\n{model_equation(models[i])}\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "    axi.set(xlabel=\"x\", title=f\"Data set: {i+1}\")\n",
    "    dist = cook[i]\n",
    "    threshold_cook = 4.0 / len(dist)\n",
    "    idx = np.where(dist > threshold_cook)[0]\n",
    "    if len(idx) > 0:\n",
    "        axi.scatter(\n",
    "            X[idx],\n",
    "            all_y[i][idx],\n",
    "            alpha=0.5,\n",
    "            label=\"Influential point(s)!\",\n",
    "            s=100,\n",
    "        )\n",
    "    axi.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44893e",
   "metadata": {},
   "source": [
    "In the figure above we have marked the points that influence the parameters of the linear model a lot.\n",
    "These are points we should investigate further. If they are outliers, we can try to delete them, and remake\n",
    "the model. Let us try this for data set number 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values for the different data sets\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(X, y4)\n",
    "ax.plot(\n",
    "    X,\n",
    "    predicted[i],\n",
    "    label=f\"All points:\\nR² = {r2_scores[i]:.3g}\\n{model_equation(models[i])}\",\n",
    "    lw=2,\n",
    ")\n",
    "dist = cook[-1]\n",
    "threshold_cook = 4.0 / len(dist)\n",
    "idx = np.where(dist > threshold_cook)[0]\n",
    "if len(idx) > 0:\n",
    "    ax.scatter(\n",
    "        X[idx], all_y[i][idx], alpha=0.5, label=\"Influential point(s)!\", s=100\n",
    "    )\n",
    "\n",
    "# Remove the unusual points, and make a new model:\n",
    "X_removed = np.delete(X, idx).reshape(-1, 1)\n",
    "y_removed = np.delete(y4, idx)\n",
    "\n",
    "new_model = LinearRegression(fit_intercept=True)\n",
    "new_model.fit(X_removed, y_removed)\n",
    "y_hat = new_model.predict(X)\n",
    "text = (\n",
    "    \"Without influential point(s):\\n\"\n",
    "    f\"R² = {r2_score(y_removed, new_model.predict(X_removed)):.3g}\\n\"\n",
    "    f\"{model_equation(new_model)}\"\n",
    ")\n",
    "ax.plot(X, y_hat, ls=\":\", label=text, lw=3)  # color=\"#964a8b\")\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\", title=f\"Data set: 4\")\n",
    "ax.legend(labelspacing=1.2)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69560e7d",
   "metadata": {},
   "source": [
    "**Note**: The Python package [yellowbrick](https://www.scikit-yb.org/en/latest/) can calculate and display\n",
    "Cook's distances for us, see [this example](https://www.scikit-yb.org/en/latest/api/regressor/influence.html) for more information.\n",
    "\n",
    "You can also experiment with methods that are robust to outliers, for instance\n",
    "[RANSAC](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html),\n",
    "[Theil Sen](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html), or\n",
    "[Huber regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor). [Here is a short comparison](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors)\n",
    "of these options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
