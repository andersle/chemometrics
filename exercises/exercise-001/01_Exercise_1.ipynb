{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Exercise set 1: Least squares regression\n",
    "\n",
    "This exercise focuses on practical applications of least squares regression with Python. You will learn how to apply least squares regression to fit models of different kinds and how to evaluate the results.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Apply ordinary least squares regression to fit a linear model to data, using [NumPy](https://numpy.org/), [SciPy](https://scipy.org/), [scikit-learn](https://scikit-learn.org/), and [statsmodels](https://www.statsmodels.org/).\n",
    "* Compare the quality of different regression models by inspecting their residuals.\n",
    "* Apply weighting to data points in least squares regression.\n",
    "* Estimate errors for coefficients by obtaining their confidence intervals.\n",
    "* Use non-linear least squares regression.\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [1.1(c)](#1.1(c)): Ordinary least squares regression and comparison of quality by residuals.\n",
    "- [1.3(b)](#1.3(b)): Non-linear least squares regression.\n",
    "\n",
    "**Files required for this exercise:**\n",
    "* For [Exercise 1.1](#Exercise-1.1:-Polynomial-regression-with-least-squares): [temperature.csv](temperature.csv)\n",
    "* For [Exercise 1.2](#Exercise-1.2:-Weighted-least-squares): [erdinger.csv](erdinger.csv)\n",
    "Please ensure that these files are saved in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1759",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Polynomial regression with least squares\n",
    "\n",
    "The temperature (°C) is measured continuously over time at a high-altitude\n",
    "in the atmosphere using a\n",
    "weather balloon. Every hour, a measurement is made and sent to an on-board computer.\n",
    "The measurements are \n",
    "shown in Figure 1 and can be found in the [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) file [temperature.csv](temperature.csv).\n",
    "\n",
    "<figure>\n",
    "<img src=\"Fig_1_1.png\" width=\"50%\">\n",
    "<figcaption><p style='text-align: center;'><b>Figure 1:</b> Measured temperature as a function of time.</p></figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75853a",
   "metadata": {},
   "source": [
    "### 1.1(a)\n",
    "\n",
    "To model the temperature ($y$) as a function of time ($x$), we choose a second-order polynomial:\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2.\n",
    "\\end{equation}\n",
    "\n",
    "Explain how you can formulate this in a form suitable for least-squares regression,\n",
    "$\\mathbf{y} = \\mathbf{X} \\mathbf{b}$. That is, **what do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ and the matrix $\\mathbf{X}$ contain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418ebb",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(a): What do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ and the matrix $\\mathbf{X}$ contain?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e143df",
   "metadata": {},
   "source": [
    "### 1.1(b)\n",
    "\n",
    "Fit a second-order polynomial model,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "to the given data by making use of [numpy.polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). \n",
    "\n",
    "1. Obtain the parameters $b_0$, $b_1$, and $b_2$.\n",
    "2. Plot your model: Create a scatter plot of the original data points and overlay the fitted quadratic curve to visualise the model's fit.\n",
    "3. Calculate the [residuals](https://en.wikipedia.org/wiki/Errors_and_residuals) and create a scatter plot of the residuals against the fitted values. \n",
    "4. Based on your results, how do you assess your model? Please see [What is Considered a Good vs. Bad Residual Plot?](https://www.statology.org/good-vs-bad-residual-plot/) for a short explanation of what to look for in the residual plot.\n",
    "\n",
    "Below, you will find some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data:\n",
    "data = pd.read_csv(\"temperature.csv\")\n",
    "# Show the first few rows of the data:\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452d298",
   "metadata": {},
   "source": [
    "To fit a polynomial to your data, you have several options. Here are three popular choices:\n",
    "\n",
    "1. [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). This is the simplest option, suitable for most basic polynomial fitting tasks. It uses a least squares approach to fit a polynomial of a given degree to your data. \n",
    "2. [Ordinary least squares (OLS)](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html) from [statsmodels](https://www.statsmodels.org): This method provides more detailed results than polyfit, including error estimates for the coefficients. It is a good choice if you need more information about the fit. \n",
    "3. [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [scikit-learn](https://scikit-learn.org/): This is a more general approach that can be used for a variety of regression tasks, including polynomial fitting. It is particularly useful if you want to combine polynomial fitting with other methods, such as cross-validation.\n",
    "\n",
    "For options 2 and 3, we have to \"construct\" the $\\mathbf{X}$-matrix (see [1.1(a)](#1.1(a))), while `polyfit` does this automatically. We select the simplest option and use `polyfit` in this exercise.\n",
    "\n",
    "Here is an example for finding a first-order polynomial to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data:\n",
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "\n",
    "# Fit a first-order polynomial:\n",
    "param = np.polyfit(x, y, deg=1)  \n",
    "\n",
    "# Show the parameters found (stored in `param`):\n",
    "equation = f\"y = {param[0]:.3f}x + {param[1]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval.\n",
    "# This will evaluate the polynomial using the parameters\n",
    "# we found at each x-value:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# Compute the residuals:\n",
    "residual = y - y_hat\n",
    "\n",
    "# Plot the fitted polynomial and residuals:\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True,  # Remove some whitespace in the figure\n",
    "    ncols=2,  # Create two columns\n",
    "    figsize=(8, 4)  # Adjust the size of the plot\n",
    ")\n",
    "# Plot the raw data:\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "\n",
    "# Plot the fitted curve:\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "\n",
    "# Show a legend:\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the residuals:\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "\n",
    "# Remove some of the spines (top and right lines around the plot):\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe755f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for fitting the second-order model, plotting it, and the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c95ca",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(b): What are the coefficients of the second-order polynomial and how do you assess (based on the two plots you made) your model?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee026a2",
   "metadata": {},
   "source": [
    "### 1.1(c)\n",
    "\n",
    "In this problem, you will explore how the choice of polynomial degree affects the model's ability to fit the temperature data.\n",
    "\n",
    "1. Extend your code from [1.1(b)](#1.1(b)) to fit polynomial models of degrees 1 to 5 to the temperature data.\n",
    "2. Create a single plot displaying the raw data as a scatter plot and overlay the fitted curves for all five models as lines.\n",
    "3. Plot the residuals for each model in separate scatter plots.\n",
    "\n",
    "Which polynomial degree best models temperature as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5874b4",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(c): Which polynomial degree best models temperature as a function of time?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863e1c9",
   "metadata": {},
   "source": [
    "### 1.1(d)\n",
    "Obtain the [sum of squared residuals](https://en.wikipedia.org/wiki/Residual_sum_of_squares) for each polynomial you made in [1.1(c)](#1.1(c)) and plot this as a function of the\n",
    "polynomial degree. \n",
    "\n",
    "Use this plot to determine (from visual inspection) the best polynomial\n",
    "degree for modelling the temperature as a function of time. Does this agree with your\n",
    "assessment from [1,1(c)](#1.1(c))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56429fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ca394",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(d): What polynomial degree do you recommend?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36ee05",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Weighted least squares\n",
    "\n",
    "In this exercise, we will use least-squares regression to investigate a real-world phenomenon: the decay of beer froth over time. The goal is to illustrate how regression can be used to extract meaningful physical quantities and quantify their associated uncertainties.\n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this [research on the decay of beer froth](https://doi.org/10.1088/0143-0807/23/1/304), and we will here reproduce the data analysis. In particular, we will use the reported raw data and carry out a weighted least squares regression. In addition, we will also obtain an error estimate (as a confidence interval) for the determined physical quantity.\n",
    "\n",
    "\n",
    "The file [erdinger.csv](erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights. \n",
    "\n",
    "\n",
    "**Please use [scikit-learn](https://scikit-learn.org/) and [statsmodels](https://www.statsmodels.org) for the fitting in this exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c00b5",
   "metadata": {},
   "source": [
    "### 1.2(a)\n",
    "Create a linear model (first-order polynomial) for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the [coefficient of determination ($R^2$)](https://en.wikipedia.org/wiki/Coefficient_of_determination), and plot\n",
    "the residuals. Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "**Note:** You do not need to write the code for this part, but ensure that you understand the implementation and the resulting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data:\n",
    "data = pd.read_csv(\"erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data as NumPy arrays:\n",
    "time = data[\"time\"].to_numpy()\n",
    "height = data[\"height\"].to_numpy()\n",
    "height_error = data[\"height-error\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for using scikit-learn:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# To fit a model with scikit-learn, we do the following:\n",
    "model1 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# We create an X-matrix for the fitting by reshaping 'time':\n",
    "X = time.reshape(-1, 1)\n",
    "model1.fit(X, height)\n",
    "\n",
    "# We can use the model for prediction by:\n",
    "y_hat_1 = model1.predict(X)\n",
    "\n",
    "# To calculate R²:\n",
    "r2_model1 = model1.score(X, height)\n",
    "# or:\n",
    "r2_model1 = r2_score(height, y_hat_1)\n",
    "\n",
    "# To calculate the mean squared error (MSE) for the model:\n",
    "mse_model1 = mean_squared_error(height, y_hat_1)\n",
    "# To summarise the model with a short text string:\n",
    "model1_txt = f\"y = {model1.coef_[0]:.3g}x + {model1.intercept_:.3g}\"\n",
    "model1_txt = f\"{model1_txt}\\n(R² = {r2_model1:.3g}, MSE = {mse_model1:.3g})\"\n",
    "print(model1_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ed443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results:\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "# Add the raw data with error bars:\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Show only the markers, no connecting lines\n",
    "    capsize=4,  # Adjust the width of the error bar caps\n",
    ")\n",
    "# Plot the fitted curve:\n",
    "ax1.plot(\n",
    "    time,\n",
    "    y_hat_1,\n",
    "    lw=3,\n",
    "    label=model1_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "# Plot the residuals:\n",
    "ax2.scatter(y_hat_1, height - y_hat_1)\n",
    "ax2.set(xlabel=\"Predicted by the model (ŷ)\", ylabel=\"Residuals (y - ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faead54",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(a): Is this linear model suitable for estimating the froth height as a function of time?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a6adb",
   "metadata": {},
   "source": [
    "### 1.2(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that this leads to\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter (characteristic time constant).\n",
    "We will assume that $h(0)$ is a known parameter, equal to the initial height of the froth.\n",
    "\n",
    "Show how you can transform the equation above into a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12feeb3",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(b):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941180",
   "metadata": {},
   "source": [
    "### 1.2(c)\n",
    "Use the linear transformation you found in [1.2(b)](#1.2(b)) to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$.\n",
    "\n",
    "**Hint:** The equation, $y=bx$, above does not include the usual constant term.\n",
    "This will modify the least-squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept).\n",
    "To do the fitting without the intercept, set `fit_intercept=False` when creating the linear regression model:\n",
    "```python\n",
    "model2 = LinearRegression(fit_intercept=False)  # New model, without intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade452",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(c): What value did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae3b04",
   "metadata": {},
   "source": [
    "### 1.2(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276 \\pm 14$s, which is likely lower than the value you obtained in [1.2(c)](#1.2(c)).\n",
    "We will now attempt to reproduce the results of Leike by using weighted least squares regression. The motivation for this approach is that the measurement errors in the raw data are not constant. Weighted regression accounts for this by assigning more influence to data points with smaller uncertainties.\n",
    "\n",
    "To assign the weights ($w_i$) we can use $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting log-transformed values to $y = \\log (h(t) / h(0))$, and this will modify the errors.\n",
    "If you are familiar with [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that the error in $y$ ($\\sigma_y$) is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_y^2 = \\frac{\\sigma_h^2}{h^2} ,\n",
    "\\end{equation}\n",
    "\n",
    "which says that we can get the error in $y$ by dividing the measured error in the height ($\\sigma_h$) by the measured height ($h$).\n",
    "\n",
    "Do the following steps to perform the weighted least squares:\n",
    "\n",
    "1. Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "2. Calculate weights for your $y$ values as $1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "3. Perform a weighted least squares fitting (see the example below) using the calculated weights. Estimate $\\tau$, plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to run weighted least squares:\n",
    "\n",
    "# Set up the model:\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Create some weights (note: these are for illustration and not correct for 1.2(d))\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "\n",
    "# Handle potential division by zero by setting infinite weights to zero:\n",
    "weights[weights == float(\"inf\")] = 0\n",
    "\n",
    "# Perform the fit using the weights:\n",
    "model.fit(X, height, sample_weight=weights)\n",
    "\n",
    "# Calculate R² (ensuring the weights are considered in the calculation):\n",
    "r2 = model.score(X, height, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe90764",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(d): What value did you get for $\\tau$? How does it compare to Leike's result?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd117a24",
   "metadata": {},
   "source": [
    "### 1.2(e)\n",
    "We can use the measured errors to estimate the uncertainty in the $\\tau$ parameter. Adopt the example code below, using [statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/wls.html) to compute a 95% confidence interval for $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for obtaining a 95% confidence interval.\n",
    "\n",
    "# Use statsmodels:\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the data for statsmodels:\n",
    "X = time.reshape(-1, 1)\n",
    "\n",
    "# Obtain weights:\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "# Set infinite values to zero:\n",
    "weights[weights == float(\"inf\")] = 0\n",
    "\n",
    "# Create the model and fit it:\n",
    "model_wls = sm.WLS(height, X, weights=weights)\n",
    "results_wls = model_wls.fit()\n",
    "\n",
    "# Display the full statistical summary:\n",
    "# alpha=0.05 calculates a 95% confidence interval (100 * (1 - alpha)%).\n",
    "print(results_wls.summary(alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7668117",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in [Appendix B](#B.-The-summary-results-from-statsmodels). We only need this part:\n",
    "\n",
    "```code\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "x1             0.0452      0.015      2.997      0.010       0.013       0.077\n",
    "==============================================================================\n",
    "```\n",
    "\n",
    "where `coef` gives the fitted coefficient and the numbers below `[0.025      0.975]` is the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can get the confidence interval by:\n",
    "results_wls.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707141e",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(e): What confidence interval did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be7fbe",
   "metadata": {},
   "source": [
    "## Exercise 1.3: Non-linear least squares\n",
    "\n",
    "In spectroscopy, it is often necessary to \"deconvolve\" a spectrum into a number of overlapping spectral peaks. The individual peaks can often be approximated as Gaussian functions of amplitude $\\beta$, peak wavelength $\\lambda$   and standard deviation $\\sigma$. Consequently, the total spectrum $S(\\lambda)$ may be written as, \n",
    "\n",
    "\\begin{equation}\n",
    "S(\\lambda) = \\sum_{i=1}^N \\beta_i \\exp \\left( -\\frac{(\\lambda - \\lambda_i)^2}{2\\sigma_i^2} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of overlapping peaks.\n",
    "\n",
    "In this exercise, we will use non-linear least squares to determine the parameters for a spectrum consisting of $N=3$ peaks. Specifically, we will use the parameters provided in Table 1 to generate a synthetic spectrum, add random noise, and then attempt to recover the original parameters through regression.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Parameters for spectral peaks</p>  \n",
    "\n",
    "|             | Peak 1   | Peak 2   | Peak 3   |\n",
    "|-------------|----------|----------|----------|\n",
    "| $\\beta_i$   | 0.2      | 0.4      | 0.3      |\n",
    "| $\\lambda_i$ | 4.0      | 5.5      | 7.2      |\n",
    "| $\\sigma_i$  | 0.5      | 0.8      | 0.9      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b013cf",
   "metadata": {},
   "source": [
    "### 1.3(a)\n",
    "\n",
    "We will first generate a synthetic spectrum using the parameters in Table 1 to use for the non-linear least-squares fitting. To create the noise-free absorption spectrum, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For displaying matplotlib plots within the Jupyter Notebook:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters from Table 1:\n",
    "betas = [0.2, 0.4, 0.3]\n",
    "lambdas = [4.0, 5.5, 7.2]\n",
    "sigmas = [0.5, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def gaussian(x, beta_i, lambda_i, sigma_i):\n",
    "    \"\"\"Evaluate a Gaussian function at points x.\"\"\"\n",
    "    return beta_i * np.exp(-((x - lambda_i) ** 2) / (2.0 * sigma_i ** 2))\n",
    "\n",
    "# Create the noise-free spectrum, sample it at 100 points between 0 and 10:\n",
    "x = np.linspace(0, 10, 100)\n",
    "spectrum_without_noise = np.zeros_like(x)\n",
    "\n",
    "# Sum the individual Gaussian peaks to create the synthetic spectrum:\n",
    "for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "    spectrum_without_noise += gaussian(x, beta_i, lambda_i, sigma_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fbc5f",
   "metadata": {},
   "source": [
    "Next, create a noisy spectrum by adding zero-mean Gaussian noise with a standard deviation $\\gamma=0.025$, using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.025\n",
    "\n",
    "# Since we will be using random numbers, we set the\n",
    "# seed to get reproducible numbers:\n",
    "np.random.seed(4175)\n",
    "\n",
    "# Generate Gaussian noise and add it to the original spectrum:\n",
    "noise = np.random.normal(loc=0, scale=gamma, size=spectrum_without_noise.shape)\n",
    "spectrum_with_noise = spectrum_without_noise + noise\n",
    "\n",
    "# Show the spectrum with and without noise:\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, spectrum_with_noise, label=\"With noise\")\n",
    "ax.plot(x, spectrum_without_noise, color=\"k\", label=\"Without noise\")\n",
    "ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e938f8",
   "metadata": {},
   "source": [
    "### 1.3(b)\n",
    "\n",
    "Use non-linear least squares to estimate the Gaussian parameters for the noisy spectrum you generated in [1.3(a)](#1.3(a)). Investigate how the solution depends on your initial guess and the noise level:\n",
    "\n",
    "- Initial guess: Compare \"good\" initial guesses (e.g., those from Table 1) with \"bad\" values (e.g. placing the peak centres far from their actual positions).\n",
    "- Noise level: Try running the fitting for a signal containing a higher level of noise, for instance, by setting $\\gamma=0.05$ in [1.3(a)](#1.3(a)).\n",
    "\n",
    "**Hint:** The code below outlines how to perform the fit using the noise-free spectrum.\n",
    "\n",
    "**Note:** We use [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) from [SciPy](https://scipy.org/) to run non-linear least squares. You can also use [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) from [SciPy](https://scipy.org/) for non-linear least squares: this is simpler and requires less setup, but you have less control over the optimisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the model we will fit:\n",
    "def model(x, params):\n",
    "    \"\"\"Calculate the total spectral signal using the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x: The independent variable (e.g., wavelength).\n",
    "        params: A flat list of parameters for N peaks.\n",
    "            The length must be a multiple of 3, ordered as:\n",
    "            [beta_1, lambda_1, sigma_1, beta_2, lambda_2, sigma_2, ...]\n",
    "\n",
    "    Returns:\n",
    "        The value of the function at each given x.\n",
    "    \"\"\"\n",
    "    betas = params[0::3]\n",
    "    lambdas = params[1::3]\n",
    "    sigmas = params[2::3]\n",
    "    \n",
    "    signal = np.zeros_like(x)\n",
    "\n",
    "    for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "        signal += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the objective function we will minimise:\n",
    "def objective(params, x, y):\n",
    "    \"\"\"Calculate sum of squared errors between the model and the data.\"\"\"\n",
    "    y_model = model(x, params)\n",
    "    residuals = y - y_model\n",
    "    # Return sum of squared errors:\n",
    "    return np.sum(residuals**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimisation:\n",
    "\n",
    "# Initial guess for the parameters:\n",
    "initial_guess = [\n",
    "    0.3, 4.0, 1.0,  # Peak 1\n",
    "    0.3, 6.0, 1.0,  # Peak 2\n",
    "    0.3, 8.0, 1.0,  # Peak 3\n",
    "]\n",
    "\n",
    "# Set up boundaries for the parameters, these\n",
    "# are in the form (min, max) for each parameter\n",
    "bounds = [\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 1\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 2\n",
    "    (0.01, 2.0), (0.0, 10.0), (0.01, 2.0),  # Peak 3\n",
    "]\n",
    "\n",
    "# Run the minimisation:\n",
    "result = minimize(\n",
    "    objective,\n",
    "    initial_guess,\n",
    "    args=(x, spectrum_without_noise),\n",
    "    bounds=bounds,\n",
    "    options={\n",
    "        \"maxiter\": 5000,\n",
    "    },\n",
    ")\n",
    "# Check if the optimisation converged successfully:\n",
    "print(result.message)\n",
    "\n",
    "# Extract the fitted parameters:\n",
    "fit_params = result.x\n",
    "print(f\"Fitted parameters: {fit_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate using the fitted parameters:\n",
    "y_model = model(x, fit_params) \n",
    "\n",
    "# Plot the fitted signal\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(x, spectrum_without_noise, label=\"True signal\", lw=5, alpha=0.4)\n",
    "ax.plot(x, y_model, label=\"Fitted model\", color=\"black\", lw=2, ls=\":\")\n",
    "ax.set(xlabel=r\"$\\lambda$\", ylabel=r\"$S(\\lambda)$\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c4526",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(b): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise level?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1eb9e",
   "metadata": {},
   "source": [
    "### 1.3(c)\n",
    "\n",
    "Modify your code to use separable least squares (SLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f36d",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(c): What values did you find for the parameters? Are they different from the ones found in [1.3(b)](#1.3(b))?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71138c6c",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1159e",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimising the sum of squared errors ($S$). With $n$\n",
    "measurements of $y$ and $x$,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2 = \\sum_{i=1}^n r_i^2\n",
    "\\end{equation}\n",
    "\n",
    "To minimise $S$ we calculate the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimisation,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in \"normal\" least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9ef76",
   "metadata": {},
   "source": [
    "## B. The summary results from `statsmodels`\n",
    "\n",
    "The summary method in `statsmodels` prints out a lot of information.\n",
    "Here is an example where we have fitted a model $y=a + bx$ to 10 $(x, y)$ points with `statsmodels`\n",
    "and the resulting summary output is printed below. In the table below, the parameter `a` corresponds to `const` and `b` corresponds to `x1`. The output is described further in the\n",
    "sections below. \n",
    "\n",
    "```text\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                      y   R-squared:                       0.956\n",
    "Model:                            OLS   Adj. R-squared:                  0.951\n",
    "Method:                 Least Squares   F-statistic:                     175.6\n",
    "Date:                Tue, 14 Feb 2023   Prob (F-statistic):           1.00e-06\n",
    "Time:                        08:42:06   Log-Likelihood:                -16.957\n",
    "No. Observations:                  10   AIC:                             37.91\n",
    "Df Residuals:                       8   BIC:                             38.52\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          4.4248      0.931      4.754      0.001       2.278       6.571\n",
    "x1             1.9235      0.145     13.253      0.000       1.589       2.258\n",
    "==============================================================================\n",
    "Omnibus:                        3.674   Durbin-Watson:                   2.067\n",
    "Prob(Omnibus):                  0.159   Jarque-Bera (JB):                0.755\n",
    "Skew:                           0.464   Prob(JB):                        0.686\n",
    "Kurtosis:                       3.975   Cond. No.                         13.0\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be87c33",
   "metadata": {},
   "source": [
    "### B.1. Information about the model\n",
    "\n",
    "\n",
    "- **Dep. Variable:** The dependent variable (the variable we are predicting, $y$) in the model.\n",
    "- **Model:** The type of model we have created (OLS = Ordinary Least Squares).\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date & Time:** The date and time for when we created the model.\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 ($x$,$y$) values here).\n",
    "\n",
    "### B.2. Information about the calculation\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares). \n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of predictors (variables excluding the constant).\n",
    "  In our case: $n - k - 1 = 10 - 1 - 1 = 8$. If we did the\n",
    "  fitting without the constant term (for instance, by centring the data first), this\n",
    "  number would be $n-k = 10-1=9$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** Calculations of standard errors assume homoscedastic errors.\n",
    "  If this is not the case, then the standard error is not computed correctly. There\n",
    "  are alternative ways of calculating the standard error; this field tells you\n",
    "  if statsmodels used a more robust method.\n",
    "  \n",
    "### B.3. Information about the overall quality\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "- **Adj. R-squared:** The adjusted $R^2$ for the model. Useful for comparing\n",
    "  models as this one will only increase (when adding more variables) if the\n",
    "  increase in $R^2$ is more than one would expect by chance.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero! Effectively, this compares the model we\n",
    "  have just made to an alternative model equal to the constant intercept term. \n",
    "  To use this value, we would have to decide on a $\\alpha$ level and look up a critical F-value.\n",
    "  This is some extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at\n",
    "  least as extreme as the one above if all regression coefficients are zero. \n",
    "  It is also known as the $p$-value.\n",
    "  If we have selected $\\alpha$ value, we will reject the null hypothesis if \n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis: We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error.\n",
    "  This is equivalent (if the errors are normally distributed)\n",
    "  to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** The\n",
    "  [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n",
    "  [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "  These can be directly calculated from the Log-Likelihood and are useful for comparing alternative\n",
    "  models. Generally, we prefer models with lower AIC and BIC.\n",
    "  \n",
    "### B.4. Information about the coefficients\n",
    "\n",
    "- **coef:** The determined coefficients for the model.\n",
    "\n",
    "- **std err:** The standard error of the coefficients. This\n",
    "  is calculated from,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  s^2 = \\frac{SSE}{n - k - 1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "\n",
    "- **t, P>|t|, and [0.025 0.975]:** Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the $p$-value for such a $t$-test.\n",
    "  Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, these coefficients are indeed different from\n",
    "  zero.\n",
    "  Finally, the **[0.025 0.975]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not specify \n",
    "  $\\alpha$ here, but we can give it as a parameter. The default is $\\alpha=0.05$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
