{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080a4ead",
   "metadata": {},
   "source": [
    "# Least squares example 1\n",
    "\n",
    "In this notebook, we will determine parameters for the equation\n",
    "\n",
    "\\begin{equation}\n",
    "y = 3 + 2.2x\n",
    "\\end{equation}\n",
    "\n",
    "by least squares regression. That is, we will fit an equation on the form\n",
    "\n",
    "\\begin{equation}\n",
    "y = a + b \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "to data from the line $y = 3 + 2.2x$.\n",
    "\n",
    "Of course, here we know that $a=3$ and $b=2.2$, but we will\n",
    "pretend we do not know this, and we will see if we can find those parameters by doing\n",
    "a least squares fit. Our approach is the following:\n",
    "\n",
    "1. We generate some x and y values that follow the relation $y = 3 + 2.2 x$.\n",
    "\n",
    "2. We fit an equation on the form $y = a + b\\cdot x$ to the generated values,\n",
    "   and we check if get $a=3$ and $b=2.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5a515",
   "metadata": {},
   "source": [
    "## Generating values for $y= 3 + 2.2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import some libraries for generating values and plotting:\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d072900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some values we will use for solving the least squares problem:\n",
    "x = np.linspace(1, 10, 10)\n",
    "print(x)\n",
    "y = 3 + 2.2 * x\n",
    "print(y)\n",
    "# Also plot them:\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='x', ylabel='y = 3+ 2.2*x')\n",
    "ax.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d5c5e",
   "metadata": {},
   "source": [
    "## Option 1: Explicitly implementing the equations:\n",
    "\n",
    "The least squares solution to $y = a + bx$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\overline{y} - b \\overline{x}, \\quad b = \\frac{\\sum_{i} (y_i - \\overline{y})(x_i - \\overline{x})}{\\sum_i (x_i - \\overline{x})^2}, \n",
    "\\end{equation}\n",
    "\n",
    "where $\\overline{y}$ denotes the mean of $y$. We can evaluate this directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116331f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the differences y_i - mean(y) for y and x:\n",
    "error_y = y - y.mean()  # The term (yi - mean(y))\n",
    "error_x = x - x.mean()  # The term (xi - mean(x))\n",
    "\n",
    "b = sum(error_y * error_x) / sum(error_x**2)\n",
    "print(f'b = {b:.3g}')\n",
    "\n",
    "a = y.mean() - b * x.mean()\n",
    "print(f'a = {a:.3g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9dafca",
   "metadata": {},
   "source": [
    "## Option 2: Explicitly using the matrix formulation\n",
    "\n",
    "In matrix notation, the least squares equation is on the form $\\mathbf{y} = \\mathbf{X}\\mathbf{b}$ where (assuming we have $n$ $y$ and $x$ values):\n",
    "\n",
    "* $\\mathbf{y}$ is a column vector (or a $n\\times 1$) matrix containing the $y$ values:\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\mathbf{y} = \\begin{pmatrix}\n",
    "  y_1 \\\\\n",
    "  y_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  y_n\n",
    "  \\end{pmatrix}\n",
    "  \\end{equation}\n",
    "  \n",
    "* $\\mathbf{b}$ is a column vector (or a $2\\times 1$) matrix containing the\n",
    "  (unknown) coefficients $a$ and $b$:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\mathbf{b} = \\begin{pmatrix}\n",
    "  a \\\\\n",
    "  b \\\\\n",
    "  \\end{pmatrix}\n",
    "  \\end{equation}\n",
    "\n",
    "* $\\mathbf{X}$ is a $n\\times 2$ matrix containing the\n",
    "  $x$ values and the constant:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\mathbf{X} = \\begin{pmatrix}\n",
    "  1 & x_1 \\\\\n",
    "  1 & x_2 \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "  1 & x_n \\\\\n",
    "  \\end{pmatrix}\n",
    "  \\end{equation}\n",
    "\n",
    "We can double-check that the dimensions make sense here for $\\mathbf{y} = \\mathbf{X}\\mathbf{b}$:\n",
    "\n",
    "* Left-hand side: This is just $\\mathbf{y}$ with dimensions $(n \\times 1)$.\n",
    "\n",
    "* Right-hand side: This is the result of a matrix product of dimensions $(n \\times 2)$ and\n",
    "  $(2 \\times 1)$: $(n \\times 2) \\times (2 \\times 1) \\to (n \\times 1)$\n",
    "  \n",
    "So we do indeed get the same dimensions on the left and right hand side.\n",
    "\n",
    "We could now think that we can solve the equation $\\mathbf{y} = \\mathbf{X}\\mathbf{b}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} = \\mathbf{X}^{-1} \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "but that only works if $\\mathbf{X}$ can be inverted. $\\mathbf{X}$ must satisfy several conditions for the inverse to exist. On such condition is that the matrix is square (same number of rows and columns).\n",
    "We have already noted that the dimensions of $\\mathbf{X}$ is $(n \\times 2)$.\n",
    "So that would limit us to $n = 2$; the case where we only have two $(x, y)$ values (just two points).\n",
    "Here, this is not the case, so we have to use the least squares solution.\n",
    "\n",
    "The least squares solution for $\\mathbf{b}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} = \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "We can check the dimensions here as well:\n",
    "\n",
    "* $\\mathbf{b}$: This is a $(2 \\times 1)$ matrix.\n",
    "\n",
    "* $\\mathbf{X}^\\top \\mathbf{X}$: This will be a $(2\\times n) \\times (n \\times 2) \\to (2 \\times 2)$ matrix. The inverse of a $(2 \\times 2)$ matrix is also a $(2 \\times 2)$ matrix.\n",
    "\n",
    "* $\\mathbf{X}^\\top \\mathbf{y}$: This will be a $(2 \\times n) \\times (n \\times 1) \\to (2 \\times 1)$ matrix.\n",
    "\n",
    "* $\\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$: From the above, this will be a $(2 \\times 2) \\times (2 \\times 1) \\to (2 \\times 1)$ matrix. This\n",
    "is equal to the dimensions for $\\mathbf{b}$.\n",
    "\n",
    "We will now calculate this solution, we begin by creating the matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the X-matrix:\n",
    "# X is a matrix where the first column is just a column of 1's, and the second\n",
    "# column are the x-values:\n",
    "ones = np.ones_like(x)  # Create a numpy array of ones, with shape like x\n",
    "print(ones)\n",
    "# Create the matrix by stacking the column of ones and the x-values:\n",
    "X = np.column_stack((ones, x))\n",
    "print('X =\\n', X)\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us just check that X cannot be inverted. This should give an error:\n",
    "Xinv = np.linalg.inv(X)  # Calculate the inverse of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for b:\n",
    "# 1. Calculate X^T X:\n",
    "mat = np.dot(X.T, X)  # dot is here the matrix product\n",
    "# Check the shape, to see if it is indeed 2x2:\n",
    "print('The shape of X.T X is:', mat.shape)\n",
    "# 2. Calculate the inverse of X.T X:\n",
    "mat_inv = np.linalg.inv(mat)\n",
    "# Check the shape, to see if it is indeed 2x2:\n",
    "print('The shape of (X.T X)^-1 is:', mat_inv.shape)\n",
    "# 3. Calculate X.T y:\n",
    "mat2 = np.dot(X.T, y)\n",
    "# Check the shape, to see if it is indeed 2x1:\n",
    "print('The shape of X.T y is:', mat2.shape)\n",
    "# 4. Calculate b:\n",
    "b = np.dot(mat_inv, mat2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do everything in one line as well (perhaps no so easy to read)\n",
    "b = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "print(b)\n",
    "# Or, NumPy defines \"@\" to mean matrix multiplication, so we can also write:\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb3ca6",
   "metadata": {},
   "source": [
    "## Option 3: Using NumPy:\n",
    "NumPy defines a method [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) that can be used to fit polynomials with least squares. We will use it here to fit a polynomial of degree 1 (a straight line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.polyfit(x, y, deg=1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f26e95",
   "metadata": {},
   "source": [
    "**Note:** The order of the parameters is different here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3d50e",
   "metadata": {},
   "source": [
    "## Option 4: Using scikit-learn\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) supports a wide range of [linear models](https://scikit-learn.org/stable/modules/linear_model.html). We will here use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model to create a least squares model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "# model.fit(x, y)  # This will give an error, sklearn is picky about dimensions!\n",
    "# sklearn want x to be a matrix, we can do this by reshaping it with reshape.(-1, 1)\n",
    "# The \"-1\" means \"we want you (numpy) to figure out what this dimension should be\"\n",
    "print(x.reshape(-1, 1))\n",
    "model.fit(x.reshape(-1, 1), y)  # Fit the model\n",
    "print(f'Intercept: {model.intercept_:.3g}')\n",
    "print(f'Coefficients: {model.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc87f3",
   "metadata": {},
   "source": [
    "## Option 5: Using a more robust matrix \"inverse\"\n",
    "\n",
    "As stated above, the solution $\\mathbf{b} = \\mathbf{X}^{-1} \\mathbf{y}$ will in general not work, since this implies that $\\mathbf{X}$ satisfies some conditions.\n",
    "\n",
    "We can, however, define an inverse that always works. This is the so-called [pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) and we use to symbol \"$+$\" in place of $-1$ to indicate that we are using the pseudoinverse.\n",
    "\n",
    "The solution to the least squares problem is then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} = \\mathbf{X}^{+} \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Let us also try this solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoinverse = np.linalg.pinv(X)\n",
    "print('Shape of pseudoinverse:', pseudoinverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.pinv(X) @ y  # Matrix product of the pseudoinverse and y:\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a2f9c",
   "metadata": {},
   "source": [
    "From the above, we could suspect that the pseudoinverse is equal to $\\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top$. It certainly looks like that would be the case from the equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat3 = np.linalg.inv(X.T @ X) @ X.T\n",
    "# Check if mat3 is (approximately) equal to the pseudoinverse:\n",
    "print('Is the pseudoinverse (approximately) equal to (X.T X)^-1 X.T?')\n",
    "np.allclose(pseudoinverse, mat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc4860",
   "metadata": {},
   "source": [
    "## Option 6: statsmodels\n",
    "\n",
    "The Python library [statsmodels](https://www.statsmodels.org/stable/index.html) focuses more on statistics.\n",
    "It will give us more statistical information for the results of a least squares fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b813758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = sm.add_constant(x)  # Add the constant column of ones (similar to what we did for X)\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "model_s = sm.OLS(y, Xs)\n",
    "result = model_s.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbce53a",
   "metadata": {},
   "source": [
    "### Interpretation of the summary from statsmodels\n",
    "\n",
    "In the summary above there are many numbers! First of all, it lists the coefficients in a table:\n",
    "\n",
    "\n",
    "|       | coef    | std err   | t         | P>\\|t\\| | \\[0.025   | 0.975\\]  |\n",
    "|-------|---------|-----------|-----------|---------|-----------|----------|\n",
    "| const | 3.0000  | 1.99e-15  | 1.51e+15  |  0.000  | 3.000     | 3.000    |\n",
    "| x1    | 2.2000  | 3.21e-16  | 6.86e+15  |  0.000  | 2.200     | 2.200    |\n",
    "\n",
    "\n",
    "The contents of this table is:\n",
    "\n",
    "- **coef**: The determined coefficients for the `const` (the constant, or intercept in the model) and `x1`\n",
    "  (the variable `x`). these coefficients are 3 and 2.2 as expected.\n",
    "\n",
    "\n",
    "- **std err**: The standard of the coefficients. This\n",
    "  is calculated from\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}\n",
    "  \\end{equation}\n",
    "  \n",
    "  where\n",
    "  \n",
    "  \\begin{equation}\n",
    "  s^2 = \\frac{SSE}{n - k - 1}\n",
    "  \\end{equation}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "  \n",
    "\n",
    "- **t**, **P>|t|**, and **[0.025 0.967]**: Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the p-value for such a t-test. Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, we think that these coefficients are indeed different from\n",
    "  zero.\n",
    "  \n",
    "  Finally, the **[0.025 0.967]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not spesify\n",
    "  $\\alpha$ here, but we can give it as a parameter (`result.summary(alpha=0.01)`). The default\n",
    "  is $\\alpha=0.05$.\n",
    "  \n",
    "If we want to reproduce these numbers, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83889364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate the standard error, t-statistic, p-value and the confidence interval:\n",
    "import scipy.stats\n",
    "k = 1  # Number of variables\n",
    "n = len(y)  # Number of data points\n",
    "SSR = sum((y - X @ b)**2)  # ŷ = Xb gives the y-values calculated by the model\n",
    "s = SSR / (n - k - 1)\n",
    "D = np.linalg.inv(X.T @ X)\n",
    "std_err = np.sqrt(s * np.diag(D))  # Pick out the diagonal elements of D.\n",
    "print('Std err:', std_err)\n",
    "t = b / std_err\n",
    "print('t:', t)\n",
    "alpha = 0.05\n",
    "p = 2 * (1 - scipy.stats.t.cdf(t, n - k - 1))\n",
    "print('p:', p)\n",
    "t_alpha_half = scipy.stats.t.ppf(1 - alpha/2, n - k - 1)\n",
    "bounds = std_err * abs(t_alpha_half)\n",
    "lower = b - bounds\n",
    "upper = b + bounds\n",
    "print('Confidence interval:', lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf38a26",
   "metadata": {},
   "source": [
    "But there are many other numbers in the output. Here is a short explanation about the different parts:\n",
    "\n",
    "#### Information about the model:\n",
    "\n",
    "- **Dep. Variable:** The dependent variable in the model. This is the variable we are predicting.\n",
    "- **Model:** The type of model we have created. OLS = Ordinary Least Squares\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date/Time:** The date and time for when we created the model\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 (x,y) values here).\n",
    "\n",
    "#### Information about the calculation:\n",
    "\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares).\n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of variables. In our case: $n - k - 1 = 10 - 1 - 1 = 8$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** The formula for the standard error used above assumes that\n",
    "  the errors are homoskedastic. If this is not the case, then the standard error\n",
    "  calculated using this formula is not correct. In statsmodels, there are\n",
    "  methods to calculate the standard errors that are heteroscedasticity robust.\n",
    "  These can be selected during fitting, for instance, `result = model_s.fit(cov_type='HC0')`.\n",
    "\n",
    "#### Information about the overall quality:\n",
    "\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "  This gives information on the overall quality of the model. We will discuss this one i more depth in\n",
    "  the lectures on least squares.\n",
    "- **Adj. R-squared:**  $R^{2}_\\text{adj}=1-(1-R^{2}){n-1 \\over n-k-1}$. Here $n$ is the number of\n",
    "  observations and $k$ is the number of variables (not including the constant term). If we make our model\n",
    "  more complex by including more variables, the ordinary $R^2$ will increase or stay unchanged\n",
    "  (it will never decrease). So including more variables (even completely irrelevant\n",
    "  ones) will typically give a higher $R^2$ and it may seem like we are making\n",
    "  a better model. The adjusted version will only increase if the\n",
    "  increase in $R^2$ is more than one would expect to see by chance.\n",
    "  The $R^2_\\text{adj}$ can be used compare models.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero. Effectively, this is comparing the model\n",
    "  we have just made, to a model which is just equal to the constant intercept term. To use this\n",
    "  value, we would have to decide on a $\\alpha$ level and look up a critical F-value. This is some\n",
    "  extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at least as extreme as the\n",
    "  one observed in the F-test above. It is also known as the $p$-value.\n",
    "  If we have selected a $\\alpha$ value, we would reject the null hypothesis if\n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis. We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "\n",
    "#### Information useful for comparing alternative models:\n",
    "These values are not immediately useful for us. The absolute values here are\n",
    "interpreted relative to alternative models and can be used for cases where we are\n",
    "considering different models for predicting $y$. Here, we only have one variable so we\n",
    "do bother trying alternative models.\n",
    "\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error. This is equivalent (if\n",
    "  the errors are normally distributed) to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) \n",
    "  and\n",
    "  [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion). These two\n",
    "  values can be calculated from the **Log-Likelihood**. Generally, we prefer models with lower AIC and BIC.\n",
    "  \n",
    "If we want to calculate these parameters ourselves, it can be done via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c90cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(n, sse):\n",
    "    \"\"\"Log-likelihood for OLS\"\"\"\n",
    "    return -0.5*n*(np.log(sse/n) + np.log(2.0*np.pi) + 1)\n",
    "\n",
    "def bic(n, k, sse):\n",
    "    \"\"\"Bayesian information criterion\"\"\"\n",
    "    return -2*loglike(n, sse) + k*np.log(n) \n",
    "\n",
    "def aic(n, k, sse):\n",
    "    \"\"\"Akaike information criterion\"\"\"\n",
    "    return -2*loglike(n, sse) + 2*k\n",
    "\n",
    "k = 1\n",
    "ssr = sum((y - X @ b)**2)\n",
    "\n",
    "print(loglike(len(y), ssr), aic(len(y), k + 1, ssr), bic(len(y), k + 1, ssr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116581c",
   "metadata": {},
   "source": [
    "#### Information about residuals:\n",
    "The residuals are useful for checking the assumption that the noise is normally distributed. This section\n",
    "contains the result of different statistical tests and it is more useful if we have many data points (not\n",
    "just 10 as we have here).\n",
    "\n",
    "- **Omnibus** and **Prob(Omnibus)**: This is a\n",
    "  [statistical test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html)\n",
    "  that checks if\n",
    "  the residuals are normally distributed. The probability indicates the\n",
    "  probability of the residuals being normally distributed.\n",
    "  \n",
    "- **Skew:** This is a measure of the asymmetry of the residuals. For a normal distribution, the skewness is 0.\n",
    "\n",
    "- **Kurtosis:** This is a measure of the \"tailedness\" of the residuals. For a normal distribution, the skewness\n",
    "  is 3.\n",
    "\n",
    "- **Jarque-Bera (JB)** and **Prob(JB)**: This is a statistical test that checks the same thing\n",
    "  as the **Omnibus** (but the test itself is different). Ideally, it should  agree with the **Omnibus**\n",
    "  test.\n",
    "\n",
    "- **Durbin-Watson:** This is a statistical test that essentially checks if there\n",
    "  is some kind of correlation (relationship) in the residuals. The value is between 0 and 4.\n",
    "  If this is equal to 2, then there is no correlation. The closer to 0 the statistic, the more\n",
    "  evidence for positive serial correlation. The closer to 4, the more\n",
    "  evidence for negative serial correlation.\n",
    "\n",
    "- **Cond. No.:** The\n",
    "  [condition number](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)\n",
    "  measures the sensitivity\n",
    "  of the solution (our parameters) to small perturbations in the input data. With just\n",
    "  one variable, this value is not so important. Statsmodels will print a warning if\n",
    "  the condition number is larger than 1000, and statsmodels interpret this as an indication\n",
    "  of multicollinearity or numerical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a43561",
   "metadata": {},
   "source": [
    "## A comparison of the different methods:\n",
    "\n",
    "* **Option 1, 2, and 5:** These options implement the least squares solution.\n",
    "  In some cases (for instance for the matrix operations) we have to do a lot of\n",
    "  typing. This is typically something that we would **not do**\n",
    "  in practice, since other people have implemented least squares many times before\n",
    "  (and probably better than what I have done here). But we get to know the underlying\n",
    "  equations. Option 5 is probably close to what \"real\" least squares libraries are doing.\n",
    "\n",
    "\n",
    "* **Option 3:** This is short and sweet and very to the point.\n",
    "  We want to fit some polynomial of a certain degree and there is a method\n",
    "  specifically designed to do just that in NumPy. The drawback here is that we\n",
    "  are limited to polynomials - maybe this is not really what we would like to fit?\n",
    "\n",
    "\n",
    "* **Option 4:** Here we are using scikit-learn to make a linear model.\n",
    "  The big plus with using scikit-learn is that we can make use of other\n",
    "  scikit-learn methods - for instance, we might want to try some more\n",
    "  \"modern\" alternative to the ordinary least squares and we can do that by\n",
    "  swapping `LinearRegression` for a different model. Or maybe we would like\n",
    "  to use cross-validation to further test our model? scikit-learn has methods\n",
    "  for doing just that. And if we want to use our model for prediction\n",
    "  (calculating y-values for new x-values) we can use the\n",
    "  method `predict()` of the model we created.\n",
    "\n",
    "\n",
    "* **Option 6:** This focuses more on the statistics and it\n",
    "  gives us useful info about the statistical significance of the\n",
    "  different parameters. This can for instance be used, in cases where\n",
    "  we have many variables, to figure out if we can drop certain variables.\n",
    "\n",
    "\n",
    "For our course, we will focus more on using scikit-learn. The motivation for doing this is that we can use the same generic approach (we have not done this yet!) for doing regression, clustering, and classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
