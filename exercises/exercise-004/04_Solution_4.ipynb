{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8296133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY313,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Solution to exercise set 4: Principal component analysis and clustering\n",
    "\n",
    "The main goals of this exercise are to perform principal component analysis (PCA) and k-means clustering.\n",
    "\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Run PCA to reduce the dimensionality of a data set.\n",
    "* Visualise PCA results by creating score plots (showing data point projections), loading plots (illustrating variable influence), and variance-explained plots (indicating component significance).\n",
    "* Interpret results from PCA by inspecting the scores and loadings plots to explain groupings and variable contributions.\n",
    "* Run k-means clustering for a data set and use the [elbow method](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Elbow_method) to select the best number of clusters.\n",
    "\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [4.1(a)](#4.1(a)) and [4.1(b)](#4.1(b)): to show that you can perform PCA and plot the scores and the variance explained per principal component.\n",
    "- [4.2(a)](#4.2(a)) and [4.2(b)](#4.2(b)): to show that you can also plot the loadings from PCA, and interpret the scores and loadings.\n",
    "- [4.3(a)](#4.3(a)): to show that you can apply k-means clustering to a data set and select the best number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a483a4",
   "metadata": {},
   "source": [
    "## Exercise 4.1 Molecular conformations\n",
    "\n",
    "We have performed molecular dynamics simulations to model the various conformations a molecule can adopt. We have collected 4004 snapshots, each representing a conformation and we have recorded the 3D coordinates of each atom in each conformation.\n",
    "\n",
    "The file `molecule.csv` contains these coordinates, organized as follows:\n",
    "\n",
    "* Each row represents a single molecular conformation.\n",
    "* The columns contain the x, y, and z coordinates of each atom.\n",
    "* The column labels follow a pattern:\n",
    "   * `1x`, `1y`, `1z` represent the coordinates of atom 1,\n",
    "   * `2x`, `2y`, `2z` those of atom 2, and so on, up to atom 22.\n",
    "\n",
    "Here is a snippet of the data (first three conformations/rows):\n",
    "\n",
    "|     |    1x |    1y |    1z |    2x | ... |   22x |   22y |   22z |\n",
    "|----:|------:|------:|------:|------:|:---:|------:|------:|------:|\n",
    "|   0 | 14.585 | 13.725 | 12.373 | 13.759 | ... | 14.882 | 14.462 | 10.500 |\n",
    "|   1 | 14.585 | 13.868 | 12.458 | 13.773 | ... | 15.061 | 14.033 | 10.411 |\n",
    "|   2 | 14.668 | 13.689 | 12.557 | 13.667 | ... | 14.914 | 14.276 | 10.359 |\n",
    "\n",
    "\n",
    "Our goal is to use Principal Component Analysis (PCA) to determine if we can identify distinct groups or clusters of these molecular conformations based on their atomic coordinate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data can be loaded as follows:\n",
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv(\"molecule.csv\")\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26bba8",
   "metadata": {},
   "source": [
    "### 4.1(a)\n",
    "\n",
    "**Task: Run PCA on this data set and plot the variance explained as a function of the principal components, for instance in a bar plot or a line plot. How much of the variance is explained by principal components 1 and 2?**\n",
    "\n",
    "**Hints:** Assuming that `X` contains our data, a PCA can be carried out as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "This will store the scores in the variable `scores` which can be directly used in a scatter plot.\n",
    "It is also useful to inspect\n",
    "how much of the variance each principal component is explaining.\n",
    "The fraction of the variance explained by each component can be accessed via:\n",
    "```python\n",
    "variance = pca.explained_variance_ratio_\n",
    "```\n",
    "\n",
    "**Note:** The raw data has already been scaled so you can use it directly without preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.to_numpy()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18957feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "percent = pca.explained_variance_ratio_ * 100\n",
    "ax.bar(np.arange(1, len(percent) + 1), percent)\n",
    "ax.set_xlabel(\"Principal component\")\n",
    "ax.set_ylabel(\"Percentage of variance explained\")\n",
    "print(\"Explained variance:\")\n",
    "print(f\"PC1: {percent[0]:.3g}%, PC2: {percent[1]:.3g}%\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60531b",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(a): How much of the variance is explained by principal components 1 and 2?\n",
    "\n",
    "PC1 explains 72.6% of the variance and PC2 17.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd90fe",
   "metadata": {},
   "source": [
    "### 4.1(b)\n",
    "\n",
    "**Task: Create a scatter plot where you show the scores for PC1 and PC2 (the data projection onto the first two principal components). Can you see any groups in your data?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.scatterplot(x=scores[:, 0], y=scores[:, 1])\n",
    "sns.despine(fig=fig)\n",
    "ax.set_xlabel(f\"Scores, PC1 ({percent[0]:.3g}%)\")\n",
    "ax.set_ylabel(f\"Scores, PC2 ({percent[1]:.3g}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c8ff0",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(b): Do you see any clusters in your plot of the scores?\n",
    "\n",
    "Yes, it seems to be 4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae41e1",
   "metadata": {},
   "source": [
    "### 4.1(c)\n",
    "\n",
    "\n",
    "**Task: Use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to investigate if there are any clusters in the data. Create a scatter plot of the t-SNE scores to visualize the data. Do you see any clusters?**\n",
    "\n",
    "**Hint:** Assuming that `X` contains our data, dimensionality reduction by t-SNE can be carried out as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_scores = tsne.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=2025)\n",
    "tsne_scores = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7253e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.scatterplot(x=tsne_scores[:, 0], y=tsne_scores[:, 1])\n",
    "ax.set(xlabel=\"t-SNE Dimension 1\", ylabel=\"t-SNE Dimension 2\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f73e74",
   "metadata": {},
   "source": [
    "The results of t-SNE will depend on the hyperparameter `perplexity`. Let us investigate a few different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd87764",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = [5, 15, 30]\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True,\n",
    "    ncols=len(perplexity),\n",
    "    figsize=(len(perplexity) * 3, 3),\n",
    ")\n",
    "\n",
    "for i, p in enumerate(perplexity):\n",
    "    tsne_i = TSNE(n_components=2, perplexity=p, random_state=2025)\n",
    "    tsne_i_scores = tsne_i.fit_transform(X)\n",
    "    sns.scatterplot(x=tsne_i_scores[:, 0], y=tsne_i_scores[:, 1], ax=axes[i])\n",
    "    axes[i].set(xlabel=\"t-SNE Dimension 1\", ylabel=\"t-SNE Dimension 2\")\n",
    "    axes[i].set_title(f\"perplexity = {p}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f82e34",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(c): Do you see any clusters when you plot the t-SNE scores?\n",
    "\n",
    "Yes, t-SNE show distinct clusters in the data. The number and definition of these clusters are influenced by the perplexity parameter, but it seems to be three prominent clusters and a fourth smaller cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5975",
   "metadata": {},
   "source": [
    "## Exercise 4.2 Detection of milk adulteration\n",
    "\n",
    "[Prabowo](https://doi.org/10.5281/zenodo.13766649) recently investigated the feasibility of using a regular smartphone for milk quality analysis, specifically for the detection of adulteration.\n",
    "\n",
    "Prabowo used image analysis techniques to extract information from digital images of various milk samples, including pure milk, milk adulterated with rice water, and milk contaminated with lead(II)-ions. The images were captured using a smartphone (iPhone 13 Pro) under controlled conditions to ensure consistency in lighting, zoom, and distance. From the images, 4 numerical values were extracted:\n",
    "\n",
    "* The intensity of the red, green, and blue colour components from an area in the middle of the sample.\n",
    "* The amount of grey colour in the same area.\n",
    "\n",
    "This data can be found in the file [milk.csv](./milk.csv) which contains the following columns:\n",
    "\n",
    "* `Red`: the red colour component intensity\n",
    "* `Green`: the green color component intensity\n",
    "* `Blue`: the blue colour component intensity\n",
    "* `Red/Blue`: the ratio of the red to blue colour component intensity\n",
    "* `Red/Green`: the ratio of the red to green colour component intensity\n",
    "* `Blue/Green`: the ratio of the blue to green colour component intensity\n",
    "* `Grey`: the average grey pixel intensity\n",
    "* `Type`: a text describing the sample (type of milk pictured):\n",
    "    * `Milk (control)`: Samples of pure milk\n",
    "    * `Rice water (control)`: Samples of pure rice water mixtures\n",
    "    * `Milk + rice water`: Samples created by mixing pure milk with rice water. This simulates adultered milk.\n",
    "    * `Milk + lead`: Samples created by mixing pure milk with lead of various concentrations. This simulates lead-contaminated milk.\n",
    "\n",
    "We will investigate if we can use this data to distinguish between the different types by performing principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026163e3",
   "metadata": {},
   "source": [
    "### 4.2(a)\n",
    "\n",
    "**Tasks:**\n",
    "1. **Load the data set and perform PCA to obtain the scores. Scale the data before performing PCA.**\n",
    "2. **Create scatter plots of the scores (you can investigate different combinations of principal components), colour the samples according to their type and investigate visually if the different sample types appear as distinct clusters.**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. In this case, the analysis may benefit from standardisation of the variance (since we may have different units or natural scales for the numbers). Assuming that our data is stored in the matrix `X`, we can standardise it as follows:\n",
    "```python\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(X)\n",
    "```\n",
    "\n",
    "2. Coloring a scatter plot according to a column in a Pandas data frame can be done with [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) from seaborn:\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data2 = pd.read_csv(\"milk.csv\")  # load data\n",
    "\n",
    "# ... assuming scores contain the PCA scores:\n",
    "sns.scatterplot(\n",
    "    data=data2,  # select the data frame\n",
    "    x=scores[:, 0],  # select data to put on the x-axis\n",
    "    y=scores[:, 1],  # select data to put on the y-axis\n",
    "    hue=\"Type\",  # select data to use for colouring (column from data2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data2 = pd.read_csv(\"milk.csv\")\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64017e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [i for i in data2.columns if i not in (\"Type\",)]\n",
    "X_milk = data2[variables].to_numpy()\n",
    "X_scaled = scale(X_milk)\n",
    "\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f47c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(12, 4))\n",
    "for k, (i, j) in enumerate([(0, 1), (0, 2), (1, 2)]):\n",
    "    sns.scatterplot(\n",
    "        x=scores[:, i], y=scores[:, j], data=data2, hue=\"Type\", ax=axes[k]\n",
    "    )\n",
    "    axes[k].set_xlabel(\n",
    "        f\"PC{i+1} ({pca.explained_variance_ratio_[i]*100:.3g}%)\"\n",
    "    )\n",
    "    axes[k].set_ylabel(\n",
    "        f\"PC{j+1} ({pca.explained_variance_ratio_[j]*100:.3g}%)\"\n",
    "    )\n",
    "    axes[k].axhline(y=0, color=\"k\", ls=\":\")\n",
    "    axes[k].axvline(x=0, color=\"k\", ls=\":\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97190f",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.2(a): Do the different sample types appear as distinct clusters?\n",
    "\n",
    "The scores plot of principal components 1 and 2 show separation between the different sample types.\n",
    "Rice water samples form a distinct cluster on the negative side of PC1, while the other samples are on the positive side. Along PC2, there is a separation between pure milk and milk samples contaminated with lead: the pure milk samples can be found at positive PC2, while the lead-contaminated samples are found at negative PC2. PC1, accounting for 70% of the variance, primarily separates rice water from milk-containing samples.\n",
    "PC2, accounting for 19% of the variance, appears to separate pure milk from lead-contaminated milk.\n",
    "\n",
    "There is also a grouping closer to the origin for the samples containing both milk and rice water. This location, close to the origin, typically represent samples that are intermediate or average relative to the other samples. This is consistent with the samples being mixtures of milk and rice water.\n",
    "\n",
    "The PC1 vs. PC3 and PC2 vs. PC3 plots provide less clear separation between the sample types compared to the PC1 vs. PC2 plot. Therefore, the PC1 vs. PC2 plot is the most useful in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bd19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try t-SNE in this case, just for comparison:\n",
    "milk_tsne = TSNE(n_components=2, perplexity=10, random_state=2025)\n",
    "milk_tsne_scores = milk_tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba82287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.scatterplot(\n",
    "    x=milk_tsne_scores[:, 0], y=milk_tsne_scores[:, 1], hue=\"Type\", data=data2\n",
    ")\n",
    "ax.set(xlabel=\"t-SNE Dimension 1\", ylabel=\"t-SNE Dimension 2\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb4d97",
   "metadata": {},
   "source": [
    "### 4.2(b)\n",
    "\n",
    "**Tasks: Interpret the scores plot(s) to identify the variables that contribute most significantly to the observed clusters, specifically:**\n",
    "\n",
    "1. **Which variables are most influential in discriminating between pure rice water and samples containing milk?**\n",
    "2. **Which variables are most influential in discriminating between pure milk and lead-contaminated milk samples?**\n",
    "\n",
    "**(Use loading plots to guide your interpretation.)**\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** Create a scatter plot of the loadings to show their importance for different principal\n",
    "components and interpret ths together with the scores. Scatterplots can be created as follows (assuming that `pca` is a `PCA` object from scikit-learn, and that the variables used are stored in a list `variables`):\n",
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "loadings = pca.components_.T  # Extract the loadings\n",
    "variables = [\n",
    "    \"Red\",\n",
    "    \"Green\",\n",
    "    \"Blue\",\n",
    "    \"Red/Blue\",\n",
    "    \"Red/Green\",\n",
    "    \"Blue/Green\",\n",
    "    \"Grey\",\n",
    "]  # Store variable names\n",
    "fig, ax = plt.subplots()  # Create empty plot\n",
    "ax.scatter(loadings[:, 0], loadings[:, 1])  # Scatter plot of the loadings\n",
    "\n",
    "for i, text in enumerate(variables):\n",
    "    # Add the name of the variable as text next to the scatter points:\n",
    "    ax.text(loadings[i, 0], loadings[i, 1], text, fontsize=\"small\")  \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We focus on PC1 and PC2 since the scores look most interesting here:\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "\n",
    "\n",
    "sns.scatterplot(x=scores[:, 0], y=scores[:, 1], data=data2, hue=\"Type\", ax=ax1)\n",
    "ax1.set_xlabel(f\"PC1, scores ({pca.explained_variance_ratio_[0]*100:.3g}%)\")\n",
    "ax1.set_ylabel(f\"PC2, scores ({pca.explained_variance_ratio_[1]*100:.3g}%)\")\n",
    "\n",
    "ax1.axhline(y=0, color=\"k\", ls=\":\")\n",
    "ax1.axvline(x=0, color=\"k\", ls=\":\")\n",
    "\n",
    "loadings = pca.components_.T\n",
    "sns.scatterplot(x=loadings[:, 0], y=loadings[:, 1], ax=ax2)\n",
    "\n",
    "for i, text in enumerate(variables):\n",
    "    ax2.text(loadings[i, 0], loadings[i, 1], text, fontsize=\"small\")\n",
    "\n",
    "\n",
    "ax2.set_xlabel(f\"PC1, loadings ({pca.explained_variance_ratio_[0]*100:.3g}%)\")\n",
    "ax2.set_ylabel(f\"PC2, loadings ({pca.explained_variance_ratio_[1]*100:.3g}%)\")\n",
    "ax2.axhline(y=0, color=\"k\", ls=\":\")\n",
    "ax2.axvline(x=0, color=\"k\", ls=\":\")\n",
    "\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314f663",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.2(b): What variables are important for distinguishing between (1) pure rice water and samples containing milk, and (2) samples of pure milk and milk contaminated by lead?\n",
    "\n",
    "1. The separation between pure rice water and milk-containing samples is primarily along PC1. The Blue/Green ratio has a negative loading on PC1. Since rice water samples are located on the negative side of PC1, they are expected to have higher Blue/Green ratio values compared to milk-containing samples. Conversely, milk-containing samples, positioned on the positive side of PC1, exhibit higher values for the other variables, all of which have positive loadings on PC1.\n",
    "\n",
    "2. The separation between pure milk and lead-contaminated milk samples is primarily driven by PC2. The Red/Green and Red/Blue ratios exhibit positive loadings on PC2, with the Red/Green ratio showing the strongest influence. Consequently, pure milk samples, located on the positive side of PC2, display higher Red/Green ratio values compared to lead-contaminated samples. Conversely, the Grey intensity, which has a negative loading on PC2, is expected to be higher in lead-contaminated samples, positioned on the negative side of PC2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd4202",
   "metadata": {},
   "source": [
    "We can test out these observations, by looking at the distributions for the identified variables. First let see if we can see the separation along PC1, by just looking at the biggest contributing variable (Blue/Green):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036dcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "sns.kdeplot(data=data2, x=\"Blue/Green\", hue=\"Type\", ax=ax1)\n",
    "sns.boxplot(data=data2, y=\"Blue/Green\", hue=\"Type\", ax=ax2)\n",
    "ax2.get_legend().remove()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330b7ba",
   "metadata": {},
   "source": [
    "And we see a clear distinction between rice water and the other samples. Next, let us check if we can se the distinction between pure milk and lead-contaminated samples along PC2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "sns.kdeplot(data=data2, x=\"Red/Green\", hue=\"Type\", ax=ax1)\n",
    "sns.boxplot(data=data2, y=\"Red/Green\", hue=\"Type\", ax=ax2)\n",
    "ax2.get_legend().remove()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baccc9f",
   "metadata": {},
   "source": [
    "A separation is observed between pure milk and lead-contaminated samples, though it is less distinct than the separation between rice water and milk, and one or two potential outliers warrant further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c72647",
   "metadata": {},
   "source": [
    "## Exercise 4.3 Clustering\n",
    "\n",
    "In [Exercise 4.1](#Exercise-4.1-Molecular-conformations), we analysed molecular conformations using PCA. The file [scores.4.1.csv](./scores.4.1.csv) contains the scores for principal components 1 (column `PC1`) and 2 (column `PC2`). In this exercise, we will investigate if we can find clusters in this data by applying [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913fb0a",
   "metadata": {},
   "source": [
    "### 4.3(a)\n",
    "\n",
    "**Tasks:**\n",
    "1. **Load the data from [scores.4.1.csv](./scores.4.1.csv) and perform k-means clustering, considering the number of clusters (k) from 1 to 10**\n",
    "2. **Plot the within-cluster sum of squared distances of the samples to their closest cluster centre as a function of the number of clusters (k).**\n",
    "3. **Use the plot created above (the \"elbow method\") to identify the best number of clusters. Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** scikit-learn can perform [k-means clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Here is one example to perform it for 3 clusters:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = pd.read_csv(\"scores.4.1.csv\")  # Load the data\n",
    "# Set up the k-means method to look for 3 clusters:\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Print out cluster centers:\n",
    "print(cluster.cluster_centers_)\n",
    "# Print out the within-cluster sum of squared distances of samples to their closest cluster centre:\n",
    "print(cluster.inertia_)\n",
    "```\n",
    "\n",
    "**Note:** The elbow method is a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41779937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data3 = pd.read_csv(\"scores.4.1.csv\")\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "for i in range(1, 11):\n",
    "    cluster_i = KMeans(n_clusters=i)\n",
    "    cluster_i.fit(data3)\n",
    "    clusters.append(cluster_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = np.array([i.inertia_ for i in clusters])\n",
    "n_clusters = np.array([i.n_clusters for i in clusters], dtype=int)\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(n_clusters, sse, marker=\"o\")\n",
    "ax.set(\n",
    "    xlabel=\"Number of clusters\",\n",
    "    ylabel=\"Within-cluster sum of squared distances\",\n",
    ")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra, let us visualise the clustering:\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4), sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data3,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=clusters[2].labels_,\n",
    "    palette=\"colorblind\",\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(f\"Clusters: {clusters[2].n_clusters}\", loc=\"left\")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=data3,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=clusters[3].labels_,\n",
    "    palette=\"colorblind\",\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(f\"Clusters: {clusters[3].n_clusters}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dcf39",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(a): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "There is a significant drop in the sum of squared distances going from 2 to 3 clusters. The drop when going from 3 to 4 is much smaller, and even smaller when going from 4 to 5. This \"elbow\" suggest that 3 clusters are enough to capture the primary structure of the data.\n",
    "\n",
    "A visual inspection (see the figure above) indicates that 4 is a better number. The fourth cluster is much smaller than the other clusters and this could explain why the within-cluster sum of squared distances did not change noteably when it was added. Since there is some ambiguity here, we will evaluate silhouette scores and the Gap statistic below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691a55c",
   "metadata": {},
   "source": [
    "### 4.3(b)\n",
    "\n",
    "The [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) measures how similar a data point is to its own cluster compared to other clusters, and can be used to select the best number of clusters by comparing silhouette values for different clusterings.\n",
    "\n",
    "\n",
    "**Task: Calculate the mean silhouette score for 2 to 10 clusters. Plot the mean silhouette value as a function of the number of cluster centres. What is the best number of clusters to use, based on this plot? Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** Given a clustering, you can find the silhouette value as follows:\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Get what cluster the different points are assigned to:\n",
    "cluster_labels = cluster.predict(data)\n",
    "silhouette_mean = silhouette_score(data, cluster_labels)\n",
    "print(silhouette_mean)\n",
    "```\n",
    "\n",
    "**Note:** The silhouette score is *not defined* for 1 cluster. (Can you explain why?)\n",
    "\n",
    "**Note:** The silhouette score is also a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette = []\n",
    "n_clusters = []\n",
    "\n",
    "\n",
    "for cluster_i in clusters:\n",
    "    if cluster_i.n_clusters < 2:\n",
    "        continue\n",
    "    cluster_labels = cluster_i.predict(data3)\n",
    "    silhouette_mean = silhouette_score(data3, cluster_labels)\n",
    "    silhouette.append(silhouette_mean)\n",
    "    n_clusters.append(cluster_i.n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(n_clusters, silhouette, marker=\"o\")\n",
    "ax.set(xlabel=\"Number of cluster centers\", ylabel=\"Mean silhouette score\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd7692",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(b): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "Analysing the mean silhouette scores across different cluster numbers, we observe that the maximum value occurs around 3 or 4 clusters. This suggests that either 3 or 4 clusters could be optimal. In the previous question, we noted that the fourth cluster is significantly smaller than the others, meaning that its inclusion might not influence the mean silhouette score noticeably (its importance is masked by its size).\n",
    "\n",
    "To gain further insights, we will next calculate the Gap statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bea5e9",
   "metadata": {},
   "source": [
    "### 4.3(c)\n",
    "\n",
    "The [Gap statistic](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_gap_statistics) compares the total within-cluster dispersion (often represented as the sum of pairwise distances within each cluster, W) with what we would expect for uniformly randomly distributed points (Ŵ). The optimal number of clusters is the point where the \"Gap\", which is the difference between log(W) and log(Ŵ), is largest.\n",
    "\n",
    "\n",
    "**Task: Obtain and plot the Gap statistic value as a function of the number of cluster centres (consider 1 to 10 clusters). What is the best number of clusters to use, based on this plot? Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** The Gap statistic can be obtained via the [gapstat](https://github.com/jmmaloney3/gapstat) package. If you do not have this one installed, you can install it via (in a terminal):\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/jmmaloney3/gapstat\n",
    "```\n",
    "\n",
    "To install it directly from a Jupyter notebook, you need to add a \"!\" in front of the command:\n",
    "```bash\n",
    "!pip install git+https://github.com/jmmaloney3/gapstat\n",
    "```\n",
    "\n",
    "To calculate the Gap statistic:\n",
    "\n",
    "```python\n",
    "from gapstat import gapstat_score\n",
    "\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Get what cluster the different points are assigned to:\n",
    "cluster_labels = cluster.predict(data)\n",
    "\n",
    "gap, _, _, _, error = gapstat_score(\n",
    "    data, cluster_labels, k=3, calcStats=True\n",
    ")\n",
    "\n",
    "# gap = the Gap statistic\n",
    "# error = standard deviation for the Gap statistic\n",
    "```\n",
    "\n",
    "**Note:** The Gap statistic is also a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gapstat import gapstat_score\n",
    "\n",
    "n_clusters = []\n",
    "gaps = []\n",
    "gaps_error = []\n",
    "\n",
    "for cluster_i in clusters:\n",
    "    cluster_labels = cluster_i.predict(data3)\n",
    "    gap, _, _, _, error = gapstat_score(\n",
    "        data3, cluster_labels, k=cluster_i.n_clusters, calcStats=True\n",
    "    )\n",
    "    n_clusters.append(cluster_i.n_clusters)\n",
    "    gaps.append(gap)\n",
    "    gaps_error.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.errorbar(n_clusters, gaps, yerr=gaps_error, marker=\"o\")\n",
    "ax.set(xlabel=\"Number of cluster centers\", ylabel=\"Gap statistic\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80a0d6",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(c): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "In this case, we observe that the Gap statistic increases when we add the fourth cluster. There is a small increase when we add a fifth cluster, and a subsequent drop when adding a sixth. This pattern suggests that 4 or 5 clusters might be optimal.\n",
    "\n",
    "Combining this analysis with the elbow plot and the silhouette scores further strengthens the case for four clusters. Both the elbow plot and the silhouette analysis indicated 3 or 4 as potential optimal cluster numbers, while the Gap statistic suggests 4 or 5. By considering the intersection of these findings and giving each metric equal weight, we arrive at 4 clusters as the most consistent choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d294897",
   "metadata": {},
   "source": [
    "### 4.3(d)\n",
    "\n",
    "**Task: Repeat [4.3(a)](#4.3(a))-[4.3(c)](#4.3(c)), but use the original data in [molecule.csv](./molecule.csv) instead of the PCA scores. What is the best number of clusters? Explain your reasoning for selecting the best number of clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv(\"molecule.csv\")\n",
    "\n",
    "sse2 = []\n",
    "silhouette2 = []\n",
    "gaps2 = []\n",
    "clusters2 = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    cluster_i = KMeans(n_clusters=i)\n",
    "    cluster_i.fit(data4)\n",
    "    cluster_labels = cluster_i.predict(data4)\n",
    "\n",
    "    clusters2.append(cluster_i)\n",
    "\n",
    "    sse2.append([i, cluster_i.inertia_])\n",
    "    if i > 1:\n",
    "        silhouette_mean = silhouette_score(data4, cluster_labels)\n",
    "        silhouette2.append([i, silhouette_mean])\n",
    "\n",
    "    gap, _, _, _, error = gapstat_score(\n",
    "        data4, cluster_labels, k=cluster_i.n_clusters, calcStats=True\n",
    "    )\n",
    "    gaps2.append([i, gap, error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f77349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse2 = np.array(sse2)\n",
    "silhouette2 = np.array(silhouette2)\n",
    "gaps2 = np.array(gaps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(sse2[:, 0], sse2[:, 1], marker=\"o\")\n",
    "ax.set(\n",
    "    xlabel=\"Number of clusters\",\n",
    "    ylabel=\"Within-cluster sum of squared distances\",\n",
    ")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(silhouette2[:, 0], silhouette2[:, 1], marker=\"o\")\n",
    "ax.set(xlabel=\"Number of cluster centers\", ylabel=\"Mean silhouette score\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.errorbar(gaps2[:, 0], gaps2[:, 1], yerr=gaps2[:, 2], marker=\"o\")\n",
    "ax.set(xlabel=\"Number of cluster centers\", ylabel=\"Gap statistic\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334a509",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(d): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "In this case, the Gap statistic does not give a clear answer since it keeps increasing when adding more cluster centres. Both the elbow method and the silhouette value indicate that 3 clusters is the optimal number: there is a clear drop in the within-cluster sum of squared distances up to 3 clusters, and there is a peak in the silhouette value at 3 clusters.\n",
    "\n",
    "Visual inspection of the clusters is not feasible due to the high dimensionality of the data (66 variables). Since both the elbow method and the silhouette value agree on 3 clusters, we conclude that the best number of clusters is 3. It is worth noting that using principal component scores for visualisation can simplify the analysis in high-dimensional cases like this, making it easier to visualise and understand the cluster structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f1f27",
   "metadata": {},
   "source": [
    "### 4.3(e)\n",
    "\n",
    "**Task: Repeat the clustering of the data in [scores.4.1.csv](./scores.4.1.csv), but use the density-based method [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). How many clusters were identified by DBSCAN?**\n",
    "\n",
    "**Hint:** Assuming that the matrix `X` contains our raw data, DBSCAN clustering can be performed with:\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = clustering.fit(X)\n",
    "```\n",
    "\n",
    "**Note:** The results from DBSCAN may depend on the hyperparameters `eps` and `min_samples`. Explore different values for these parameters and investigate how they affect the number of clusters and noise points identified. Consider visualising the clusters or calculating silhouette scores.\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** Assuming that the matrix `X` contains our raw data, DBSCAN clustering can be performed with:\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = clustering.fit(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# We try some different values for eps and min_samples:\n",
    "eps = [0.1, 1.0, 3.0]\n",
    "min_samples = [5, 15, 30]\n",
    "\n",
    "results = {\n",
    "    \"eps\": [],\n",
    "    \"min_samples\": [],\n",
    "    \"silhouette\": [],\n",
    "    \"silhouette-noise\": [],\n",
    "    \"clusters\": [],\n",
    "    \"noise\": [],\n",
    "}\n",
    "\n",
    "for eps_i in eps:\n",
    "    for min_samples_i in min_samples:\n",
    "        cluster_i = DBSCAN(eps=eps_i, min_samples=min_samples_i)\n",
    "        labels = cluster_i.fit_predict(data3)\n",
    "\n",
    "        count = {}\n",
    "\n",
    "        for i in labels:\n",
    "            idx = int(i)\n",
    "            if idx not in count:\n",
    "                count[idx] = 0\n",
    "            count[idx] += 1\n",
    "\n",
    "        noise = count.get(-1, 0)\n",
    "\n",
    "        n_clusters = sum(1 if key != -1 else 0 for key in count)\n",
    "\n",
    "        if n_clusters > 1:\n",
    "            # Skip noise points for silhouette:\n",
    "            mask = labels != -1\n",
    "            mask_noise = labels == -1\n",
    "\n",
    "            silh = silhouette_samples(data3, labels)\n",
    "            silhouette_mean = np.mean(silh[mask])\n",
    "\n",
    "            if sum(mask_noise) < 1:\n",
    "                silhouette_noise_mean = float(\"nan\")\n",
    "            else:\n",
    "                silhouette_noise_mean = np.mean(silh[mask_noise])\n",
    "        else:\n",
    "            silhouette_mean = float(\"nan\")\n",
    "            silhouette_noise_mean = float(\"nan\")\n",
    "\n",
    "        results[\"eps\"].append(eps_i)\n",
    "        results[\"min_samples\"].append(min_samples_i)\n",
    "        results[\"noise\"].append(noise)\n",
    "        results[\"clusters\"].append(n_clusters)\n",
    "        results[\"silhouette\"].append(silhouette_mean)\n",
    "        results[\"silhouette-noise\"].append(silhouette_noise_mean)\n",
    "\n",
    "table = pd.DataFrame(results)\n",
    "table.sort_values(by=\"silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da899a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9, 3))\n",
    "cluster_4 = DBSCAN(eps=1.0, min_samples=10)\n",
    "labels4 = cluster_4.fit_predict(data3)\n",
    "cluster_3 = DBSCAN(eps=1.0, min_samples=17)\n",
    "labels3 = cluster_3.fit_predict(data3)\n",
    "\n",
    "cluster_0 = DBSCAN(eps=0.1, min_samples=17)\n",
    "labels0 = cluster_0.fit_predict(data3)\n",
    "\n",
    "\n",
    "def show_clusters(ax, clustering, labels):\n",
    "    n = 0\n",
    "    for clu in set(labels):\n",
    "        xval = data3[\"PC1\"][labels == clu]\n",
    "        yval = data3[\"PC2\"][labels == clu]\n",
    "        if clu == -1:\n",
    "            ax.scatter(\n",
    "                xval,\n",
    "                yval,\n",
    "                label=f\"Noise\",\n",
    "                color=\"k\",\n",
    "            )\n",
    "        else:\n",
    "            n += 1\n",
    "            ax.scatter(\n",
    "                xval,\n",
    "                yval,\n",
    "                label=f\"Cluster {clu+1}\",\n",
    "            )\n",
    "    ax.legend()\n",
    "\n",
    "    cluster_3.eps\n",
    "    ax.set_title(\n",
    "        f\"{n} clusters | {clustering}\", loc=\"left\", fontsize=\"x-small\"\n",
    "    )\n",
    "\n",
    "\n",
    "show_clusters(axes[0], cluster_0, labels0)\n",
    "show_clusters(axes[1], cluster_3, labels3)\n",
    "show_clusters(axes[2], cluster_4, labels4)\n",
    "\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141657a",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(e): How many clusters did you find with DBSCAN? How is this influenced by the hyperparameters?\n",
    "\n",
    "DBSCAN identified a varying number of clusters depending on the hyperparameters `eps` and `min_samples`:\n",
    "\n",
    "* Low `eps` values resulted in more noise points (samples are too far away to be considered neighbours) while higher `eps` values resulted in fewer noise points (when keeping `min_samples` fixed).\n",
    "* High `min_samples` (i.e., higher than 16 in this case) misses the smallest cluster since it only contains 16 samples.\n",
    "\n",
    "Interpretation of the silhouette values is more complex since we may have noise points, and the mean silhouette value was only calculated considering the non-noise points. While a high silhouette score might suggest a good clustering, it can be misleading if a significant portion of the data is labelled as noise. In some cases where 3 clusters were identified, the mean silhouette score for the 16 noise points was high (0.91). This occurred when `min_samples` was greater than 16. This suggests that these 16 samples may be considered as an independent cluster.\n",
    "\n",
    "Overall, the DBSCAN result points towards 4 clusters in this data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
