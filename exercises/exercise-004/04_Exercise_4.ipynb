{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Exercise set 4: Principal component analysis and clustering\n",
    "\n",
    "The main goals of this exercise are to perform principal component analysis (PCA) and k-means clustering.\n",
    "\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Run PCA to reduce the dimensionality of a data set.\n",
    "* Visualise PCA results by creating score plots (showing data point projections), loading plots (illustrating variable influence), and variance-explained plots (indicating component significance).\n",
    "* Interpret results from PCA by inspecting the scores and loadings plots to explain groupings and variable contributions.\n",
    "* Run k-means clustering for a data set and use the [elbow method](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Elbow_method) to select the best number of clusters.\n",
    "\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [4.1(a)](#4.1(a)) and [4.1(b)](#4.1(b)): to show that you can perform PCA and plot the scores and the variance explained per principal component.\n",
    "- [4.2(a)](#4.2(a)) and [4.2(b)](#4.2(b)): to show that you can also plot the loadings from PCA, and interpret the scores and loadings.\n",
    "- [4.3(a)](#4.3(a)): to show that you can apply k-means clustering to a data set and select the best number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a483a4",
   "metadata": {},
   "source": [
    "## Exercise 4.1 Molecular conformations\n",
    "\n",
    "We have performed molecular dynamics simulations to model the various conformations a molecule can adopt. We have collected 4004 snapshots, each representing a conformation and we have recorded the 3D coordinates of each atom in each conformation.\n",
    "\n",
    "The file `molecule.csv` contains these coordinates, organized as follows:\n",
    "\n",
    "* Each row represents a single molecular conformation.\n",
    "* The columns contain the x, y, and z coordinates of each atom.\n",
    "* The column labels follow a pattern:\n",
    "   * `1x`, `1y`, `1z` represent the coordinates of atom 1,\n",
    "   * `2x`, `2y`, `2z` those of atom 2, and so on, up to atom 22.\n",
    "\n",
    "Here is a snippet of the data (first three conformations/rows):\n",
    "\n",
    "|     |    1x |    1y |    1z |    2x | ... |   22x |   22y |   22z |\n",
    "|----:|------:|------:|------:|------:|:---:|------:|------:|------:|\n",
    "|   0 | 14.585 | 13.725 | 12.373 | 13.759 | ... | 14.882 | 14.462 | 10.500 |\n",
    "|   1 | 14.585 | 13.868 | 12.458 | 13.773 | ... | 15.061 | 14.033 | 10.411 |\n",
    "|   2 | 14.668 | 13.689 | 12.557 | 13.667 | ... | 14.914 | 14.276 | 10.359 |\n",
    "\n",
    "\n",
    "Our goal is to use Principal Component Analysis (PCA) to determine if we can identify distinct groups or clusters of these molecular conformations based on their atomic coordinate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data can be loaded as follows:\n",
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv(\"molecule.csv\")\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26bba8",
   "metadata": {},
   "source": [
    "### 4.1(a)\n",
    "\n",
    "**Task: Run PCA on this data set and plot the variance explained as a function of the principal components, for instance in a bar plot or a line plot. How much of the variance is explained by principal components 1 and 2?**\n",
    "\n",
    "**Hints:** Assuming that `X` contains our data, a PCA can be carried out as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "This will store the scores in the variable `scores` which can be directly used in a scatter plot.\n",
    "It is also useful to inspect\n",
    "how much of the variance each principal component is explaining.\n",
    "The fraction of the variance explained by each component can be accessed via:\n",
    "```python\n",
    "variance = pca.explained_variance_ratio_\n",
    "```\n",
    "\n",
    "**Note:** The raw data has already been scaled so you can use it directly without preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60531b",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(a): How much of the variance is explained by principal components 1 and 2?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd90fe",
   "metadata": {},
   "source": [
    "### 4.1(b)\n",
    "\n",
    "**Task: Create a scatter plot where you show the scores for PC1 and PC2 (the data projection onto the first two principal components). Can you see any groups in your data?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c8ff0",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(b): Do you see any clusters in your plot of the scores?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae41e1",
   "metadata": {},
   "source": [
    "### 4.1(c)\n",
    "\n",
    "\n",
    "**Task: Use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to investigate if there are any clusters in the data. Create a scatter plot of the t-SNE scores to visualize the data. Do you see any clusters?**\n",
    "\n",
    "**Hint:** Assuming that `X` contains our data, dimensionality reduction by t-SNE can be carried out as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_scores = tsne.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f82e34",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.1(c): Do you see any clusters when you plot the t-SNE scores?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be5975",
   "metadata": {},
   "source": [
    "## Exercise 4.2 Detection of milk adulteration\n",
    "\n",
    "[Prabowo](https://doi.org/10.5281/zenodo.13766649) recently investigated the feasibility of using a regular smartphone for milk quality analysis, specifically for the detection of adulteration.\n",
    "\n",
    "Prabowo used image analysis techniques to extract information from digital images of various milk samples, including pure milk, milk adulterated with rice water, and milk contaminated with lead(II)-ions. The images were captured using a smartphone (iPhone 13 Pro) under controlled conditions to ensure consistency in lighting, zoom, and distance. From the images, 4 numerical values were extracted:\n",
    "\n",
    "* The intensity of the red, green, and blue colour components from an area in the middle of the sample.\n",
    "* The amount of grey colour in the same area.\n",
    "\n",
    "This data can be found in the file [milk.csv](./milk.csv) which contains the following columns:\n",
    "\n",
    "* `Red`: the red colour component intensity\n",
    "* `Green`: the green color component intensity\n",
    "* `Blue`: the blue colour component intensity\n",
    "* `Red/Blue`: the ratio of the red to blue colour component intensity\n",
    "* `Red/Green`: the ratio of the red to green colour component intensity\n",
    "* `Blue/Green`: the ratio of the blue to green colour component intensity\n",
    "* `Grey`: the average grey pixel intensity\n",
    "* `Type`: a text describing the sample (type of milk pictured):\n",
    "    * `Milk (control)`: Samples of pure milk\n",
    "    * `Rice water (control)`: Samples of pure rice water mixtures\n",
    "    * `Milk + rice water`: Samples created by mixing pure milk with rice water. This simulates adultered milk.\n",
    "    * `Milk + lead`: Samples created by mixing pure milk with lead of various concentrations. This simulates lead-contaminated milk.\n",
    "\n",
    "We will investigate if we can use this data to distinguish between the different types by performing principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026163e3",
   "metadata": {},
   "source": [
    "### 4.2(a)\n",
    "\n",
    "**Tasks:**\n",
    "1. **Load the data set and perform PCA to obtain the scores. Scale the data before performing PCA.**\n",
    "2. **Create scatter plots of the scores (you can investigate different combinations of principal components), colour the samples according to their type and investigate visually if the different sample types appear as distinct clusters.**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. In this case, the analysis may benefit from standardisation of the variance (since we may have different units or natural scales for the numbers). Assuming that our data is stored in the matrix `X`, we can standardise it as follows:\n",
    "```python\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(X)\n",
    "```\n",
    "\n",
    "2. Coloring a scatter plot according to a column in a Pandas data frame can be done with [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) from seaborn:\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data2 = pd.read_csv(\"milk.csv\")  # load data\n",
    "\n",
    "# ... assuming scores contain the PCA scores:\n",
    "sns.scatterplot(\n",
    "    data=data2,  # select the data frame\n",
    "    x=scores[:, 0],  # select data to put on the x-axis\n",
    "    y=scores[:, 1],  # select data to put on the y-axis\n",
    "    hue=\"Type\",  # select data to use for colouring (column from data2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97190f",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.2(a): Do the different sample types appear as distinct clusters?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb4d97",
   "metadata": {},
   "source": [
    "### 4.2(b)\n",
    "\n",
    "**Tasks: Interpret the scores plot(s) to identify the variables that contribute most significantly to the observed clusters, specifically:**\n",
    "\n",
    "1. **Which variables are most influential in discriminating between pure rice water and samples containing milk?**\n",
    "2. **Which variables are most influential in discriminating between pure milk and lead-contaminated milk samples?**\n",
    "\n",
    "**(Use loading plots to guide your interpretation.)**\n",
    "\n",
    "\n",
    "**Hint:** Create a scatter plot of the loadings to show their importance for different principal\n",
    "components and interpret ths together with the scores. Scatterplots can be created as follows (assuming that `pca` is a `PCA` object from scikit-learn, and that the variables used are stored in a list `variables`):\n",
    "```python\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "loadings = pca.components_.T  # Extract the loadings\n",
    "variables = [\n",
    "    \"Red\",\n",
    "    \"Green\",\n",
    "    \"Blue\",\n",
    "    \"Red/Blue\",\n",
    "    \"Red/Green\",\n",
    "    \"Blue/Green\",\n",
    "    \"Grey\",\n",
    "]  # Store variable names\n",
    "fig, ax = plt.subplots()  # Create empty plot\n",
    "ax.scatter(loadings[:, 0], loadings[:, 1])  # Scatter plot of the loadings\n",
    "\n",
    "for i, text in enumerate(variables):\n",
    "    # Add the name of the variable as text next to the scatter points:\n",
    "    ax.text(loadings[i, 0], loadings[i, 1], text, fontsize=\"small\")  \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314f663",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.2(b): What variables are important for distinguishing between (1) pure rice water and samples containing milk, and (2) samples of pure milk and milk contaminated by lead?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c72647",
   "metadata": {},
   "source": [
    "## Exercise 4.3 Clustering\n",
    "\n",
    "In [Exercise 4.1](#Exercise-4.1-Molecular-conformations), we analysed molecular conformations using PCA. The file [scores.4.1.csv](./scores.4.1.csv) contains the scores for principal components 1 (column `PC1`) and 2 (column `PC2`). In this exercise, we will investigate if we can find clusters in this data by applying [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913fb0a",
   "metadata": {},
   "source": [
    "### 4.3(a)\n",
    "\n",
    "**Tasks:**\n",
    "1. **Load the data from [scores.4.1.csv](./scores.4.1.csv) and perform k-means clustering, considering the number of clusters (k) from 1 to 10**\n",
    "2. **Plot the within-cluster sum of squared distances of the samples to their closest cluster centre as a function of the number of clusters (k).**\n",
    "3. **Use the plot created above (the \"elbow method\") to identify the best number of clusters. Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** scikit-learn can perform [k-means clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Here is one example to perform it for 3 clusters:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data = pd.read_csv(\"scores.4.1.csv\")  # Load the data\n",
    "# Set up the k-means method to look for 3 clusters:\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Print out cluster centers:\n",
    "print(cluster.cluster_centers_)\n",
    "# Print out the within-cluster sum of squared distances of samples to their closest cluster centre:\n",
    "print(cluster.inertia_)\n",
    "```\n",
    "\n",
    "**Note:** The elbow method is a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41779937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dcf39",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(a): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691a55c",
   "metadata": {},
   "source": [
    "### 4.3(b)\n",
    "\n",
    "The [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) measures how similar a data point is to its own cluster compared to other clusters, and can be used to select the best number of clusters by comparing silhouette values for different clusterings.\n",
    "\n",
    "\n",
    "**Task: Calculate the mean silhouette score for 2 to 10 clusters. Plot the mean silhouette value as a function of the number of cluster centres. What is the best number of clusters to use, based on this plot? Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** Given a clustering, you can find the silhouette value as follows:\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Get what cluster the different points are assigned to:\n",
    "cluster_labels = cluster.predict(data)\n",
    "silhouette_mean = silhouette_score(data, cluster_labels)\n",
    "print(silhouette_mean)\n",
    "```\n",
    "\n",
    "**Note:** The silhouette score is *not defined* for 1 cluster. (Can you explain why?)\n",
    "\n",
    "**Note:** The silhouette score is also a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd7692",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(b): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bea5e9",
   "metadata": {},
   "source": [
    "### 4.3(c)\n",
    "\n",
    "The [Gap statistic](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_gap_statistics) compares the total within-cluster dispersion (often represented as the sum of pairwise distances within each cluster, W) with what we would expect for uniformly randomly distributed points (Ŵ). The optimal number of clusters is the point where the \"Gap\", which is the difference between log(W) and log(Ŵ), is largest.\n",
    "\n",
    "\n",
    "**Task: Obtain and plot the Gap statistic value as a function of the number of cluster centres (consider 1 to 10 clusters). What is the best number of clusters to use, based on this plot? Explain your reasoning for selecting the best number of clusters.**\n",
    "\n",
    "**Hint:** The Gap statistic can be obtained via the [gapstat](https://github.com/jmmaloney3/gapstat) package. If you do not have this one installed, you can install it via (in a terminal):\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/jmmaloney3/gapstat\n",
    "```\n",
    "\n",
    "To install it directly from a Jupyter notebook, you need to add a \"!\" in front of the command:\n",
    "```bash\n",
    "!pip install git+https://github.com/jmmaloney3/gapstat\n",
    "```\n",
    "\n",
    "To calculate the Gap statistic:\n",
    "\n",
    "```python\n",
    "from gapstat import gapstat_score\n",
    "\n",
    "cluster = KMeans(n_clusters=3)  # n_clusters selects the number of clusters\n",
    "cluster.fit(data)  # Run clustering on our data\n",
    "# Get what cluster the different points are assigned to:\n",
    "cluster_labels = cluster.predict(data)\n",
    "\n",
    "gap, _, _, _, error = gapstat_score(\n",
    "    data, cluster_labels, k=3, calcStats=True\n",
    ")\n",
    "\n",
    "# gap = the Gap statistic\n",
    "# error = standard deviation for the Gap statistic\n",
    "```\n",
    "\n",
    "**Note:** The Gap statistic is also a heuristic, and does not always provide a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80a0d6",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(c): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d294897",
   "metadata": {},
   "source": [
    "### 4.3(d)\n",
    "\n",
    "**Task: Repeat [4.3(a)](#4.3(a))-[4.3(c)](#4.3(c)), but use the original data in [molecule.csv](./molecule.csv) instead of the PCA scores. What is the best number of clusters? Explain your reasoning for selecting the best number of clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334a509",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(d): What is the best number of clusters, and how did you select it?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f1f27",
   "metadata": {},
   "source": [
    "### 4.3(e)\n",
    "\n",
    "**Task: Repeat the clustering of the data in [scores.4.1.csv](./scores.4.1.csv), but use the density-based method [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). How many clusters were identified by DBSCAN?**\n",
    "\n",
    "**Hint:** Assuming that the matrix `X` contains our raw data, DBSCAN clustering can be performed with:\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = clustering.fit(X)\n",
    "```\n",
    "\n",
    "**Note:** The results from DBSCAN may depend on the hyperparameters `eps` and `min_samples`. Explore different values for these parameters and investigate how they affect the number of clusters and noise points identified. Consider visualising the clusters or calculating silhouette scores.\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** Assuming that the matrix `X` contains our raw data, DBSCAN clustering can be performed with:\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = clustering.fit(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141657a",
   "metadata": {},
   "source": [
    "#### Your answer to question 4.3(e): How many clusters did you find with DBSCAN? How is this influenced by the hyperparameters?\n",
    "\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cd76a",
   "metadata": {},
   "source": [
    "## Your feedback for Exercise 4\n",
    "\n",
    "1. **Time & Difficulty:**\n",
    "    * Length (1=too short, 5=too long): 1  2  3  4  5\n",
    "    * Difficulty (1=too easy, 5=too difficult): 1  2  3  4  5\n",
    "    * Most challenging part: _________________________\n",
    "\n",
    "2. **Code Examples:**\n",
    "    * More or less example code?  More  Less  About Right\n",
    "    * Areas where more examples would be helpful: _________________________\n",
    "\n",
    "3. **Errors/Inconsistencies:** Did you encounter any?  Yes  No  If yes, please describe: _________________________\n",
    "\n",
    "4. **Suggestions:** How could this exercise be improved? _________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
