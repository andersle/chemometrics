{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Solution to Exercise set 2: \n",
    "\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Perform least squares regression with multiple predictor variables\n",
    "- Perform variable selection by inspecting correlations.\n",
    "- Explain the distinction between training and testing data.\n",
    "- Calculate effects from an experimental design.\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [2.1(b)](#2.1(b)): To show that you can select variables by using information on correlations.\n",
    "\n",
    "- [2.2(c)](#2.2(c)) and [2.2(e)](#2.2(e)): To show that you can create a linear model with many variables and test it.\n",
    "\n",
    "- [2.3(a)](#2.3(a)), [2.3(b)](#2.3(b)), and [2.3(c)](#2.3(c)): To show that you can calculate effects from an experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a483a4",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Predicting blood pressure\n",
    "\n",
    "The file [bloodpress.csv](bloodpress.csv) contains data for 20 individuals with high blood pressure.\n",
    "Your goal is to create a least squares model for predicting blood pressure (BP) that achieves $R^2 > 0.95$, using a maximum of two predictor variables.\n",
    "\n",
    "The columns present in the data file are:\n",
    "\n",
    "| Column | Description                                                              |             Unit |\n",
    "|:-------|:-------------------------------------------------------------------------|-----------------:|\n",
    "| BP     | Blood pressure                                                           |             mmHg |\n",
    "| Age    | Age                                                                      |            years |\n",
    "| Weight | Weight                                                                   |               kg |\n",
    "| BSA    | Body surface area                                                        |            m$^2$ |\n",
    "| DUR    | Duration of hypertension                                                 |            years |\n",
    "| BHR    | Basal heart rate                                                         | beats per minute |\n",
    "| Stress | Stress index (score derived from a standardized questionnaire)           |              --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26bba8",
   "metadata": {},
   "source": [
    "### 2.1(a)\n",
    "\n",
    "Before building your model, explore the relationships between the blood pressure and the other variables to identify potential predictor candidates for a linear regression model. Do this by creating scatter plots of BP against the other variables and by calculating the [correlation coefficients](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between BP and the other variables.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Calculate the correlation coefficient between BP and each other variable. This can be done with [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"bloodpress.csv\")\n",
    "data.corr()\n",
    "```\n",
    "\n",
    "2. Create scatter plots of BP against each other variable.\n",
    "\n",
    "Based on your analysis of the correlations and scatter plots, which of the variables seem most promising for predicting BP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The correlations:\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"bloodpress.csv\")\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"Age\", \"Weight\", \"BSA\", \"Dur\", \"BHR\", \"Stress\"]\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=len(variables), figsize=(len(variables) * 3, 3), sharey=True\n",
    ")\n",
    "y = data[\"BP\"]\n",
    "for vari, axi in zip(variables, axes):\n",
    "    x = data[vari]\n",
    "    axi.scatter(x, y)\n",
    "    axi.set(xlabel=vari)\n",
    "axes[0].set(ylabel=\"BP\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60531b",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.3(a): Which variables could be most predictive of BP?\n",
    "\n",
    "The scatter plots and correlation matrix reveal the strongest correlations with BP for Age (0.659), Weight (0.950), and BSA (0.866). The other variables show weeker correlations and they scatter plots appear without a clear linear trend. The most promising variables seem to be Weight and BSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103eac20",
   "metadata": {},
   "source": [
    "### 2.1(b)\n",
    "\n",
    "**Task:** Construct a least squares model to predict BP. You can use a maximum of two predictor variables, and the model must achieve an R² value of at least 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Pick the two variables with highest correlation:\n",
    "X = data[[\"Weight\", \"BSA\"]].to_numpy()\n",
    "y = data[\"BP\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "model_bp = LinearRegression(fit_intercept=True)\n",
    "model_bp.fit(X, y)\n",
    "y_hat = model_bp.predict(X)\n",
    "\n",
    "r2 = r2_score(y, y_hat)\n",
    "print(f\"R² = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b25ad9",
   "metadata": {},
   "source": [
    "Well, that did not work as planned. What did we miss? Let us investigate the correlation between weight and BSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.scatterplot(data, x=\"Weight\", y=\"BSA\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce8f3a",
   "metadata": {},
   "source": [
    "Since Weight and BSA are themselves correlated, using both in the model may not provide substantial additional predictive power. We need to identify a variable that is predictive of BP but independent of Weight. From the correlation matrix we observe that Age is weakly correlated with Weight (0.407). Let us try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Weight\", \"Age\"]].to_numpy()\n",
    "y = data[\"BP\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "model_bp = LinearRegression(fit_intercept=True)\n",
    "model_bp.fit(X, y)\n",
    "y_hat = model_bp.predict(X)\n",
    "\n",
    "r2 = r2_score(y, y_hat)\n",
    "print(f\"R² = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6476f",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.1(b): What variables are you using for your prediction\n",
    "\n",
    "In the final model, I am using Weight and Age.  While BSA showed a strong correlation with BP, it was also highly correlated with Weight (0.875), meaning that it provided redundant information. Age, while having a somewhat weaker correlation with BP, was chosen as it is not correlated with Weight and therefore provides independent information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1759",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Predicting solubility in water\n",
    "\n",
    "The dataset [solubility.csv](solubility.csv) contains the measured solubility in water (mol/L) for several molecules. Solubility is a critical property in fields such as drug discovery and environmental chemistry, as it influences how molecules interact with biological systems or the environment. \n",
    "\n",
    "In addition to the solubility measurements, we have calculated some molecular descriptors (for instance, the weight of the molecules) using the [RDKit](https://www.rdkit.org/) library. You will use the descriptors to build a predictive least squares model for the solubility.\n",
    "\n",
    "The raw data is in the following format (showing the first two molecules):\n",
    "\n",
    "| name                           |   measured log(solubility:mol/L) | SMILES              |   MolWt |   HeavyAtomCount |   RingCount |   LogP    |   MaxPartialCharge |   MinPartialCharge |   NOCount |\n",
    "|:-------------------------------|---------------------------------:|:--------------------|--------:|-----------------:|------------:|----------:|-------------------:|-------------------:|----------:|\n",
    "| 1,1,1,2-Tetrachloroethane      |                            -2.18 | ClCC(Cl)(Cl)Cl      | 167.85  |                6 |           0 |    2.5954 |           0.203436 |         -0.122063  |         0 |\n",
    "| 1,1,1-Trichloroethane          |                            -2    | CC(Cl)(Cl)Cl        | 133.405 |                5 |           0 |    2.3765 |           0.187382 |         -0.0840135 |         0 |\n",
    "\n",
    "\n",
    "The columns are defined as follows:\n",
    "\n",
    "\n",
    "| **Column** | **Description** |\n",
    "|:------------|:-----------------|\n",
    "| name       | The name of the molecule. |\n",
    "| measured log(solubility:mol/L) | The logarithm of the measured solubility. The logarithm is used since the solubility can span many orders of magnitude. |\n",
    "| SMILES     | [SMILES](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System) (Simplified Molecular Input Line Entry System) - a way to represent molecular structures as text strings. Included if you want to display the molecule. |\n",
    "| MolWt      | The molecular weight, measured in g/mol. |\n",
    "| HeavyAtomCount | The number of heavy (non-hydrogen) atoms in the molecule. |\n",
    "| RingCount  |  The number of rings in the molecule. |\n",
    "| LogP    | [LogP (octanol-water partition coefficient)](https://en.wikipedia.org/wiki/Octanol-water_partition_coefficient) - a measure of a molecule's lipophilicity (affinity for fatty or oily environments).  A higher LogP indicates greater lipophilicity, while a lower LogP indicates greater hydrophilicity (affinity for water). |\n",
    "| MaxPartialCharge | The largest charge on any atom in the molecule (atomic units). |\n",
    "| MinPartialCharge | The smallest charge on any atom in the molecule (atomic units). |\n",
    "| NOCount          | The number of Nitrogen and Oxygen atoms in the molecule. |\n",
    "\n",
    "\n",
    "To reiterate, the goal is to predict `measured log(solubility:mol/L)` from `MolWt`, `HeavyAtomCount`, `RingCount`, `LogP`, `MaxPartialCharge`, `MinPartialCharge`, and `NOCount`.\n",
    "\n",
    "\n",
    "If you want to visualize some of the molecules, you can use the [RDKit](https://www.rdkit.org/docs/GettingStartedInPython.html) library. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# RDKit imports for working with molecules:\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdCoordGen\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "# Draw molecules using svg (instead of png) for better quality:\n",
    "IPythonConsole.ipython_useSVG = True\n",
    "\n",
    "data = pd.read_csv(\"solubility.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85426e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 33\n",
    "smiles_str = data[\"SMILES\"][index]\n",
    "solubility = data[\"measured log(solubility:mol/L)\"][index]\n",
    "print(f\"Showing SMILES string at index {index}: {smiles_str}\")\n",
    "print(f\"Solubility = {solubility}\")\n",
    "# Create a molecule from a smiles string:\n",
    "mol = Chem.MolFromSmiles(smiles_str)\n",
    "# Generate 2D coordinates for better visualization (this is not needed):\n",
    "rdCoordGen.AddCoords(mol)\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75853a",
   "metadata": {},
   "source": [
    "### 2.2(a)\n",
    "\n",
    "To predict the solubility $y$, we will make a linear model on the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= b_0 + b_1 \\times (\\text{MolWt}) + b_2 \\times (\\text{HeavyAtomCount}) + b_3 \\times (\\text{RingCount})\\\\\n",
    "&+ b_4 \\times (\\text{LogP}) + b_5 \\times (\\text{MaxPartialCharge}) + b_6 \\times (\\text{MinPartialCharge}) + b_7 \\times (\\text{NOCount}) .\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This can be expressed in matrix form as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \\mathbf{X} \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{y}$ is the vector of measured solubility, $\\mathbf{X}$ is the design matrix containing the descriptors, and $\\mathbf{b}$ is the vector of coefficients ($b_0$, $b_1$, ..., $b_7$).\n",
    "\n",
    "The code below attempts to solve for the coefficients $\\mathbf{b}$ by directly inverting $\\mathbf{X}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} =\\mathbf{X}^{-1}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "However, this approach will fail here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b56887",
   "metadata": {},
   "source": [
    "**Task:** Consider the code below, and do the following:\n",
    "\n",
    "1. Explain why the code adds a column of ones to $\\mathbf{X}$.\n",
    "2. Explain why inverting $\\mathbf{X}$ directly will fail.\n",
    "3. Correct the code and find $\\mathbf{b}$ by using the least squares solution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a07796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"solubility.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the column with solubility:\n",
    "y = data[\"measured log(solubility:mol/L)\"].to_numpy().reshape(-1, 1)\n",
    "# The .reshape(-1, 1) is to reshape y into a column vector.\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "# Select variables:\n",
    "descriptors_txt = [\n",
    "    \"MolWt\",\n",
    "    \"HeavyAtomCount\",\n",
    "    \"RingCount\",\n",
    "    \"LogP\",\n",
    "    \"MaxPartialCharge\",\n",
    "    \"MinPartialCharge\",\n",
    "    \"NOCount\",\n",
    "]\n",
    "\n",
    "# Extract the descriptors:\n",
    "descriptors = data[descriptors_txt].to_numpy()\n",
    "# Make a column of ones using np.ones:\n",
    "ones = np.ones((descriptors.shape[0], 1))\n",
    "# Add a column of ones to form X:\n",
    "X = np.hstack((ones, descriptors))\n",
    "print(f\"Shape of X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert X **this will fail**:\n",
    "b = np.linalg.inv(X) @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59513a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_for_b = pd.DataFrame(\n",
    "    {\"Matrix inversion\": [i[0] for i in b]}, index=[f\"b{i}\" for i in range(8)]\n",
    ")\n",
    "table_for_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418ebb",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.2(a):\n",
    "\n",
    "1. We add the column of ones to be able to also calculate the intercept (the $b_0$ coefficient).\n",
    "2. The technical reason is that the matrix X is not square; it has more rows (observations) than columns (predictors), and only square matrices are invertible. Another explanation is to say that the problem is overdetermined, meaning there are more equations (data points) than unknowns (coefficients). This implies that there is no single solution that satisfies all equations exactly. No set of coefficients perfectly predicts all the observed solubility values. \n",
    "3. See the parameters found above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286642d",
   "metadata": {},
   "source": [
    "### 2.2(b)\n",
    "\n",
    "The least squares problem from [2.2(a)](#2.2(a)) can also be solved using [numpy.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) function or the [Moore-Penrose pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), denoted as $\\mathbf{X}^+$. The pseudo-inverse is a generalization of the matrix inverse and allows us to calculate the least squares solution b as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{b} = \\mathbf{X}^+  \\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7bc7c",
   "metadata": {},
   "source": [
    "**Task:** Calculate the least squares solution $\\mathbf{b}$ using both [numpy.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) and the pseudo-inverse and compare with the solution you found in [2.2(a)](#2.2(a))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20318c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First via np.linalg.lstsq:\n",
    "bnp = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "table_for_b[\"linalg.lstsq\"] = [i[0] for i in bnp]\n",
    "# 2. Then with the pseduinverse:\n",
    "pinv = np.linalg.pinv(X) @ y\n",
    "table_for_b[\"pseudo-inverse\"] = [i[0] for i in pinv]\n",
    "table_for_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab1e2e",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.2(b): Do you find the same $\\mathbf{b}$?\n",
    "\n",
    "Yes, we get the same values for the parameters (see the table above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb24da8",
   "metadata": {},
   "source": [
    "### 2.2(c)\n",
    "\n",
    "Calculating least squares solutions using matrix operations can be cumbersome, especially remembering to add the column of ones for the intercept. Libraries like [scikit-learn](https://scikit-learn.org) provide more convenient tools for this task. Scikit-learn models are typically created and trained using the `.fit` method of the appropriate model class. The fit method takes two main arguments:\n",
    "\n",
    "* X: The design matrix. Each row of X represents a single sample and each column represents a variable (feature).\n",
    "* y: The target variable. The values we want to predict from X.\n",
    "\n",
    "\n",
    "For least squares regression, here is how we can fit a least squares\n",
    "model with scikit-learn:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0886a53",
   "metadata": {},
   "source": [
    "**Task:** Create a least squares model using [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [scikit-learn](https://scikit-learn.org). Compare the model's intercept and coefficients with the least squares solution you calculated in previous parts of this exercise.  You can access the intercept and coefficients using:\n",
    "\n",
    "```python\n",
    "print(model.intercept_)  # b0\n",
    "print(model.coef_)  #b1, b2, ..., b7\n",
    "```\n",
    "\n",
    "**Hint:** When using scikit-learn's `LinearRegression`, the design matrix X should not include a column of ones.\n",
    "The intercept is handled automatically by setting `fit_intercept=True` (which is the default).\n",
    "\n",
    "**Note:** You may have noted that we import from `sklearn`, this is the *import name* of the `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X = data[descriptors_txt].to_numpy()\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ef913",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_for_b[\"scikit-learn\"] = [model.intercept_[0]] + [\n",
    "    i for i in model.coef_[0]\n",
    "]\n",
    "table_for_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62890f25",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.2(c): What coefficients do you find and what is the intercept?\n",
    "\n",
    "As shown in the table above, the calculated coefficients remain unchanged from the previous calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e40cf0",
   "metadata": {},
   "source": [
    "### 2.2(d)\n",
    "\n",
    "A primary purpose of building a predictive model is to *predict* properties for *new* samples. We will now evaluate our solubility model's predictive performance by comparing its predictions (ŷ) to the true measured solubilities (y). With scikit-learn, we can use the `.predict(X)` method to generate predictions from a trained model. This method takes in a design matrix X (containing the feature values for the samples we want to predict) as input:\n",
    "\n",
    "```python\n",
    "y_hat = model.predict(X)  # Generate predictions\n",
    "```\n",
    "\n",
    "**Task:** Evaluate how well your model is predicting the solubility by:\n",
    "1. Calculating the [coefficient of determination (R²)](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n",
    "2. Calculating the [mean absolute error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error).\n",
    "3. Creating a scatter plot with the true solubilities (y) on the x-axis plotted against the predicted solubilities (ŷ) on the y-axis. For perfect predictions, all scatter points should fall on the $y=x$ line. \n",
    "\n",
    "**Hint:** Both R² and the MAE can be calculated with methods from [sklearn.metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html): [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) and [mean_absolute_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed260c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# 1.+2. We first use the model and then compute R² and MAE\n",
    "y_hat = model.predict(X)\n",
    "r2 = r2_score(y, y_hat)\n",
    "mae = mean_absolute_error(y, y_hat)\n",
    "print(f\"R² = {r2:.3f}\")\n",
    "print(f\"MAE = {mae:.3f}\")\n",
    "\n",
    "# 3.\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat)\n",
    "ax.set(xlabel=\"Observed values (y)\", ylabel=\"Predicted by model (ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f8452",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.2(d): What values do you get for R² and the MAE?\n",
    "\n",
    "The R² was 0.832 and the MAE was 0.651."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d68d5",
   "metadata": {},
   "source": [
    "### 2.2(e)\n",
    "\n",
    "When we build a predictive model, as you did in [2.2(c)](#2.2(c)), we say that we **train** the model. We refer to the data we used for this process as the **training data** or the **training set**.\n",
    "\n",
    "In [2.2(d)](#2.2(d)), you evaluated how well your model reproduces the solubility values it was trained on. This, however, does not provide a realistic evaluation of how well the model generalizes to *new, unseen data*. A model could perfectly \"memorize\" the training data, achieving excellent performance on it, but fail miserably on new samples. This phenomenon, known as **overfitting**, occurs when the model becomes too specific to the training data and loses its ability to generalize.\n",
    "\n",
    "To assess how well our model is predicting *new* samples and check for overfitting, we must apply it to a set of data that was not part of the training data.\n",
    "The file [solubility_test.csv](solubility_test.csv) contains data for additional molecules *not* used in training. This data, called the **test set** or **test data** can be used to **test** how well our model is predicting solubilities for new samples.\n",
    "\n",
    "\n",
    "**Task:** Evaluate how well your model is predicting the test set:\n",
    "\n",
    "1. Load the data from `solubility_test.csv` into a new design matrix, `X_test`.\n",
    "\n",
    "2. Use your trained model to predict the solubilities for the molecules in `X_test`.\n",
    "\n",
    "3. Calculate R² and the MAE using the *test set* predictions and the true values from `solubility_test.csv`.\n",
    "\n",
    "4. Create a scatter plot of predicted vs. actual solubilities for the *test set*.\n",
    "\n",
    "5. Compare the test set performance metrics (R², MAE, and the scatter plot) with the training set performance metrics you calculated previously. Note any differences you observe and explain what these differences might suggest about your model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "data_test = pd.read_csv(\"solubility_test.csv\")\n",
    "X_test = data_test[descriptors_txt].to_numpy()\n",
    "# 2.\n",
    "y_test_true = (\n",
    "    data_test[\"measured log(solubility:mol/L)\"].to_numpy().reshape(-1, 1)\n",
    ")\n",
    "y_test_hat = model.predict(X_test)\n",
    "# 3.\n",
    "r2_test = r2_score(y_test_true, y_test_hat)\n",
    "mae_test = mean_absolute_error(y_test_true, y_test_hat)\n",
    "print(f\"R² = {r2_test:.3f}\")\n",
    "print(f\"MAE = {mae_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat, label=\"Training data\")\n",
    "ax.scatter(y_test_true, y_test_hat, label=\"Testing data\")\n",
    "ax.set(xlabel=\"Observed values (y)\", ylabel=\"Predicted by model (ŷ)\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd12a2b",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.2(e): What values do you get for R² and the MAE for the test set? Are they significantly different from the training data?\n",
    "\n",
    "he R² for the training set is 0.83, and for the test set, it is 0.85. The MAE for the training set is 0.65, and for the test set, it is 0.61. The close agreement between training and test set R² and MAE values indicates that the model generalizes well to unseen data and is not overfitting.  The MAE of around 0.61, given that the (logarithmic) solubility values range from -12 to 2, suggests that the model provides reasonably accurate predictions of solubility for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88587546",
   "metadata": {},
   "source": [
    "### Remarks for exercise 2.2\n",
    "\n",
    "**Remark 1:** The variables you have used in X have different units and they span different ranges. Some modelling methods are sensitive to these differences. We will later in TKJ4175 scale such variables to a common range to address this.\n",
    "\n",
    "**Remark 2:** Machine learning libraries like scikit-learn provide a consistent interface for predictive models. Most models, including linear regression, implement the `.fit()` and `.predict()` methods. This means you can easily experiment with more advanced models by largely reusing your existing code - simply replace the model instantiation, for example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso  # Use LASSO regression\n",
    "model2 = Lasso(fit_intercept=True)\n",
    "model2.fit(X, y)\n",
    "y_hat = model2.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ff098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To follow up on Remark 2, here is one example:\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model2 = CatBoostRegressor()\n",
    "model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_2 = model2.predict(X)\n",
    "y_test_hat_2 = model2.predict(X_test)\n",
    "r2_test2 = r2_score(y_test_true, y_test_hat_2)\n",
    "mae_test2 = mean_absolute_error(y_test_true, y_test_hat_2)\n",
    "print(f\"R² = {r2_test2:.3f}\")\n",
    "print(f\"MAE = {mae_test2:.3f}\")\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(y, y_hat_2, label=\"Training data\")\n",
    "ax.scatter(y_test_true, y_test_hat_2, label=\"Testing data\")\n",
    "ax.set(xlabel=\"Observed values (y)\", ylabel=\"Predicted by model (ŷ)\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f7b68",
   "metadata": {},
   "source": [
    "## Exercise 2.3 Analyzing an experimental design\n",
    "\n",
    "The growth rate of a particular bacterium species depends on the concentration of nutrients such as phosphate ($P$),\n",
    "sucrose ($S$), and nitrate ($N$). The table below displays the experimental design used to investigate how these three concentrations influence the growth rate.\n",
    "\n",
    "\n",
    "|$P$  | $S$ | $N$ | $PS$ | $PN$ | $SN$ | $PSN$ | **Growth rate**  |\n",
    "|:---:|:---:|:---:|:----:|:----:|:----:|:-----:|:----------------:|\n",
    "| $+$ | $-$ | $-$ | $-$  | $-$  | $+$  | $+$   | $7$              |  \n",
    "| $-$ | $+$ | $-$ | $-$  | $+$  | $-$  | $+$   | $10$             | \n",
    "| $+$ | $-$ | $+$ | $-$  | $+$  | $-$  | $-$   | $8$              | \n",
    "| $-$ | $+$ | $+$ | $-$  | $-$  | $+$  | $-$   | $11$             |  \n",
    "| $-$ | $-$ | $-$ | $+$  | $+$  | $+$  | $-$   | $11$             |\n",
    "| $+$ | $+$ | $+$ | $+$  | $+$  | $+$  | $+$   | $12$             |\n",
    "| $+$ | $+$ | $-$ | $+$  | $-$  | $-$  | $-$   | $7$              |\n",
    "| $-$ | $-$ | $+$ | $+$  | $-$  | $-$  | $+$   | $7$              | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202d5f8",
   "metadata": {},
   "source": [
    "### 2.3(a)\n",
    "Use the information in the table and compute the main effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb944a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([7, 10, 8, 11, 11, 12, 7, 7])\n",
    "P = np.array([1, -1, +1, -1, -1, 1, 1, -1])\n",
    "S = np.array([-1, 1, -1, 1, -1, 1, 1, -1])\n",
    "N = np.array([-1, -1, 1, 1, -1, 1, -1, 1])\n",
    "\n",
    "effect_P = np.dot(P, y) / 4\n",
    "effect_S = np.dot(S, y) / 4\n",
    "effect_N = np.dot(N, y) / 4\n",
    "\n",
    "print(f\"Effect(P): {effect_P}\")\n",
    "print(f\"Effect(S): {effect_S}\")\n",
    "print(f\"Effect(N): {effect_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f6740",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.3(a): What are the main effects?\n",
    "The effects are:\n",
    "\n",
    "- Effect(P): -1.25\n",
    "- Effect(S): 1.75\n",
    "- Effect(N): 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bdf51",
   "metadata": {},
   "source": [
    "### 2.3(b)\n",
    "Verify that the columns for the 2-factor and 3-factor interaction effects are correct in table 2 and compute the interaction effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad953853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Multiply for the new columns:\n",
    "PS = P * S\n",
    "PN = P * N\n",
    "SN = S * N\n",
    "PSN = P * S * N\n",
    "\n",
    "# Let us try to print this out:\n",
    "design = np.column_stack((P, S, N, PS, PN, SN, PSN, y))\n",
    "design = pd.DataFrame(\n",
    "    design, columns=[\"P\", \"S\", \"N\", \"PS\", \"PN\", \"SN\", \"PSN\", \"Growth rate\"]\n",
    ")\n",
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ffc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the remaining effects:\n",
    "effect_PS = np.dot(PS, y) / 4\n",
    "effect_PN = np.dot(PN, y) / 4\n",
    "effect_SN = np.dot(SN, y) / 4\n",
    "effect_PSN = np.dot(PSN, y) / 4\n",
    "\n",
    "print(f\"Effect(PS): {effect_PS}\")\n",
    "print(f\"Effect(PN): {effect_PN}\")\n",
    "print(f\"Effect(SN): {effect_SN}\")\n",
    "print(f\"Effect(PSN): {effect_PSN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd76b2",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.3(b): What are the interaction effects?\n",
    "\n",
    "The interaction effects are:\n",
    "\n",
    "- Effect(PS): 0.25\n",
    "- Effect(PN): 2.25\n",
    "- Effect(SN): 2.25\n",
    "- Effect(PSN): -0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00eb421",
   "metadata": {},
   "source": [
    "### 2.3(c)\n",
    "What factors and interactions seem to increase the growth rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4067408",
   "metadata": {},
   "source": [
    "#### Your answer to question 2.3(c):\n",
    "\n",
    "The growth rate is increased by the factors S and N and the interactions PS, PN and SN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
