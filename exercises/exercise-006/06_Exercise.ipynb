{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801b2291",
   "metadata": {},
   "source": [
    "# Exercise set 6: Classification\n",
    "\n",
    "The main goals of this exercise are to create classifiers and calculate and interpret some performance metrics that can be used to assess the classifiers.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "- Create classification models.\n",
    "- Create and interpret the confusion matrix and use it to evaluate classifier performance.\n",
    "- Visualise how a decision tree is making its classification.\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [6.1(b)](#6.1(b)), [6.1(c)](#6.1(c)), [6.1(d)](#6.1(d)), and [6.1(e)](#6.1(e)): To show that you can create a decision tree, plot the confusion matrix and visualise the decision tree itself, and compare classifiers using the confusion matrix.\n",
    "\n",
    "**Note:** A solution to [Exercise 6.2](#Exercise-6.2) is available online (see Blackboard or [GitHub](https://github.com/andersle/chemometrics/tree/main/exercises/exercise-006)) for those who wish to practice the interpretation aspect ([6.2(b)](#6.2(b)) and [6.2(f)](#6.2(f))) without completing the programming portion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b4748",
   "metadata": {},
   "source": [
    "## Exercise 6.1 Penguins\n",
    "\n",
    "In this exercise, we will have a look at penguins! We will attempt to figure out the species of penguins based\n",
    "on their bill length, bill depth, flipper length, and body mass.\n",
    "The data is from a paper by \n",
    "[Gorman, Williams, and Fraser](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081)\n",
    "and can also be found in the R package [palmerpenguins](https://github.com/allisonhorst/palmerpenguins).\n",
    "Here, we will use a version of the data set [penguins.csv](penguins.csv) where missing\n",
    "values have been removed and we only have two species of penguins: [Adelie](https://en.wikipedia.org/wiki/Ad%C3%A9lie_penguin) and [Chinstrap](https://en.wikipedia.org/wiki/Chinstrap_penguin).\n",
    "\n",
    "The image below shows the three islands where these penguins can be found (click the image to make it larger): \n",
    "| <a href=\"penguins.png\"><img src=\"penguins2.png\" width=\"50%\"></a>           |\n",
    "|:-:|\n",
    "| **Fig. 1** *Location of islands and images of the penguin species.*    |\n",
    "\n",
    "\n",
    "You will find seven columns in the [penguins.csv](./Data/penguins.csv) data file. Each row is a measurement for\n",
    "a single penguin for the seven variables found in the columns:\n",
    "\n",
    "\n",
    "| Column            |  Description                                                        |\n",
    "|:------------------|--------------------------------------------------------------------:|\n",
    "| species           | The species (Chinstrap or Adelie)                                   |\n",
    "| island            | The island where the observation was made (Dream/Torgersen/Biscoe)  |\n",
    "| bill_length_mm    | (See the illustration below) (measured in mm)                       |\n",
    "| bill_depth_mm     | (See the illustration below) (measured in mm)                       |\n",
    "| flipper_length_mm | (See the illustration below) (measured in mm)                       |\n",
    "| body_mass_g       | The weight of the penguin (in grams)                                |\n",
    "| sex               | Female/Male                                                         |\n",
    "\n",
    "\n",
    "| <img src=\"bill.png\" width=\"50%\">                                   |\n",
    "|:-:|\n",
    "| **Fig. 2** *Illustration of bill length, bill depth, and flipper length. (The foot is not used in this data set.)*    |\n",
    "\n",
    "The data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f161cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"penguins3.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dad47d",
   "metadata": {},
   "source": [
    "### 6.1(a)\n",
    "\n",
    "**Task: Investigate (by creating figures) if the variables `bill_length_mm`, `bill_depth_mm`,\n",
    "`flipper_length_mm`, and `body_mass_g` can be used to separate the different\n",
    "species.**\n",
    "\n",
    "**Hint:** Several plots can be used to get an overview of the data. For instance, the [scatter plot matrix](https://seaborn.pydata.org/examples/scatterplot_matrix.html), [jointplot](https://seaborn.pydata.org/tutorial/introduction.html#multivariate-views-on-complex-datasets)\n",
    "from seaborn, or a [boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html). Here is one example of how to create the scatter plot matrix:\n",
    "```python\n",
    "# To create a scatter plot matrix with seaborn:\n",
    "grid = sns.pairplot(\n",
    "    data,\n",
    "    corner=True,\n",
    "    hue=\"species\",  # Hue is used to select a column from data to use for colouring\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80184e",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(a): Are there any promising variables that could separate the species?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab6e77",
   "metadata": {},
   "source": [
    "### 6.1(b)\n",
    "\n",
    "**Task: Create a training set and a test set to use to classify the penguin species. What is the fraction of Adelie penguins in the original, test, and training data?**\n",
    "\n",
    "**Hint:** With scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), splitting the data can be done with\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    stratify=y\n",
    ")\n",
    "```\n",
    "In the example above, we use stratification for the y-values, this is to **ensure that each split** (training and testing) **contains approximately the same proportion of samples from each class as the original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0a567",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(b): What is the fraction of Adelie penguins?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f980a",
   "metadata": {},
   "source": [
    "### 6.1(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify the penguin species. Use two levels for the tree and show the confusion matrix for the training and the test set. Is your classifier making any mistakes?**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. A decision tree can be created using scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the tree. The parameter max_depth selects the number of levels in the tree\n",
    "my_first_tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Train the tree:\n",
    "my_first_tree.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "2. To show the confusion matrix:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    my_first_tree,  # The classifier to construct the confusion matrix for.\n",
    "    X_train,  # The X data.\n",
    "    y_train,  # The true y data.\n",
    "    colorbar=True,  # Add a colorbar to show the color scale.\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff977844",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(c): Is your classifier making any mistakes?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842d87d",
   "metadata": {},
   "source": [
    "### 6.1(d)\n",
    "\n",
    "**Task: Visualise your decision tree and use this to describe how the classification is made.**\n",
    "\n",
    "**Hint:** The decision tree can be visualized using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) or [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html),\n",
    "\n",
    "1. Using [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html):\n",
    "\n",
    "```python\n",
    "from sklearn import tree\n",
    "\n",
    "variables = [\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    \"body_mass_g\",\n",
    "]\n",
    "\n",
    "tree.plot_tree(\n",
    "    my_first_tree,  # The tree to plot\n",
    "    filled=True,  # Add color to the boxes.\n",
    "    feature_names=variables,  # Get name for variables from the variables list.\n",
    "    class_names=my_first_tree.classes_,  # Get the name of the different classes from the tree.\n",
    ")\n",
    "```\n",
    "\n",
    "2. Alternative: Using [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html):\n",
    "\n",
    "```python\n",
    "from sklearn.tree import export_graphviz  # To create the tree.\n",
    "import graphviz  # To turn the three into a graph, you may need to install this (pip install graphviz).\n",
    "from IPython.display import display  # To show the graph.\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    my_first_tree,  # The tree to plot.\n",
    "    out_file=None,  # Do not write to file.\n",
    "    feature_names=variables,  # Get name for variables from the variables list.\n",
    "    class_names=my_first_tree.classes_,  # Get the name of the different classes from the tree.\n",
    "    rounded=True,  # Show the boxes in the tree with rounded corners.\n",
    "    filled=True,  # Add color to the boxes.\n",
    ")\n",
    "display(graphviz.Source(dot_data))  # Show the tree in a notebook.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b855114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a17287",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(d): Describe the decision-making process of your classifier.\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f484ff",
   "metadata": {},
   "source": [
    "### 6.1(e)\n",
    "\n",
    "The figure below compares a decision tree classifier to a k-nearest neighbours classifier (using one neighbour) for the test set.\n",
    "\n",
    "**Task: Use the figure to compare the two classifiers (the left part shows the confusion matrix of the tree classifier applied to the test set, and the right part shows the k-nearest neighbours classifier applied to the same test set). Which one performs best?**\n",
    "\n",
    "![Compare classifiers](comparecls.png)\n",
    "\n",
    "**Note:** Your confusion matrix in [6.1(c)](#6.1(c)) may differ from the one shown here since the splitting into a test and training set is randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408da68",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.1(e): Which of the two classifiers performs best?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44457c41",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "\n",
    "[Schummer *et al.*](https://doi.org/10.1016/S0378-1119(99)00342-X) used microarray technology to analyze the expression of 1536 genes in ovarian cancer and non-cancer tissues. Their primary objective was to identify differentially expressed genes in ovarian cancer versus non-cancer tissues to discover genes with diagnosis potential.\n",
    "\n",
    "The data file [`ovo.csv`](ovo.csv) contains numerical gene expressions (for 1536 genes) for 54 tissue samples. Each column corresponds to a specific gene, named `X.1`, `X.2`, and so on. Each tissue sample has been classified as non-cancer (`N`) or cancer (`C`) tissue, and these labels can be found in the column `class`. The raw data has been preprocessed by centring each gene expression so that no further preprocessing is needed. The raw data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the data set.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_ovo = pd.read_csv(\"ovo.csv\")\n",
    "classes = data_ovo[\"class\"]  # Classification of samples.\n",
    "# Turn the class labels into numbers for numeric methods\n",
    "y_ovo = [1 if i == \"C\" else 0 for i in classes]\n",
    "X_ovo = data_ovo.filter(like=\"X.\", axis=1)  # Gene expressions for samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2574ba0",
   "metadata": {},
   "source": [
    "### 6.2(a)\n",
    "\n",
    "**Task: Explore the raw data. Do you find genes that appear to show significant differences in expression between non-cancer and cancer tissue?**\n",
    "\n",
    "**Hint:** You can, for instance, inspect the raw data by running a principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c552259",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(a): Did you find any promising genes?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea56348",
   "metadata": {},
   "source": [
    "### 6.2(b)\n",
    "\n",
    "**Task: In the following task, you will develop a classifier to predict whether a tissue sample is cancerous or non-cancerous based on gene expression data. Which error type (false positive or false negative) should be minimised?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9cf63",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(b): Will you minimise false positives or negatives?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a9963",
   "metadata": {},
   "source": [
    "### 6.2(c)\n",
    "\n",
    "**Task: Create a decision tree classifier to classify tissue type from the gene expressions. Optimize the tree depth using cross-validation on a training set. Report the optimal maximum depth of the resulting tree.**\n",
    "\n",
    "With reference to the previous problem:\n",
    "\n",
    "* If you prioritised minimising false positives, use the `precision` as your optimisation metric.\n",
    "* If you prioritised minimising false negatives, use the `recall` as your optimisation metric.\n",
    "* If you opted for a balanced approach, use the `balanced_accuracy` as your optimisation metric.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the decision tree can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"max_depth\": range(1, 10)}\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_tree = grid.best_estimator_\n",
    "print(\"Best tree:\", best_tree)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b87d1",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(c): What depth did you get for your tree?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc51e4",
   "metadata": {},
   "source": [
    "### 6.2(d)\n",
    "\n",
    "**Task: Create a k-nearest neighbours classifier to classify tissue type from the gene expressions. Optimize the number of neighbours using cross-validation on a training set. Report the optimal number of neighbours.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the k-nearest neighbours classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\"n_neighbors\": range(1, 15)}\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_knn = grid.best_estimator_\n",
    "print(\"Best knn:\", best_knn)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3886e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a116b6",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(d): What was the optimal number of neighbours?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a61c",
   "metadata": {},
   "source": [
    "### 6.2(e)\n",
    "\n",
    "**Task: Create a random forest classifier to classify tissue type from the gene expressions. Optimize the number of trees and levels using cross-validation on a training set. Report the optimal number of trees and levels.**\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. The optimisation of the random forest classifier can be done as follows (assuming that you have already split into the training and test sets):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up a grid search:\n",
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 500],  # the number of trees\n",
    "    \"max_depth\": range(1, 11),  # the maximum depth\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    parameters,\n",
    "    scoring=\"recall\",  # Swap this with the metric you prefer\n",
    "    verbose=2,  # Print out text to show the progress of the fitting\n",
    ")\n",
    "# Run the grid search:\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best classifier from the grid search:\n",
    "best_forest = grid.best_estimator_\n",
    "print(\"Best forest:\", best_forest)\n",
    "print(\"Best score\", grid.best_score_)\n",
    "print(\"Best parameters\", grid.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7828d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9512269",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(e): What was the optimal number of estimators and tree depth?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894199b8",
   "metadata": {},
   "source": [
    "### 6.2(f)\n",
    "\n",
    "**Task: Compare the three optimised classifiers you have made by applying them to the test set and obtaining the corresponding confusion matrices. Also compute the [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), and the [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) for the test set. Which classifier performs best?**\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The metrics can be computed as follows:\n",
    "```python\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "y_hat = best_tree.predict(X_test)\n",
    "recall_tree = recall_score(y_test, y_hat)\n",
    "precision_tree = precision_score(y_test, y_hat)\n",
    "bac_tree = balanced_accuracy_score(y_test, y_hat)\n",
    "print(f\"Recall: {recall_tree:.3f}\")\n",
    "print(f\"Precision: {precision_tree:.3f}\")\n",
    "print(f\"Balanced accuracy: {bac_tree:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_tree,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    colorbar=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace6af5",
   "metadata": {},
   "source": [
    "#### Your answer to question 6.2(f): Which classifier performs best?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a91a9",
   "metadata": {},
   "source": [
    "## Your feedback for Exercise 6\n",
    "\n",
    "1. **Time & Difficulty:**\n",
    "* Length (1=too short, 5=too long): 1  2  3  4  5\n",
    "* Difficulty (1=too easy, 5=too difficult): 1  2  3  4  5\n",
    "* Most challenging part: ________________________\n",
    "\n",
    "2. **Code Examples:**\n",
    "* More or less example code?  More  Less  About Right\n",
    "* Areas where more examples would be helpful: ________________________\n",
    "\n",
    "3. **Errors/Inconsistencies:** Did you encounter any?  Yes  No  If yes, please describe: ________________________\n",
    "    \n",
    "4. **Suggestions:** How could this exercise be improved? ________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
