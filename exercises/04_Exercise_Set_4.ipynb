{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 4\n",
    "\n",
    "> This exercise aims to show you how to perform **least squares regression** \n",
    "> for real experimental data. In the first part, we will use data that\n",
    "> contains uncertainties, and we are going \n",
    "> to make use of this in the fitting and for estimating errors in\n",
    "> the fitted parameter.\n",
    "> In the second part, we will use testing/training to estimate\n",
    "> what kind of errors we can expect when using a model for estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "\n",
    "In this exercise we will use least-squares regression to investigate a physical phenomenon: the decay of\n",
    "beer froth with time. The file [Data/erdinger.csv](Data/erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights.\n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this work. In\n",
    "the [original study](https://doi.org/10.1088/0143-0807/23/1/304), Leike reported data\n",
    "for two more beers. The data for these two are in the\n",
    "files [Data/augustinerbrau.csv](Data/augustinerbrau.csv) and [Data/budweiser.csv](Data/budweiser.csv).\n",
    "If you have extra time, you can try to redo [4.1(d)](#4.1(d)) also for these two beers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(a)\n",
    "Create a linear model for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the coefficient of determination, $R^2$ , and plot\n",
    "the residuals. What do you think about your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # Styling of plots\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "\n",
    "data = pd.read_csv(\"Data/erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you are encouraged to try sklearn\n",
    "# and its LinearRegression method:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Here is an example on how to use it:\n",
    "\n",
    "# First, we create the model:\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Next, set up the x and y data.\n",
    "\n",
    "X = data[\"time\"].to_numpy().reshape(-1, 1)\n",
    "y = data[\"height\"].to_numpy()\n",
    "\n",
    "# The data from pandas is converted to a\n",
    "# numpy array with to_numpy(). The data is then reshaped\n",
    "# with reshape(-1, 1). This is to convert X into a matrix\n",
    "# and not just a vector. sklearn assumes in general that the input\n",
    "# X is a matrix, so for 1D cases, we often have to add a reshape\n",
    "# operation like the one below.\n",
    "\n",
    "# Also note that we do not add a column of ones to X.\n",
    "# We have already said fit_intercept=True above and\n",
    "# it will then be added automatically.\n",
    "\n",
    "# To fit the model, give it both X and y:\n",
    "model.fit(X, y)\n",
    "# To show the coefficients:\n",
    "print(model.intercept_, model.coef_)\n",
    "# To use the model for predicting something:\n",
    "y_hat = model.predict(X)\n",
    "# To calculate R²:\n",
    "r2 = model.score(X, y)\n",
    "# or:\n",
    "r2 = r2_score(y, y_hat)\n",
    "print(\"R² =\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is a hint for the plottUing:\n",
    "# Since the raw data contains errors, it is nice to\n",
    "# visualize them as error bars:\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    data[\"time\"],\n",
    "    data[\"height\"],\n",
    "    yerr=data[\"height-error\"],\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4,  # Size of end of the error bars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(a): \"What do you think about your model?\"\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that we get\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter.\n",
    "We will assume that $h(0)$ is a known parameter equal to the height of the froth at the initial time.\n",
    "\n",
    "Show how you can transform the equation above to a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$.\n",
    "\n",
    "**Note:** The equation $y=bx$ does not include the usual constant term.\n",
    "This will modify the least squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept)\n",
    "You can use the equation from the appendix to calculate $b$ in the following or (recommended!)\n",
    "make use of methods where you can turn off the intercept, for instance\n",
    "[``LinearRegression(fit_intercept=False)``](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(b):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(c)\n",
    "Use the transformation you found above to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(c): What value did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276$ s which is probably lower than the\n",
    "value you found in the previous task.\n",
    "We will now try to reproduce the results of Leike, but to\n",
    "do that, we have to do weighted least squares.\n",
    "\n",
    "As you have seen,\n",
    "the raw data includes errors that are not constant. We can use\n",
    "these errors to give weights to the data points in the fitting:\n",
    "we give more importance\n",
    "to points with smaller errors and less importance to points with larger errors.\n",
    "\n",
    "One way forward is to assign weights ($w_i$) as $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting to $y = \\log (h(t) / h(0))$, and this will also modify the errors.\n",
    "If you remember [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that $\\sigma_y^2 = \\sigma_h^2 / h^2$, and this is\n",
    "the transformation we need.\n",
    "\n",
    "Do the following steps to perform the weighted\n",
    "least squares:\n",
    "* (i) Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "* (ii) Calculate weights for your $y$ values as $w = 1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "* (iii) Run a weighted least squares fitting using your $w$'s as weights (see the Jupyter notebook version\n",
    "  for more hints), and find $\\tau$. Plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to run weighted least squares:\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# Just create some weights (not correct for 4.1(d))\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "model.fit(X, y, sample_weight=weights)  # Do fitting, but use the weights\n",
    "r2 = model.score(\n",
    "    X, y, sample_weight=weights\n",
    ")  # Calculate R² (considering the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(d): What value did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(e)\n",
    "Since we do have measured errors here, we can use them to estimate the error in the\n",
    "parameter you just found. For a weighted least squares fit to the equation $y = bx$,\n",
    "the error estimate ($\\sigma_b$) for $b$ is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_b^2 = \\frac{1}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "Estimate the error for the $\\tau$-value you just found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(e): What boundaries ($\\pm$) did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "[Forbes](https://doi.org/10.1017/S0080456800032075) investigated the\n",
    "relationship between the boiling point of water and\n",
    "the atmospheric pressure, and collected data in the Alps and Scotland.\n",
    "Forbes' goal\n",
    "was to estimate altitudes from the boiling point alone. We will see if we can\n",
    "estimate the atmospheric pressure from Forbes' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2(a) \n",
    "Load the data from Forbes (data file [Data/forbes.csv](Data/forbes.csv)), plot it,\n",
    "and create a linear model\n",
    "that predicts the atmospheric pressure from the temperature. Report the R² and [mean\n",
    "squared error (MSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: sklearn has a method for the MSE:\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.2(a): What R² did you get and what was the MSE?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2(b) \n",
    "\n",
    "Estimate the error you can expect to make if you use your model for predicting the pressure.\n",
    "Do this by Leave-one-out cross-validation (LOOCV) and calculate the mean squared error\n",
    "of cross-validation ($\\text{MSE}_\\text{CV}$)\n",
    "\n",
    "LOOCV is a special case of **training** and **testing**, and you can find a short description of it\n",
    "in [appendix B](#B.-Leave-one-out-cross-validation). Please see the Jupyter notebook for a code example you can use. The code\n",
    "example for LOOCV is concise, so make sure you understand what goes on here (that is,\n",
    "what LOOCV is doing). If you are working with someone, try explaining testing/training\n",
    "and how LOOCV works to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 of LOOCV:\n",
    "# sklearn has a method to pick out samples for leave-one-out:\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# To split into training and testing, we can use loo.split()\n",
    "error = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # train_index = index of samples to use for training\n",
    "    # test_index = index of samples to use for testing\n",
    "    # Pick out samples (for training and testing):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Fit a new model with the training set:\n",
    "    model = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    # Predict y for the test set:\n",
    "    y_hat = model.predict(X_test)\n",
    "    # Compare the predicted y values in the test set with the measured ones:\n",
    "    error.append((y_test - y_hat) ** 2)\n",
    "mse_cv_1 = np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 of LOOCV:\n",
    "# sklearn has a method for leave-one-out selection, and a method for\n",
    "# cross-validation. And these two can be combined:\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# Create \"empty\" model for fitting:\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "# Run cross validation, where we select testing and\n",
    "# training with LeaveOneOut:\n",
    "scores = cross_val_score(\n",
    "    model, X, y, scoring=\"neg_mean_squared_error\", cv=LeaveOneOut()\n",
    ")\n",
    "mse_cv_2 = np.mean(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The scoring is `\"neg_mean_squared_error\"` above, which is the negative of the mean squared error. This is maybe schematics, but many methods in sklearn return a \"score\", and for most of us, a better score = a better result. So if we used the mean squared error as the score, then a larger score = a larger error = a poorer result. However, with the negative sign, a larger score (closer to zero) = smaller error = better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.2(b): What $\\text{MSE}_\\text{CV}$ did you get?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimizing the sum of squared errors (assuming that we have $n$\n",
    "measurements of $y$ and $x$),\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimization,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in the ``normal'' least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Leave-one-out cross-validation\n",
    "\n",
    "In Leave-one-out cross-validation (LOOCV), we first pick one sample,\n",
    "measurement number $j$, and we fit the model using the $n-1$ other points\n",
    "(all points except $j$). After the fitting, we check how well the model can predict\n",
    "measurement $j$ by calculating the difference between the\n",
    "measured ($y_j$) and predicted ($\\tilde{y}_j$) value. This difference, $r_j = y_{j} - \\tilde{y}_j$, is\n",
    "called the predicted residual, and it tells us the error we just made.\n",
    "\n",
    "There is nothing special about picking point $j$, and we can try all possibilities\n",
    "of leaving one point out, fitting the model using the remaining $n-1$\n",
    "measurements, and predicting the value we left out.\n",
    "After doing this for all possibilities, we have fitted the model\n",
    "$n$ times and calculated $n$ predicted residuals. The mean squared error (obtained from the squared\n",
    "residuals), $\\mathrm{MSE}_{\\mathrm{CV}}$, can then be used\n",
    "to estimate the error in the model,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the measured $y$ in experiment $i$, and $\\tilde{y}_i$ is the\n",
    "predicted $y$, using a model which was fitted using all points *except* $y_i$.\n",
    "\n",
    "For a polynomial fitting, there is an alternative to refitting the model $n$ times. In fact,\n",
    "we can show that for polynomial fitting, the mean squared error can\n",
    "be obtained by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2 =\n",
    "\\frac{1}{n}\\sum_{i=1}^{m} \\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{y}_i$'s are predicted values using the\n",
    "model fitted with *all data points*,\n",
    "and $h_{ii}$ is the $i$'th diagonal element of the\n",
    "$\\mathbf{H}$ matrix (the projection matrix,\n",
    "see Eq.(4.49) on page 49 in our textbook),\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H} =\n",
    "\\mathbf{X} \n",
    "\\left( \n",
    "  \\mathbf{X}^\\mathrm{T} \\mathbf{X}\n",
    "\\right)^{-1}\n",
    "\\mathbf{X}^\\mathrm{T} = \\mathbf{X} \\mathbf{X}^+,\n",
    "\\end{equation}\n",
    "\n",
    "Note the difference between $\\hat{y}_i$ and $\\tilde{y}_i$, and the\n",
    "fact that we  do not have to do the\n",
    "refitting(!) to obtain the $\\mathrm{MSE}_{\\mathrm{CV}}$.\n",
    "\n",
    "When you calculate $\\mathrm{MSE}_{\\mathrm{CV}}$, use one of the two approaches above or both\n",
    "if you want to see if they give the same answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
