{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6021080e",
   "metadata": {},
   "source": [
    "# Heteroscedasticity\n",
    "\n",
    "[Weighted least squares](https://en.wikipedia.org/wiki/Weighted_least_squares) is not a part\n",
    "of TKJ4175, but there was a question in Lecture 4 on how we can set\n",
    "the weights if we want to use them. Here is an example of how this can be done. We first create a model with\n",
    "ordinary least squares, and then we use the residuals from this model to set weights for the weighted least squares method.\n",
    "\n",
    "In some cases, heteroscedasticity can be \"fixed\" by a suitable data transformation. There are also two examples of this here, and then a link to an article with additional information on what\n",
    "such transformations might do to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746858aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057bb6f",
   "metadata": {},
   "source": [
    "The file [noise.csv](./noise.csv) contains a set of x and y values where the noise is heteroscedastic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ffa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"noise.csv\")\n",
    "x = data[\"x\"].to_numpy()\n",
    "y = data[\"y\"].to_numpy()\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(x, y)\n",
    "ax.set(xlabel=\"x\", ylabel=\"y\", title='Raw data: \"noise.csv\"')\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59e08d",
   "metadata": {},
   "source": [
    "## Standard least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "X = x.reshape(-1, 1)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(X, y, model, weights=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    "    )\n",
    "    ax1.scatter(X, y)\n",
    "    y_hat = model.predict(X)\n",
    "    r2 = model.score(X, y, sample_weight=weights)\n",
    "    if weights is not None:\n",
    "        r2_2 = r2_score(y, y_hat)\n",
    "        text = f\"ŷ = {model.intercept_:.3g} + {model.coef_[0]:.3g}*x\\nR² (weighted) = {r2:.3g}\\nR² (non-weighted) {r2_2:.3g}\"\n",
    "    else:\n",
    "        text = f\"ŷ = {model.intercept_:.3g} + {model.coef_[0]:.3g}*x\\nR² = {r2:.3g}\"\n",
    "    ax1.plot(X, y_hat, color=\"k\", label=text)\n",
    "    ax1.set(xlabel=\"x\", ylabel=\"y\")\n",
    "    ax1.legend()\n",
    "\n",
    "    residual = y - y_hat\n",
    "    ax2.scatter(y_hat, residual, label=\"Residuals (non-weighted)\")\n",
    "    if weights is not None:\n",
    "        ax2.scatter(\n",
    "            y_hat, residual * weights, label=\"Residuals (weighted)\", marker=\"s\"\n",
    "        )\n",
    "        ax2.legend()\n",
    "    ax2.set(xlabel=\"ŷ\", ylabel=\"Residuals (y - ŷ)\", title=\"Residuals\")\n",
    "    ax2.axhline(y=0, ls=\":\", color=\"k\")\n",
    "    sns.despine(fig=fig)\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361bc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = plot_results(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214dfe11",
   "metadata": {},
   "source": [
    "## Weighted least squares\n",
    "\n",
    "Let us try weighted least squares. Here, we say that the weights are equal to the residuals we got\n",
    "from the ordinary least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07af8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1.0 / abs(residuals)  # Make sure weights are positive\n",
    "# or, alternatively:\n",
    "# weights = 1.0 / residuals**2\n",
    "weights = weights / np.sqrt(np.dot(weights, weights))  # Normalize weights\n",
    "model2 = LinearRegression(fit_intercept=True)\n",
    "model2.fit(X, y, sample_weight=weights)\n",
    "_ = plot_results(X, y, model2, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461328f6",
   "metadata": {},
   "source": [
    "**Note:** R² looks a lot better for the weighted model. If we just calculate R² without weights, it will be similar to R² for the ordinary least squares model we made first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b8f78",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "\n",
    "Sometimes, heteroscedasticity can be \"removed\" by transforming the y variables. For instance, we can take the\n",
    "square root of the y-values (note here that the y-values are shifted so that they are all positive). This\n",
    "is a so-called [variance-stabilizing transformation](https://en.wikipedia.org/wiki/Variance-stabilizing_transformation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1780573",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = np.sqrt(y - y.min() + 1)\n",
    "model3 = LinearRegression(fit_intercept=True)\n",
    "model3.fit(X, y_new)\n",
    "_ = plot_results(X, y_new, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14338d",
   "metadata": {},
   "source": [
    "Another option is to log-transform the (x and) y values. Note also here that we shift the x- and y-values so\n",
    "that they are all greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new2 = np.log(y - y.min() + 1)\n",
    "x_new2 = np.log(x - x.min() + 1)\n",
    "X_new2 = x_new2.reshape(-1, 1)\n",
    "model4 = LinearRegression(fit_intercept=True)\n",
    "model4.fit(X_new2, y_new2)\n",
    "_ = plot_results(X_new2, y_new2, model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ccd0a",
   "metadata": {},
   "source": [
    "**If you are interested:** You can read more about what this transformation is doing to the data in this article: [Regression analysis of log-transformed data: Statistical bias and its correction](https://setac.onlinelibrary.wiley.com/doi/10.1002/etc.5620120618)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
