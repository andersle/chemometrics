{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e2b5f9",
   "metadata": {},
   "source": [
    "# Partial least squares regression \"by hand\"\n",
    "\n",
    "Here, we will find the latent variables in partial least squares (PLS) regression by hand. As an\n",
    "example, we will consider the solubility data again but in a reduced form:\n",
    "\n",
    "* We will only consider alcohols\n",
    "\n",
    "* We use only logP and the number of atoms in the molecule as our X\n",
    "\n",
    "* We use the solubility and the molecular weight as our Y\n",
    "\n",
    "From the regression we did on the complete data set, we know that logP is important for predicting the\n",
    "solubility and we expect the molecular weight to correlate with the number of atoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af7e74",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in the file [solubility_alc.csv](./solubility_alc.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d24b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import svd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61976b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"solubility_alc.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = [\"MolLogP\", \"nAtom\"]\n",
    "yvars = [\"solubility (mol/L)\", \"Molecular Weight\"]\n",
    "\n",
    "scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "\n",
    "Y = scaler_y.fit_transform(data[yvars].to_numpy())\n",
    "X = scaler_x.fit_transform(data[xvars].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a6675",
   "metadata": {},
   "source": [
    "### Plotting X- and Y-data\n",
    "\n",
    "Before we start, let us first plot the X- and Y-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1])\n",
    "axes[0].set(xlabel=xvars[0], ylabel=xvars[1])\n",
    "axes[0].set_title(\"X-data\", loc=\"left\")\n",
    "\n",
    "axes[1].scatter(Y[:, 0], Y[:, 1])\n",
    "axes[1].set(xlabel=yvars[0], ylabel=yvars[1])\n",
    "axes[1].set_title(\"Y-data\", loc=\"left\")\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf3fd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, nrows=2, figsize=(8, 8)\n",
    ")\n",
    "for i in (0, 1):\n",
    "    xi = X[:, i]\n",
    "    for j in (0, 1):\n",
    "        ax = axes[i, j]\n",
    "        yj = Y[:, j]\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(xi.reshape(-1, 1), yj)\n",
    "        y_hat = model.predict(xi.reshape(-1, 1))\n",
    "\n",
    "        ax.scatter(xi, yj)\n",
    "        ax.plot(\n",
    "            xi,\n",
    "            y_hat,\n",
    "            label=f\"RÂ² = {r2_score(yj, y_hat):.3f}\",\n",
    "            color=\"red\",\n",
    "            lw=3,\n",
    "        )\n",
    "        ax.set(xlabel=xvars[i], ylabel=yvars[j])\n",
    "        ax.set_title(f\"Predicing {yvars[j]} from {xvars[i]}\", loc=\"left\")\n",
    "        ax.legend()\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a688800",
   "metadata": {},
   "source": [
    "## The PLS method\n",
    "\n",
    "In PLS, we are looking for scores $\\mathbf{t}$ (for X) and $\\mathbf{u}$ (for y) so that the covariance\n",
    "is maximised. The main idea is that the scores should explain the variance in X and Y and the\n",
    "covariance between X and Y.\n",
    "\n",
    "The covariance can be calculated by $\\mathbf{t}^\\top \\mathbf{u}$ (the dot product between\n",
    "the scores). We shall see later how we can find the directions in X and Y to maximise the covariance, but let us assume that we have found them. In PLS, these directions\n",
    "are referred to as the *weights* $\\mathbf{w}_x$ (for X) and $\\mathbf{w}_y$ (for Y).\n",
    "We can use these *weights* to calculate the scores:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{t} &= \\mathbf{X} \\mathbf{w}_x, \\\\\n",
    "\\mathbf{u} &= \\mathbf{Y} \\mathbf{w}_y .\n",
    "\\end{align}\n",
    "\n",
    "Assume that we will predict $\\mathbf{Y}$ from some measured $\\mathbf{X}$. Then, the relations\n",
    "above do not seem too helpful since they do not directly relate $\\mathbf{Y}$ and $\\mathbf{X}$. So we need\n",
    "to connect the scores.\n",
    "\n",
    "Ideally, we would have a perfect correlation between the scores so\n",
    "that $\\mathbf{u} = g \\mathbf{t}$ where $g$ is some number.\n",
    "But in general, this is not the case. Instead, we will approximate $\\mathbf{u}$ from $\\mathbf{t}$ using\n",
    "this relation. The least squares approximation for $g$ is,\n",
    "\n",
    "\\begin{equation}\n",
    "g = \\frac{\\mathbf{t}^\\top \\mathbf{u}}{\\mathbf{t}^\\top \\mathbf{t}},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "so that the Y-scores approximated from the X-scores are $\\hat{\\mathbf{u}} = g \\mathbf{t}$. Note the following:\n",
    "\n",
    "- When we train the model, we supply both $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "  These are used to find $\\mathbf{w}_x$ and $\\mathbf{w}_y$ and the scores, which\n",
    "  are used to find $g$, connecting the scores.\n",
    "- We supply only $\\mathbf{X}$ when we use the model. We can then calculate the X-scores using $\\mathbf{w}_x$. We then calculate the Y-scores (via $g$ calculated in training) from these new X-scores.\n",
    "\n",
    "\n",
    "So we now have a strategy for estimating the Y-scores $\\hat{\\mathbf{u}}$ from the X-scores. We now only\n",
    "need a way of calculating Y from the estimated Y-scores. If you remember principal component analysis (PCA),\n",
    "we used the *loadings* for this. In PCA we write $\\mathbf{T} = \\mathbf{X} \\mathbf{P}$ where $\\mathbf{T}$ are\n",
    "the scores and $\\mathbf{P}$ are the loadings. Also, in PCA, we could invert this relation to\n",
    "$\\mathbf{X} = \\mathbf{T} \\mathbf{P}^\\top$. In PLS, this is no longer valid for the weights, and PLS introduces\n",
    "separate *loadings* to do this,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\hat{\\mathbf{u}} \\mathbf{q}^T .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The equation above tells us that if we know $\\mathbf{q}$, then we can estimate $\\mathbf{Y}$ from the\n",
    "(estimated) Y-scores.\n",
    "If we left-multiply this equation with $\\hat{\\mathbf{u}}^\\top$ we get,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{q}^T = \\frac{\\hat{\\mathbf{u}}^\\top \\mathbf{Y}}{\\hat{\\mathbf{u}}^\\top \\hat{\\mathbf{u}}} \\implies  \n",
    " \\mathbf{q} = \\frac{\\mathbf{Y}^\\top \\hat{\\mathbf{u}}}{\\hat{\\mathbf{u}}^\\top \\hat{\\mathbf{u}}} .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the estimated Y-scores ($\\hat{\\mathbf{u}} = g \\mathbf{t}$) we get,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{q} = \\frac{\\mathbf{Y}^\\top \\hat{\\mathbf{u}}}{\\hat{\\mathbf{u}}^\\top \\hat{\\mathbf{u}}} =\n",
    " \\frac{g \\mathbf{Y}^\\top \\mathbf{t}}{g^2 \\mathbf{t}^\\top \\mathbf{t}} \\implies g  \\mathbf{q} =\n",
    " \\frac{\\mathbf{Y}^\\top \\mathbf{t}}{\\mathbf{t}^\\top \\mathbf{t}} .\n",
    "\\end{equation}\n",
    "\n",
    "We now have everything in place to predict $\\mathbf{Y}$ from $\\mathbf{X}$. If we use $\\mathbf{t} = \\mathbf{X} \\mathbf{w}_x$, then we can write,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\hat{\\mathbf{u}} \\mathbf{q}^T = g \\mathbf{t} \\mathbf{q}^\\top =\n",
    "\\mathbf{X} g \\mathbf{w}_x \\mathbf{q}^\\top .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Comparing this\n",
    "to a linear equation on the form $\\mathbf{Y} = \\mathbf{X} \\mathbf{B}_\\text{PLS}$ with regression\n",
    "coefficients $B_\\text{PLS}$ we see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{B}_\\text{PLS} = g \\mathbf{w}_x \\mathbf{q}^\\top .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "What we will do now is to create some $\\mathbf{w}_x$ and $\\mathbf{w}_y$ vectors by hand and check what\n",
    "the correlation is (by calculating the correlation and by plotting the scores $\\mathbf{t}$ vs. $\\mathbf{u}$).\n",
    "Both $\\mathbf{X}$ and $\\mathbf{Y}$ have\n",
    "dimensions $n \\times 2$ (we have $n$ samples and $2$ variables). So if $\\mathbf{w}_x$ and $\\mathbf{w}_y$\n",
    "have dimensions $2 \\times 1$ (that is, they are *column* vectors), then the scores will\n",
    "have a dimension of $(n \\times 2) \\times (2 \\times 1) = n \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81cd53",
   "metadata": {},
   "source": [
    "## A short example\n",
    "\n",
    "We set set $\\mathbf{w}_x = (0.0, 1.0)^\\top$ and $\\mathbf{w}_y = (0.0, 1.0)^\\top$. Then \n",
    "the product $\\mathbf{X} \\mathbf{w}_x$ will just pick out\n",
    "the second column of $\\mathbf{X}$ (just the number of atoms) and $\\mathbf{Y} \\mathbf{w}_y$ will\n",
    "pick out the second column of $\\mathbf{Y}$ (just the molecular weight). Then a plot of\n",
    "$\\mathbf{t}$ vs $\\mathbf{u}$ will show how the number of atoms is correlated with the\n",
    "molecular weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(X, Y, wx, wy):\n",
    "    \"\"\"Plot X, Y and t vs. u (calculated using wx and wy)\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        constrained_layout=True,\n",
    "        ncols=3,\n",
    "        figsize=(9, 3),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    axes[0].scatter(X[:, 0], X[:, 1])\n",
    "    axes[0].set(xlabel=xvars[0], ylabel=xvars[1], title=\"X\")\n",
    "\n",
    "    axes[1].scatter(Y[:, 0], Y[:, 1])\n",
    "    axes[1].set(xlabel=yvars[0], ylabel=yvars[1], title=\"Y\")\n",
    "\n",
    "    vecx = wx.flatten()\n",
    "    vecy = wy.flatten()\n",
    "\n",
    "    vecx /= norm(vecx)\n",
    "    vecy /= norm(vecy)\n",
    "\n",
    "    axes[0].quiver(\n",
    "        0,\n",
    "        0,\n",
    "        vecx[0],\n",
    "        vecx[1],\n",
    "        color=\"black\",\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.2,\n",
    "        width=0.015,\n",
    "    )\n",
    "\n",
    "    axes[1].quiver(\n",
    "        0,\n",
    "        0,\n",
    "        vecy[0],\n",
    "        vecy[1],\n",
    "        color=\"red\",\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.2,\n",
    "        width=0.015,\n",
    "    )\n",
    "\n",
    "    t = X @ (wx / norm(wx))\n",
    "    u = Y @ (wy / norm(wy))\n",
    "    # Technical detail: we norm both wx and wy here to get similar reults from different methods\n",
    "\n",
    "    cov = t.T @ u\n",
    "\n",
    "    axes[2].scatter(t[:, 0], u[:, 0])\n",
    "    axes[2].set(\n",
    "        xlabel=\"X-scores (t)\",\n",
    "        ylabel=\"Y-scores (u)\",\n",
    "        title=f\"Covariance: {cov[0][0]:.2f}\",\n",
    "    )\n",
    "    sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights by hand:\n",
    "wx = np.array([0.0, 1.0])\n",
    "wx = wx.reshape(2, -1)  # Make it a column vector\n",
    "\n",
    "wy = np.array([0.0, 1.0])\n",
    "wy = wy.reshape(2, -1)  # Make it a column vector\n",
    "\n",
    "print(f\"wx.T = {wx.T}, shape of w: {wx.shape}\")\n",
    "print(f\"wy.T = {wy.T}, shape of q: {wy.shape}\")\n",
    "make_plot(X, Y, wx, wy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78880d",
   "metadata": {},
   "source": [
    "In the plot above, the X-data is projected onto the black vector, giving the X-scores. Similarly, the Y-data is projected onto the red vector, giving the Y-scores.\n",
    "The rightmost plot shows the scores plotted against each other.\n",
    "\n",
    "Here, we can use the X-scores to predict the Y-scores. And we could make a good prediction! But we aim to predict the whole $\\mathbf{Y}$! If we convert the Y-scores to $\\mathbf{Y}$, we would probably\n",
    "predict the molecular weight quite well but fail at predicting the solubility. Let us check what the\n",
    "regression coefficients are in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_coefficients(X, Y, wx, wy):\n",
    "    t = X @ wx\n",
    "    u = Y @ wy\n",
    "    # Find g (approximate u from t):\n",
    "    g = t.T @ u / (t.T @ t)\n",
    "    # Find q:\n",
    "    q = (Y.T @ t / (t.T @ t)) / g\n",
    "    # Find b:\n",
    "    B = g * wx * q.T\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, Y, wx, wy)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68425e3c",
   "metadata": {},
   "source": [
    "The regression coefficients above show that the first row is just zero. Effectively this means that we\n",
    "are predicting *both* Y-variables using only the number of atoms. From one of the first figures in this\n",
    "notebook, we expect this to give us a $RÂ²$ around 0.48. Let us check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(\n",
    "    Y[:, 0], Y_hat[:, 0], label=f\"RÂ² = {r2_score(Y[:, 0], Y_hat[:, 0]):.3f}\"\n",
    ")\n",
    "ax.scatter(\n",
    "    Y[:, 1], Y_hat[:, 1], label=f\"RÂ² = {r2_score(Y[:, 1], Y_hat[:, 1]):.3f}\"\n",
    ")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1027fc",
   "metadata": {},
   "source": [
    "*Below, you will find the same code for creating $\\mathbf{w}_x$ and $\\mathbf{w}_y$ as above. Can you find\n",
    "two other vectors that give a larger correlation?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wx = np.array([0.0, 1.0])\n",
    "wx = wx.reshape(2, -1)  # Make it a column vector\n",
    "wx /= norm(wx)\n",
    "\n",
    "wy = np.array([0.0, 1.0])\n",
    "wy = wy.reshape(2, -1)  # Make it a column vector\n",
    "wy /= norm(wy)\n",
    "\n",
    "B = get_regression_coefficients(X, Y, wx, wy)\n",
    "print(B)\n",
    "make_plot(X, Y, wx, wy)\n",
    "Y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(\n",
    "    Y[:, 0], Y_hat[:, 0], label=f\"RÂ² = {r2_score(Y[:, 0], Y_hat[:, 0]):.3f}\"\n",
    ")\n",
    "ax.scatter(\n",
    "    Y[:, 1], Y_hat[:, 1], label=f\"RÂ² = {r2_score(Y[:, 1], Y_hat[:, 1]):.3f}\"\n",
    ")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc471b3",
   "metadata": {},
   "source": [
    "## Maximizing the covariance\n",
    "\n",
    "The covariance between $\\mathbf{t}$ and $\\mathbf{u}$ is\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{t}^\\top \\mathbf{u} = (\\mathbf{X} \\mathbf{w}_x)^\\top (\\mathbf{Y} \\mathbf{w}_y)\n",
    "= \\mathbf{w}_x^\\top \\mathbf{X}^\\top \\mathbf{Y} \\mathbf{w}_y\n",
    "\\end{equation}\n",
    "\n",
    "Maximizing this covariance turns out to be the\n",
    "same as finding the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "of $\\mathbf{X}^\\top \\mathbf{Y}$. Let us also try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5455b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, _, Vt = np.linalg.svd(X.T @ Y)  # Find the singular value decomposition.\n",
    "wx = U[:, 0].reshape(2, -1)\n",
    "wy = Vt[0, :].reshape(2, -1)\n",
    "make_plot(X, Y, -wx, -wy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = get_regression_coefficients(X, Y, wx, wy)\n",
    "print(B)\n",
    "Y_hat = X @ B\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(\n",
    "    Y[:, 0], Y_hat[:, 0], label=f\"RÂ² = {r2_score(Y[:, 0], Y_hat[:, 0]):.3f}\"\n",
    ")\n",
    "ax.scatter(\n",
    "    Y[:, 1], Y_hat[:, 1], label=f\"RÂ² = {r2_score(Y[:, 1], Y_hat[:, 1]):.3f}\"\n",
    ")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acdc76",
   "metadata": {},
   "source": [
    "## Comparing with `PLSRegression` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLSRegression(scale=False, n_components=1)\n",
    "model.fit(X, Y)\n",
    "make_plot(X, Y, model.x_weights_, model.y_weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B / model.coef_.T)\n",
    "Y_hat = model.predict(X)\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(\n",
    "    Y[:, 0], Y_hat[:, 0], label=f\"RÂ² = {r2_score(Y[:, 0], Y_hat[:, 0]):.3f}\"\n",
    ")\n",
    "ax.scatter(\n",
    "    Y[:, 1], Y_hat[:, 1], label=f\"RÂ² = {r2_score(Y[:, 1], Y_hat[:, 1]):.3f}\"\n",
    ")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64ef5a",
   "metadata": {},
   "source": [
    "## Finding the next latent variables\n",
    "\n",
    "So far, we have found one latent variable. The strategy to find the next PLS components is\n",
    "to essentially repeat the process, but we first \"subtract\" the latent variable we just found\n",
    "from $\\mathbf{X}^T \\mathbf{Y}$. This is referred to as deflation.\n",
    "In addition, we make sure that the next latent variable\n",
    "we find will have scores orthogonal to the previous one. A consequence of this is\n",
    "that loadings and scores we find in the next steps of the method do not\n",
    "directly operate on $\\mathbf{X}$\n",
    "and $\\mathbf{Y}$, but on the deflated versions of these matrices.\n",
    "\n",
    "It would be nice to have loadings that we could apply directly \n",
    "to $\\mathbf{X}$ and $\\mathbf{Y}$ (and not the deflated version).\n",
    "PLS methods will typically calculate these as well, and in `sklearn`, they\n",
    "are called *rotations*. We will use these later for the interpretation of correlations between\n",
    "variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ae4d1",
   "metadata": {},
   "source": [
    "For the curious: In the paper\n",
    "[SIMPLS: An alternative approach to partial least squares regression](https://doi.org/10.1016/0169-7439(93)85002-X)\n",
    "pseudocode for a PLS algorithm is given. This algorithm is different from the algorithm `sklearn` uses\n",
    "by default, and it shows how one can calculate the variance explained by the PLS components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
