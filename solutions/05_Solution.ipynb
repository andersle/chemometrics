{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 5\n",
    "\n",
    "\n",
    ">This exercise aims to give you more experience with\n",
    "least squares and the [statsmodels](https://www.statsmodels.org)\n",
    "library. In particular, you will check for outliers in a\n",
    "least squares model and perform least squares\n",
    "regression for a case where we have several variables (some\n",
    "of which may be irrelevant or correlated to other variables).\n",
    "The last point should give you some experience in judging if\n",
    "certain variables should be included or not in a least squares model.\n",
    ">\n",
    ">The last part of the exercise aims to give you experience\n",
    "with calculating effects from the results of a full fractional factorial\n",
    "experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "We will revisit [Forbes' data](https://doi.org/10.1017/S0080456800032075) from exercise 4 and investigate\n",
    "if we have any outliers. Below you will find some code that will help you along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1(a)\n",
    "\n",
    "Create a linear model that predicts the atmospheric pressure\n",
    "from the boiling point\n",
    "with `statsmodels`. Plot your model together with the raw data, and plot the residuals. Do you have\n",
    "any comments about the residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have statsmodels installed you can uncomment\n",
    "# and run this line to try to install it:\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_forbes = pd.read_csv(\"Data/forbes.csv\")\n",
    "data_forbes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a least squares model with statsmodels:\n",
    "x = data_forbes[\"Temperature (F)\"]\n",
    "y = data_forbes[\"Pressure (inches Hg)\"]\n",
    "\n",
    "X = sm.add_constant(x)  # Make a matrix with a column of ones and then x\n",
    "\n",
    "model = sm.OLS(y, X)  # Set up for OLS = Ordinary Least Squares\n",
    "results = model.fit()  # Find parameters!\n",
    "y_hat = results.predict(X)  # Use the model to predict y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a small summary to show you R², and the coefficients:\n",
    "print(results.summary(slim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in the [Appendix](#Appendix:-The-summary-results-from-statsmodels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].scatter(x, y, label=\"Raw data\")\n",
    "axes[0].set(xlabel=\"Temperature (F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "axes[0].plot(x, y_hat, label=f'Model: y = {results.params[\"const\"]:.3g} + {results.params[\"Temperature (F)\"]:.3g}x')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_hat, y-y_hat)\n",
    "axes[1].set(xlabel=\"ŷ\", ylabel=\"y-ŷ\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(a): Do you have any comments about the residuals?\n",
    "\n",
    "There seems to be some kind of trend in the residuals, and one of the points is far away from the others.\n",
    "It could be that this is an outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1(b)\n",
    "\n",
    "Calculate [influence/outlier measures with statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.OLSInfluence.html#statsmodels.stats.outliers_influence.OLSInfluence) and plot the following:\n",
    "\n",
    "* (i) the studentized residuals,\n",
    "* (II) the leverage ($h_{ii}$ from the $\\mathbf{H}$-matrix),\n",
    "* (iii) the Cook's distance, and\n",
    "* (iv) the [influence plot](https://www.statsmodels.org/dev/generated/statsmodels.graphics.regressionplots.influence_plot.html).\n",
    "\n",
    "Based on these plots: Do you see any potential outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The influence measures can be computed with:\n",
    "influence = results.get_influence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A table of results is:\n",
    "influence_table = influence.summary_frame()\n",
    "influence_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get several things from this table, to use for the plotting:\n",
    "\n",
    "# (i) studentized residuals:\n",
    "studentized_residual = influence_table[\"student_resid\"]\n",
    "\n",
    "# (ii) the leverage:\n",
    "hii = influence_table[\"hat_diag\"]\n",
    "\n",
    "# (iii) Cook's distance\n",
    "cooks_distance = influence_table[\"cooks_d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your plots (i)-(iii) here:\n",
    "fig, axes = plt.subplots(constrained_layout=True, ncols=3, figsize=(9,3))\n",
    "\n",
    "axes[0].scatter(y_hat, studentized_residual)\n",
    "axes[0].axhline(y=-2, ls=\":\", color=\"k\")\n",
    "axes[0].axhline(y=2, ls=\":\", color=\"k\")\n",
    "axes[0].set(xlabel=\"ŷ\", ylabel=\"Studentized residuals\")\n",
    "axes[0].set_title(\"Residuals\", loc=\"left\")\n",
    "\n",
    "xpos = np.arange(1, len(hii) + 1, 1)\n",
    "axes[1].bar(xpos, hii)\n",
    "axes[1].set(xlabel=\"Observation no.\", ylabel=\"$h_{ii}$\")\n",
    "axes[1].set_title(\"Influence\", loc=\"left\")\n",
    "\n",
    "axes[2].bar(xpos, cooks_distance)\n",
    "axes[2].set(xlabel=\"Observation no.\", ylabel=\"$D_{i}$\")\n",
    "axes[2].set_title(\"Cook's distance\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The influence plot, part (iv) can be created with:\n",
    "fig = influence.plot_influence()\n",
    "ax = fig.get_axes()[0]\n",
    "ax.set_ylim(-1.3, 3.5)\n",
    "ax.set_title(\"\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(b): Do you see any potential outliers?\n",
    "\n",
    "Yes, one point is a bit suspicious - observation no. 11 (counting from zero) has a large studentized residual and a large influence (Cook's distance). This could be a potential outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1(c)\n",
    "\n",
    "Run a hypothesis test for outliers - use the [outlier test](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.outlier_test.html)\n",
    "method from `statsmodels`. This test outputs:\n",
    "* `student_resid`: The studentized residuals.\n",
    "* `unadj_p`: The unadjusted p-value for the hypothesis test that the expected value of the studentized residual for point *i* is zero, under the null hypothesis that the point is not an outlier.\n",
    "* `bonf(p)`: A Bonferroni corrected p-value, which adjusts for the increased risk of Type I errors (mistaken rejection of a true null hypothesis) due to multiple comparisons.\n",
    "\n",
    "The `outlier_test` method tests the null hypothesis that each point is not an outlier by considering if its studentized residual is significantly different from zero. Since we perform this test *N* times for *N* points, the risk of incorrectly labelling at least one point as an outlier (Type I error) increases. To mitigate this, `outlier_test` will apply a [correction](https://en.wikipedia.org/wiki/Bonferroni_correction), and we should base our decisions on these corrected p-values.\n",
    "\n",
    "Are any of the points classified as outliers? If yes, remake the model without these points and\n",
    "compare it with the model you made in part [5.1(a)](#5.1(a)). Did removing the point(s) change the\n",
    "model substantially?\n",
    "\n",
    "**Hint:** You can mark a point as an outlier if the corrected p-value is smaller than the significance level, set to `alpha = 0.05` by default in the `outlier_test()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you run the hypothesis test:\n",
    "test = results.outlier_test()\n",
    "test[test[\"bonf(p)\"] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the suspicious point:\n",
    "x_new = x.drop(11)\n",
    "y_new = y.drop(11)\n",
    "X_new = sm.add_constant(x_new)\n",
    "\n",
    "\n",
    "new_model = sm.OLS(y_new, X_new).fit()\n",
    "\n",
    "y_hat_new = new_model.predict(X_new)\n",
    "\n",
    "print(new_model.summary(slim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].scatter(x, y, label=\"Raw data\")\n",
    "axes[0].set(xlabel=\"Temperature (F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "axes[0].plot(x, y_hat, label=f'Model 1: y = {results.params[\"const\"]:.3g} + {results.params[\"Temperature (F)\"]:.3g}x')\n",
    "axes[0].plot(x_new, y_hat_new, label=f'Model 2: y = {new_model.params[\"const\"]:.3g} + {new_model.params[\"Temperature (F)\"]:.3g}x')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_hat, y-y_hat, label=\"Model 1\")\n",
    "axes[1].scatter(y_hat_new, y_new-y_hat_new, label=\"Model 2\")\n",
    "axes[1].set(xlabel=\"ŷ\", ylabel=\"y-ŷ\")\n",
    "axes[1].legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.1(c):\n",
    "\n",
    "Removing the point does not seem to change the model a lot. The calculated Cook's distance is not a lot\n",
    "bigger for this point, compared to some of the other points. So in this case, the outlier was not so\n",
    "important for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2\n",
    "\n",
    "The file [Data/bloodpress.csv](Data/bloodpress.csv) contains data for 20 individuals with high blood pressure.\n",
    "Table 1 describes the columns in this file.\n",
    "Your goal is to create a least squares model for predicting \n",
    "the blood pressure (from the other variables) that achieves $R^2 > 0.95$ with a maximum of two variables.\n",
    "\n",
    "\n",
    "| Column | Description              |             Unit |\n",
    "|:-------|:-------------------------|-----------------:|\n",
    "| BP     | Blood pressure           |             mmHg |\n",
    "| Age    | Age                      |            years |\n",
    "| Weight | Weight                   |               kg |\n",
    "| BSA    | Body surface area        |            m$^2$ |\n",
    "| DUR    | Duration of hypertension |            years |\n",
    "| BHR    | Basal heart rate         | beats per minute |\n",
    "| Stress | Stress index             |              --- |\n",
    "||**Table 1:** *Data columns present in the file [Data/bloodpress.csv](Data/bloodpress.csv)*|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2(a)\n",
    "\n",
    "Begin by exploring the data. Do this by creating\n",
    "the [scatter plot matrix](https://seaborn.pydata.org/examples/scatterplot_matrix.html)\n",
    "and the [correlation heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html).\n",
    "\n",
    "Based on your plots, do you find any promising correlations between blood pressure and the\n",
    "other variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/bloodpress.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example for creating the scatter plot matrix:\n",
    "grid = sns.pairplot(\n",
    "    data,\n",
    "    kind=\"reg\",  # Show a regression line\n",
    "    #y_vars=[\"BP\",],  # Uncomment to just use BP as y (this makes a smaller plot!)\n",
    "    #corner=True,  # Uncomment to just plot under the diagonal!\n",
    ")\n",
    "grid.fig.tight_layout()  # Clean up the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to create the correlation heat map:\n",
    "\n",
    "# Simple, with just pandas:\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heat map with seaborn (please experiment with colors etc):\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "sns.heatmap(\n",
    "    data.corr(),\n",
    "    cmap=\"vlag\",  # Select color scheme\n",
    "    annot=True,  # Annotate with numbers\n",
    "    ax=ax,  # Axis to plot in\n",
    "    #annot_kws={\"size\": \"small\"},  # Font size for numbers (in case they are too big/small)\n",
    "    linewidths=4,  # Just to separate the squares\n",
    "    square=True,\n",
    "    fmt=\".2f\"  # Formatting to the numbers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(a): Do you find any promising correlations between blood pressure and the other variables?\n",
    "\n",
    "Yes, we have strong correlations with Weight, BSA, and BHR. We also have some (weaker) positive correlation with the Age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2(b)\n",
    "\n",
    "Create a linear model in which you predict the blood pressure ($y$) from\n",
    "all six available variables (Age, Weight, BSA, DUR, BHR, and Stress).\n",
    "Use the `statsmodels` package\n",
    "and the ordinary least squares (OLS) method\n",
    "to create the model. Scale the variables before making the model (if you think\n",
    "this is a good idea!).\n",
    "\n",
    "Print out the summary created by `statsmodels` (please see [Appendix](#Appendix:-The-summary-results-from-statsmodels) for a short overview of the summary output from `statsmodels`).\n",
    "What values do you get for $R^2$ and $R^2_\\text{adjusted}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale  # If you want to scale the data\n",
    "\n",
    "# Here is an example to get you started:\n",
    "y = data[\"BP\"]\n",
    "variables = [i for i in data.columns if i != \"BP\"]\n",
    "X = data[variables]  \n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# And another example if you want to scale the variables:\n",
    "y = data[\"BP\"]\n",
    "X = data[variables] \n",
    "y = scale(y)\n",
    "X = scale(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(b): What values do you get for $R^2$ and $R^2_\\text{adjusted}$?\n",
    "\n",
    "Here, it can be beneficial to scale the variables since we have many different units. This will make it\n",
    "easier to compare regression coefficients.\n",
    "\n",
    "$R^2 = 0.996$ and $R^2_{\\text{adj}} = 0.995$ here (really high values!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2(c)\n",
    "Inspect the $p$-values for the different variables in the summary you just printed.\n",
    "Use this to create a new model with fewer variables, recalculate $R^2$\n",
    "and  $R^2_\\text{adjusted}$, and compare with the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"BP\"]\n",
    "X = data[[\"Age\", \"Weight\", \"BSA\"]]\n",
    "y = scale(y)\n",
    "X = scale(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(c): (Any comments for your comparison with the previous model?)\n",
    "\n",
    "I pick Age, Weight and BSA here since these have the smallest $p$-values (and they are all smaller than 0.05).\n",
    "For this new model: $R^2 = 0.995$ (it dropped slightly) and $R^2_{\\text{adj}} = 0.994$ here (also slightly smaller, but still very high!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2(d)\n",
    "Simplify the model to two variables and recalculate\n",
    "$R^2$ and $R^2_\\text{adjusted}$. What variables are you using now,\n",
    "and do you still get a $R^2 > 0.95$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us just try some different combinations:\n",
    "from itertools import combinations\n",
    "variables = [\"Age\", \"Weight\", \"BSA\"]\n",
    "\n",
    "table = {\"variables\": [], \"R²\": [], \"R²(adj)\": []}\n",
    "models = []\n",
    "\n",
    "y = data[\"BP\"]\n",
    "y = scale(y)\n",
    "\n",
    "for vari in variables:  # Try just one variable:\n",
    "    X = data[vari]\n",
    "    X = scale(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    models.append(model)\n",
    "    table[\"variables\"].append(vari)\n",
    "    table[\"R²\"].append(model.rsquared)\n",
    "    table[\"R²(adj)\"].append(model.rsquared_adj)  \n",
    "\n",
    "for comb in combinations(variables, 2):  # Try all combinations of two:\n",
    "    X = data[[comb[0], comb[1]]]\n",
    "    X = scale(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    models.append(model)\n",
    "    table[\"variables\"].append(comb)\n",
    "    table[\"R²\"].append(model.rsquared)\n",
    "    table[\"R²(adj)\"].append(model.rsquared_adj)\n",
    "    \n",
    "table = pd.DataFrame(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.2(d): What variables are you using now?\n",
    "\n",
    "Yes, with Age and Weight (see the table above) we still get a high $R² > 0.95$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.3\n",
    "\n",
    "The growth rate of a particular bacterium species depends\n",
    "on the concentration of nutrients such as phosphate ($P$),\n",
    "sucrose ($S$), and nitrate ($N$). \n",
    "Table 2 displays the experimental design\n",
    "used to investigate how these three concentrations\n",
    "influence the growth rate.\n",
    "\n",
    "\n",
    "|$P$  | $S$ | $N$ | $PS$ | $PN$ | $SN$ | $PSN$ | **Growth rate**  |\n",
    "|:---:|:---:|:---:|:----:|:----:|:----:|:-----:|:----------------:|\n",
    "| $+$ | $-$ | $-$ | $-$  | $-$  | $+$  | $+$   | $7$              |  \n",
    "| $-$ | $+$ | $-$ | $-$  | $+$  | $-$  | $+$   | $10$             | \n",
    "| $+$ | $-$ | $+$ | $-$  | $+$  | $-$  | $-$   | $8$              | \n",
    "| $-$ | $+$ | $+$ | $-$  | $-$  | $+$  | $-$   | $11$             |  \n",
    "| $-$ | $-$ | $-$ | $+$  | $+$  | $+$  | $-$   | $11$             |\n",
    "| $+$ | $+$ | $+$ | $+$  | $+$  | $+$  | $+$   | $12$             |\n",
    "| $+$ | $+$ | $-$ | $+$  | $-$  | $-$  | $-$   | $7$              |\n",
    "| $-$ | $-$ | $+$ | $+$  | $-$  | $-$  | $+$   | $7$              | \n",
    "\n",
    "**Table 2:** *Experimental design matrix for the growth rate of the investigated bacteria. The factors are the concentration of phosphate ($P$), the concentration of sucrose ($S$), and the concentration of nitrate ($N$).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3(a)\n",
    "Compute the main effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([7, 10, 8, 11, 11, 12, 7, 7])\n",
    "P = np.array([1, -1, +1, -1, -1, 1, 1, -1])\n",
    "S = np.array([-1, 1, -1, 1, -1, 1, 1, -1])\n",
    "N = np.array([-1, -1, 1, 1, -1, 1, -1, 1])\n",
    "\n",
    "effect_P = np.dot(P, y) / 4\n",
    "effect_S = np.dot(S, y) / 4\n",
    "effect_N = np.dot(N, y) / 4\n",
    "\n",
    "print(f\"Effect(P): {effect_P}\")\n",
    "print(f\"Effect(S): {effect_S}\")\n",
    "print(f\"Effect(N): {effect_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(a):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3(b)\n",
    "Verify that the columns for the 2-factor and 3-factor interaction effects are correct in table 2 and compute the interaction effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply for the new columns:\n",
    "PS = P * S\n",
    "PN = P * N\n",
    "SN = S * N\n",
    "PSN = P * S * N\n",
    "\n",
    "# Let us try to print this out:\n",
    "design = np.column_stack((P, S, N, PS, PN, SN, PSN, y))\n",
    "design = pd.DataFrame(design, columns=[\"P\", \"S\", \"N\", \"PS\", \"PN\", \"SN\", \"PSN\", \"Growth rate\"])\n",
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the remaining effects:\n",
    "effect_PS = np.dot(PS, y) / 4\n",
    "effect_PN = np.dot(PN, y) / 4\n",
    "effect_SN = np.dot(SN, y) / 4\n",
    "effect_PSN = np.dot(PSN, y) / 4\n",
    "\n",
    "print(f\"Effect(PS): {effect_PS}\")\n",
    "print(f\"Effect(PN): {effect_PN}\")\n",
    "print(f\"Effect(SN): {effect_SN}\")\n",
    "print(f\"Effect(PSN): {effect_PSN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(b):\n",
    "\n",
    "See the table and calculated effects above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3(c)\n",
    "What factors and interactions seem\n",
    "to increase the growth rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 5.3(c):\n",
    "\n",
    "The growth rate is increased by the factors S and N, but also the\n",
    "interaction of these with P: PS and PN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: The summary results from `statsmodels`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary method in `statsmodels` prints out a lot of information.\n",
    "We have fitted a model $y=a + bx$ to 10 $(x, y)$ points with `statsmodels`\n",
    "and the resulting summary output is printed below. This output is described in the\n",
    "sections below. The most important one for this exercise is the [Information-about-the-coefficients](#Information-about-the-coefficients).\n",
    "\n",
    "```code\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                      y   R-squared:                       0.956\n",
    "Model:                            OLS   Adj. R-squared:                  0.951\n",
    "Method:                 Least Squares   F-statistic:                     175.6\n",
    "Date:                Tue, 14 Feb 2023   Prob (F-statistic):           1.00e-06\n",
    "Time:                        08:42:06   Log-Likelihood:                -16.957\n",
    "No. Observations:                  10   AIC:                             37.91\n",
    "Df Residuals:                       8   BIC:                             38.52\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          4.4248      0.931      4.754      0.001       2.278       6.571\n",
    "x1             1.9235      0.145     13.253      0.000       1.589       2.258\n",
    "==============================================================================\n",
    "Omnibus:                        3.674   Durbin-Watson:                   2.067\n",
    "Prob(Omnibus):                  0.159   Jarque-Bera (JB):                0.755\n",
    "Skew:                           0.464   Prob(JB):                        0.686\n",
    "Kurtosis:                       3.975   Cond. No.                         13.0\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the model\n",
    "\n",
    "\n",
    "- **Dep. Variable:** The dependent variable (the variable we are predicting, $y$) in the model.\n",
    "- **Model:** The type of model we have created (OLS = Ordinary Least Squares).\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date & Time:** The date and time for when we created the model.\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 ($x$,$y$) values here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the calculation\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares). \n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of variables. In our case: $n - k - 1 = 10 - 1 - 1 = 8$. If we did the\n",
    "  fitting without the constant term (for instance, by centering the data first), this\n",
    "  number would be $n-k = 10-1=9$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** Calculations of standard errors assume homoscedastic errors.\n",
    "  If this is not the case, then the standard error is not computed correctly. There\n",
    "  are alternative ways of calculating the standard error; this field tells you\n",
    "  if statsmodels used a more robust method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the overall quality\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "- **Adj. R-squared:** The adjusted $R^2$ for the model. Useful from comparing\n",
    "  models as this one will only increase (when adding more variables) if the\n",
    "  increase in $R^2$ is more than one would expect by chance.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero! Effectively, this compares the model we\n",
    "  have just made to an alternative model equal to the constant intercept term. \n",
    "  To use this value, we would have to decide on a $\\alpha$ level and look up a critical F-value.\n",
    "  This is some extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at\n",
    "  least as extreme as the one above if all regression coefficients are zero. \n",
    "  It is also known as the $p$-value.\n",
    "  If we have selected $\\alpha$ value, we will reject the null hypothesis if \n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis: We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error.\n",
    "  This is equivalent (if the errors are normally distributed)\n",
    "  to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** The\n",
    "  [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n",
    "  [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "  These can be directly calculated from the Log-Likelihood and are useful for comparing alternative\n",
    "  models. Generally, we prefer models with lower AIC and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the coefficients\n",
    "\n",
    "- **coef:** The determined coefficients for the model.\n",
    "\n",
    "- **std err:** The standard of the coefficients. This\n",
    "  is calculated from,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  s^2 = \\frac{SSE}{n - k - 1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "\n",
    "- **t, P>|t|, and [0.025 0.975]:** Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the $p$-value for such a $t$-test.\n",
    "  Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, these coefficients are indeed different from\n",
    "  zero.\n",
    "  Finally, the **[0.025 0.975]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not specify \n",
    "  $\\alpha$ here, but we can give it as a parameter. The default is $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about residuals\n",
    "- **Omnibus and Prob(Omnibus):** This is a\n",
    "  [statistical test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html)\n",
    "  that checks if\n",
    "  the residuals are normally distributed. The probability indicates the\n",
    "  probability of the residuals being normally distributed.\n",
    "- **Skew:** This is a measure of the asymmetry of the residuals. For a normal distribution, the skewness is 0.\n",
    "- **Kurtosis:** This measures the \"tailedness\" of the residuals. For a normal distribution, the skewness is 3.\n",
    "- **Jarque-Bera (JB) and Prob(JB):** This is a statistical test that checks the\n",
    "  same thing as the **Omnibus** (but the test itself is different). Ideally, it should\n",
    "  agree with the **Omnibus** test.\n",
    "- **Durbin-Watson:** This is a statistical test that essentially checks if there\n",
    "  is some correlation (relationship) in the residuals. The value is between 0 and 4.\n",
    "  If this is equal to 2, then there is no correlation. Values close to 0 indicate\n",
    "  a positive serial correlation. Values close to 4 indicate \n",
    "  a negative serial correlation.\n",
    "- **Cond. No.:** The [condition number](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)\n",
    "  measures the sensitivity\n",
    "  of the solution (our parameters) to small perturbations in the input data. With just\n",
    "  one variable, this value is not so important. Statsmodels will print a warning if\n",
    "  the condition number is larger than 1000; statsmodels interpret this as an indication\n",
    "  of multicollinearity or numerical problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
