{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 8\n",
    "\n",
    ">The goal of this exercise is to perform **principal component analysis**\n",
    ">and **clustering** on a data set with many variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.1\n",
    "\n",
    "This exercise will explore the [wine data set](https://archive.ics.uci.edu/ml/datasets/Wine), a data set commonly used as an example for classification.\n",
    "The data set contains the results of\n",
    "a chemical analysis of wines from a region in Italy. These\n",
    "wines are made using grapes grown by three different cultivators.\n",
    "In this first exercise, we will explore the\n",
    "data set using principal component analysis and investigate\n",
    "if the results from the chemical analysis can be used to separate\n",
    "the wines into groups that correspond to the cultivator of the grapes.\n",
    "\n",
    "The data set contains the following columns:\n",
    "\n",
    "\n",
    "| Column name                    | Description                                              |\n",
    "|--------------------------------|----------------------------------------------------------|\n",
    "| alcohol                        | The alcohol content of the wine.                         | \n",
    "| malic_acid                     | The amount of malic acid in the wine (malic acid has an apple aroma).  |\n",
    "| ash                            | The amount of ash in the wine (ash is the matter that remains after evaporation and incineration).   | \n",
    "| alcalinity_of_ash              | The alkalinity of the ash content of the wine.           |\n",
    "| magnesium                      | The amount of magnesium in the wine.                      |\n",
    "| total_phenols                  | The total amount of [phenols](https://en.wikipedia.org/wiki/Phenolic_content_in_wine) (that are not flavanoids) in the wine. |\n",
    "| flavanoids                     | The amount of [flavanoids](https://en.wikipedia.org/wiki/Flavonoid) in the wine |\n",
    "| nonflavanoid_phenols           | The total amount of phenols in the wine.   |\n",
    "| proanthocyanins                | The amount of [proanthocyanins](https://en.wikipedia.org/wiki/Proanthocyanidin) in the wine (important for red/blue/purple colors).   |\n",
    "| color_intensity                | Color intensity of the wine (measured spectroscopically).  |\n",
    "| hue                            | Color hue of the wine (measured spectroscopically).         |\n",
    "| od280/od315_of_diluted_wines   | The protein content of the wine. OD280/OD315 is a method for determining protein concentration.                                     |\n",
    "| proline                        | The amount of [proline](https://en.wikipedia.org/wiki/Proline) in the wine (proline is the main amino acid found in red wine).   |  \n",
    "| target                         | The cultivator of the wine, given as 0, 1, or 2.   |\n",
    "\n",
    "The data can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data set as a pandas frame:\n",
    "data_set = load_wine(as_frame=True)[\"frame\"]\n",
    "data_set.head()\n",
    "\n",
    "\n",
    "variables = [i for i in data_set.columns if i != \"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1(a)\n",
    "Begin by exploring the raw data. Here, you should choose\n",
    "the method yourself. You can, for instance, look at histograms of the\n",
    "different measured quantities, correlations between the quantities,\n",
    "or other plots of the raw data (for instance, the \n",
    "[scatter plot matrix](https://seaborn.pydata.org/examples/scatterplot_matrix.html) we used in a previous exercise). After looking at the raw data, are there some of the\n",
    "variables that seem to be able to distinguish\n",
    "between the wines produced by the different cultivators?\n",
    "\n",
    "To make things a bit more interesting (and to show you how to make things slightly more interactive in a\n",
    "Jupyter notebook); here are two examples that create a dropdown selector for picking variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown, interact\n",
    "\n",
    "# This code shows the distributions for the three targets for one variable:\n",
    "\n",
    "\n",
    "def show_data(variable):\n",
    "    fig1, (ax1, ax2) = plt.subplots(\n",
    "        constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    "    )\n",
    "    sns.boxplot(data=data_set, y=variable, x=\"target\", ax=ax1)\n",
    "    sns.kdeplot(\n",
    "        data=data_set,\n",
    "        x=variable,\n",
    "        hue=\"target\",\n",
    "        fill=True,\n",
    "        palette=\"muted\",\n",
    "        ax=ax2,\n",
    "    )\n",
    "\n",
    "\n",
    "dropdown = Dropdown(options=variables, description=\"Variable:\")\n",
    "interact(show_data, variable=dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 2D plot to show the distribution with two variables:\n",
    "\n",
    "\n",
    "def show_data2(variable_x, variable_y):\n",
    "    grid = sns.jointplot(\n",
    "        data=data_set,\n",
    "        x=variable_x,\n",
    "        y=variable_y,\n",
    "        hue=\"target\",\n",
    "        palette=\"muted\",\n",
    "    )\n",
    "\n",
    "\n",
    "dropdown1 = Dropdown(options=variables, description=\"Variable X:\")\n",
    "dropdown2 = Dropdown(options=variables, description=\"Variable Y:\")\n",
    "interact(show_data2, variable_x=dropdown1, variable_y=dropdown2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the exploring, we first try to make the scatter plot matrix:\n",
    "grid = sns.pairplot(\n",
    "    data_set,\n",
    "    corner=True,\n",
    "    hue=\"target\",\n",
    "    palette=\"muted\",\n",
    ")\n",
    "grid.fig.tight_layout()\n",
    "sns.move_legend(grid, \"upper center\", fontsize=\"xx-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then a heatmap of correlations:\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(12, 12))\n",
    "data2 = pd.get_dummies(data_set, columns=[\"target\"])\n",
    "sns.heatmap(\n",
    "    data2.corr(),\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    "    fmt=\".2g\",\n",
    "    annot_kws={\"size\": \"x-small\"},\n",
    "    cbar_kws={\"shrink\": 0.75},\n",
    "    linewidths=4,\n",
    "    square=True,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cmap=\"vlag\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also try out some sklearn methods to pick the, say 4 best variables for\n",
    "# distinguishing between the cultivators:\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = data_set[variables]\n",
    "y = data_set[\"target\"]\n",
    "\n",
    "select = SelectKBest(f_classif, k=4)  # Ask sklearn to do feature selection\n",
    "X_select = select.fit_transform(X, y)\n",
    "print(select.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.1(a): Did you find some variables that seem to distinguish between cultivators?\n",
    "\n",
    "Yes, from the scatter plot matrix, we see for instance the pairs:\n",
    "\n",
    "* alcohol & flavanoids\n",
    "* proline & hue\n",
    "* proline & od280/od315_of_diluted_wines\n",
    "\n",
    "The heatmap of correlations is more difficult to read here, but we see the same information\n",
    "as in the scatter plot matrix (for instance, we see that proline is positively correlated\n",
    "with cultivator 0, and negatively with the two other cultivators).\n",
    "\n",
    "Finally, the \"automatic\" selection of the four most promising variables highlights the same\n",
    "variables as in the two figures. Let us just visualize some pairs of the most promising variables (so that we can compare this with the scores from PCA later): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.jointplot(\n",
    "    data=data_set,\n",
    "    x=\"alcohol\",\n",
    "    y=\"flavanoids\",\n",
    "    hue=\"target\",\n",
    "    palette=\"muted\",\n",
    "    height=4,\n",
    ")\n",
    "grid = sns.jointplot(\n",
    "    data=data_set,\n",
    "    x=\"proline\",\n",
    "    y=\"hue\",\n",
    "    hue=\"target\",\n",
    "    palette=\"muted\",\n",
    "    height=4,\n",
    ")\n",
    "grid = sns.jointplot(\n",
    "    data=data_set,\n",
    "    x=\"proline\",\n",
    "    y=\"od280/od315_of_diluted_wines\",\n",
    "    hue=\"target\",\n",
    "    palette=\"muted\",\n",
    "    height=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1(b)\n",
    "Perform a PCA on the data set (see the example code \n",
    "for this below)\n",
    "and consider the following:\n",
    "\n",
    "* (i)  Do you need to scale your data before\n",
    "  performing PCA in this case (why/why not)?\n",
    "\n",
    "\n",
    "* (ii)  Should you include the `target` column in the data you use for the PCA?\n",
    "\n",
    "\n",
    "* (iii)  How many principal components are needed to explain 95 % of the\n",
    "  variance in the data? Answer this by plotting the explained variance\n",
    "  as a function of the number of principal components.\n",
    "\n",
    "\n",
    "Example code for PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and run PCA.\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data_set = load_wine(as_frame=True)[\"frame\"]\n",
    "variables = [i for i in data_set.columns if i != \"target\"]\n",
    "X = data_set[variables].to_numpy()\n",
    "\n",
    "# Uncomment the following line to scale your data:\n",
    "X = scale(X)\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X)\n",
    "\n",
    "# Print out the percentage of variance explained by each component:\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(constrained_layout=True)\n",
    "variance = [0] + list(np.cumsum(pca.explained_variance_ratio_))\n",
    "x = range(len(variance))\n",
    "ax1.plot(x, variance, marker=\"o\")\n",
    "ax1.set(\n",
    "    xlabel=\"Number of principal components\",\n",
    "    ylabel=\"Fraction of explained variance\",\n",
    ")\n",
    "ax1.set_xticks(x)\n",
    "ax1.axhline(y=0.95, ls=\":\", color=\"k\")\n",
    "sns.despine(ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.1(b):\n",
    "\n",
    "> (i)  Do you need to scale your data before\n",
    "  performing PCA in this case (why/why not)?\n",
    "\n",
    "Here, we should scale the variables.\n",
    "This ensures that we weight the different variables\n",
    "equally when we perform PCA. There is no information given on the units, but from\n",
    "the nature of the variables (they measure very different things) we expect\n",
    "that the units will not all be equal.\n",
    "\n",
    "> (ii)  Should you include the `target` column in the data you use for the PCA?\n",
    "\n",
    "In this case we do not include the `target` column.\n",
    "The reason for this is that we\n",
    "are going to investigate if we can discover this distinction, without supplying the\n",
    "information on the cultivars. This would be different if we were using a supervised\n",
    "method, where we aim to predict the â€œtargetâ€ column. We will later see classification\n",
    "methods, and for such methods, we would use the `target` information as the ð‘¦-values\n",
    "we aim to predict.\n",
    "\n",
    "> (iii)  How many principal components are needed to explain 95 % of the\n",
    "  variance in the data? Answer this by plotting the explained variance\n",
    "  as a function of the number of principal components.\n",
    "\n",
    "From the plot above: We need 10 components to account for 95 % of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1(c)\n",
    "\n",
    "* (i)  Rerun the PCA with\n",
    "  the number of components you found in the previous question. Select the number of components with the argument\n",
    "  `n_components` in `PCA()`, e.g. `pca = PCA(n_components=13)`,\n",
    "  or, (for 95 % of the variance) `pca = PCA(n_components=0.95)`\n",
    "\n",
    "\n",
    "* (ii)  Obtain the scores, and make a plot of the scores for\n",
    "  principal component 1 (on the $x$-axis) and principal component 2 (on the $y$-axis).\n",
    "\n",
    "\n",
    "* (iii)  Do you see any grouping(s) (\"clusters\") in your scores plot?\n",
    "  Here, you can choose to color the scores according\n",
    "  to the cultivator (i.e., by using the values in the `target`\n",
    "  column in the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plot for the scores:\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(scores[:, 0], scores[:, 1])  # Plot scores on first and second PC\n",
    "# Example for coloring:\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.scatterplot(x=scores[:, 0], y=scores[:, 1], hue=data_set[\"target\"], palette=\"muted\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i)\n",
    "pca = PCA(n_components=0.95)\n",
    "scores = pca.fit_transform(X)\n",
    "print(\"Number of components:\", pca.n_components_)\n",
    "\n",
    "# (ii)\n",
    "fig1, ax1 = plt.subplots(constrained_layout=True)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=scores[:, 0],\n",
    "    y=scores[:, 1],\n",
    "    hue=data_set[\"target\"],\n",
    "    palette=\"muted\",\n",
    "    ax=ax1,\n",
    ")\n",
    "var1 = pca.explained_variance_ratio_[0] * 100\n",
    "var2 = pca.explained_variance_ratio_[1] * 100\n",
    "ax1.set(\n",
    "    xlabel=f\"PC1 ({var1:.2g}%)\",\n",
    "    ylabel=f\"PC2 ({var2:.2g}%)\",\n",
    ")\n",
    "ax1.axhline(y=0, ls=\":\", lw=1, color=\"k\")\n",
    "ax1.axvline(x=0, ls=\":\", lw=1, color=\"k\")\n",
    "sns.despine(ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.1(c):\n",
    "\n",
    "> (iii)  Do you see any grouping(s) (\"clusters\") in your scores plot?\n",
    "  Here, you can choose to color the scores according\n",
    "  to the cultivator (i.e., by using the values in the `target`\n",
    "  column in the data set).\n",
    "  \n",
    "Yes, there is a grouping of the points that correspond to the cultivators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1(d)\n",
    "Explore the loadings for your PCA model by plotting the\n",
    "loadings for the variables (on principal component 1 and\n",
    "principal component 2). Are any of the variables correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loadings are stored as the transpose in pca.components_\n",
    "# The loadings for PC1 is:\n",
    "load1 = pca.components_[0, :]\n",
    "# The loadings for PC2 is:\n",
    "load2 = pca.components_[1, :]\n",
    "\n",
    "# Aternatively:\n",
    "# loadings = pca.components_.T\n",
    "# load1 = loadings[:, 0]\n",
    "# load2 = loadings[:, 1]\n",
    "\n",
    "# Example plot:\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y=0, ls=\":\", color=\"black\", lw=1)\n",
    "ax.axvline(x=0, ls=\":\", color=\"black\", lw=1)\n",
    "ax.set_xlim(-0.6, 0.6)\n",
    "ax.set_ylim(-0.6, 0.6)\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# Just plotting the points:\n",
    "ax.scatter(load1, load2)\n",
    "\n",
    "# Adding text (name of variables):\n",
    "for i, variablei in enumerate(variables):\n",
    "    ax.text(load1[i], load2[i], variablei, fontsize=\"small\")\n",
    "\n",
    "# Here, you can probably make the plot easier to read. Maybe it should be bigger,\n",
    "# more colorful, or maybe interactive like in the appedix in exercise 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will add some colors:\n",
    "colors = sns.color_palette(\"husl\", len(variables))\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8, 8))\n",
    "ax.axhline(y=0, ls=\":\", color=\"black\", lw=1)\n",
    "ax.axvline(x=0, ls=\":\", color=\"black\", lw=1)\n",
    "ax.set_xlim(-0.6, 0.6)\n",
    "ax.set_ylim(-0.6, 0.6)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set(\n",
    "    xlabel=f\"Loadings, PC1 ({var1:.2g}%)\",\n",
    "    ylabel=f\"Loadings, PC2 ({var2:.2g}%)\",\n",
    ")\n",
    "\n",
    "# Adding text (name of variables):\n",
    "for i, variablei in enumerate(variables):\n",
    "    x, y = load1[i], load2[i]\n",
    "    ha = \"right\" if x < 0 else \"left\"\n",
    "    ax.text(x, y, variablei, fontsize=\"small\", ha=ha, va=\"center\")\n",
    "    ax.annotate(\n",
    "        \"\",\n",
    "        xy=(x, y),\n",
    "        xytext=(0, 0),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"-|>\", lw=2, color=colors[i], mutation_scale=25\n",
    "        ),\n",
    "        zorder=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.1(d):\n",
    "\n",
    "From the plot of the loadings above, we see that `flavanoids`, `total_phenols`, and `proanthocyanins` are grouped and (positively) correlated. Both\n",
    "flavonoids and proanthocyanins are polyphenolic compounds so this is not so surprising.\n",
    "The `nonflavanoid_phenols` is negatively correlated (points in the opposite direction)\n",
    "with `flavanoids` (and `total_phenols`). This could indicate that wines with a higher\n",
    "phenol content will have more polyphenolic compounds.\n",
    "\n",
    "We also see that `alcalinity_of_ash` and `nonflavanoid_phenols` are positively correlated, and `malic_acid` and `hue` are negatively correlated. Here, I do not have a good chemical\n",
    "explanation - please let me know if this makes sense to you!\n",
    "\n",
    "\n",
    "We also note that the\n",
    "loadings are of similar size (so that the principal components can not be described by just\n",
    "a few of the original variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1(e)\n",
    "Save the scores for the first two principal components.\n",
    "We will use this information in the next part\n",
    "of the exercise, where we will try to find clusters in our data.\n",
    "Saving the scores can be\n",
    "done with `pandas` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the scores are in the matrix scores, you can\n",
    "# do the following to save the data (remember to limit to the first\n",
    "# two PCs):\n",
    "\n",
    "data_set = load_wine(as_frame=True)[\"frame\"]\n",
    "variables = [i for i in data_set.columns if i != \"target\"]\n",
    "X = scale(data_set[variables].to_numpy())\n",
    "pca = PCA(n_components=2)\n",
    "scores = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# 1. Create variable names for the principal components:\n",
    "pc_name = [f\"PC{i+1}\" for i in range(scores.shape[1])]\n",
    "# 2. Create a DataFrame from the scores:\n",
    "scores_data = pd.DataFrame(scores, columns=pc_name)\n",
    "scores_data[\"target\"] = data_set[\"target\"]\n",
    "# 3. Save the scores to a comma separated values-file:\n",
    "scores_data.to_csv(\"scores.csv\", index=False)\n",
    "\n",
    "# Note, here you could also save it into many other formats,\n",
    "# for instance, Excel:\n",
    "# scores_data.to_excel(\"scores.xlsx\", index=False)\n",
    "# or maybe as LaTeX for a report:\n",
    "# print(scores_data.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code below, the file should be available here: [scores.csv](./scores.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check that the file is present:\n",
    "my_data = pd.read_csv(\"scores.csv\")\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.2\n",
    "\n",
    "We will continue exploring the wine data set. We will pretend that we do not\n",
    "know that there are three cultivators in the data set, and we will investigate\n",
    "what the `KMeans` clustering method can tell us about it. For this\n",
    "exercise, it is a good idea to read through all points below before\n",
    "starting, since you will do the same analysis twice (first for the complete data set,\n",
    "and then for the PCA scores you saved in part [8.1(e)](#8.1(e)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2(a)\n",
    "Outline the steps in the `KMeans` clustering algorithm.\n",
    "How can we use this algorithm without knowing the number of clusters in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.2(a):\n",
    "\n",
    "The KMeans clustering starts with first defining the number of clusters we are going to find.\n",
    "Then we find initial positions for our centroids. This can, for instance, be done by giving the\n",
    "centroids random positions.\n",
    "\n",
    "After having set the initial positions of the clusters, we do the following:\n",
    "\n",
    "1. For each observation (data point) we assign it to the nearest centroid.\n",
    "\n",
    "\n",
    "2. For each centroid, we calculate a new center by taking the mean of the locations of\n",
    "   the observations assigned to it in the previous step.\n",
    "\n",
    "\n",
    "3. We update the locations for the centroids according to the mean found in the previous\n",
    "   step.\n",
    "\n",
    "\n",
    "4. We repeat the steps above until the locations of the cluster centers do not change.\n",
    "\n",
    "\n",
    "If we do not know how many clusters there are, we have to try different values. After\n",
    "trying different possible cluster numbers, we compare them using metrics such as the sum\n",
    "of squared distances of the samples to their closest cluster center, or the silhouette values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2(b)\n",
    "Run `KMeans` clustering on the wine data set (see the example code below).\n",
    "Here, you will have to\n",
    "select a set of numbers of clusters to look for (limit yourself to\n",
    "a maximum of 10 clusters).\n",
    "\n",
    "After running the clustering for your \n",
    "data, obtain and plot the following metrics:\n",
    "\n",
    "* (i) The sum of squared distances of the samples to\n",
    "  their closest cluster center as a function of the number of clusters considered.\n",
    "  \n",
    "  \n",
    "* (ii) The average silhouette value as a function of the number of clusters considered. (Note:\n",
    "  if you want to plot the distribution of silhouette values (not required here!), take\n",
    "  a look at this\n",
    "  [silhouette example](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html).)\n",
    "\n",
    "\n",
    "* (iii) The Gap statistic as a function of the number of clusters considered. (Skip this point if you are unable to install [gapstap](https://github.com/jmmaloney3/gapstat) - see the instructions below).\n",
    "\n",
    "Explain briefly (with a few lines of text) how you use these plots to identify the \"best\" number of clusters and use them to decide how many clusters there are in the wine data set.\n",
    "\n",
    "The cells below show Python code that runs the clustering and calculates the metrics to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the wine data set and run KMeans.\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data_set = load_wine(as_frame=True)[\"frame\"]\n",
    "variables = [i for i in data_set.columns if i != \"target\"]\n",
    "X_wine = scale(data_set[variables].to_numpy())\n",
    "# We scale the variance here (you have probably already\n",
    "# figured out this is a good idea during the PCA part in 8.1.)\n",
    "\n",
    "# Define a set of numbers of clusters to run KMeans for:\n",
    "number_of_clusters = range(1, 11)\n",
    "# Set up variables for storing the results\n",
    "results = []  # Results for the clustering\n",
    "\n",
    "for i in number_of_clusters:\n",
    "    # Set up the KMeans method with i cluster centers:\n",
    "    cluster_k = KMeans(n_clusters=i, n_init=\"auto\")\n",
    "    # Run the clustering method:\n",
    "    cluster_k.fit(X)\n",
    "    # Store the results:\n",
    "    results.append(cluster_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `cluster_k` object contains the following results as attributes:\n",
    " * `cluster_centers_`: Coordinates of cluster centers.\n",
    " * `labels_`: Labels of each sample. Each sample is assigned to a cluster, and the label shows which cluster a sample belongs to. Note that these are just\n",
    "    labels - the actual numbers (0, 1, ...) do not have any meaning except being a label.\n",
    " * `inertia_`: Sum of squared distances of samples to their closest cluster center.\n",
    " * `n_iter_`: Number of iterations run.\n",
    " \n",
    "The silhouette values can be calculated with [sklearn.metrics.silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), and\n",
    "the Gap statistic can be obtained via the [gapstat](https://github.com/jmmaloney3/gapstat) package. If you do not have this one installed, you can install it via:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/jmmaloney3/gapstat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install gapstat:\n",
    "# !pip install git+https://github.com/jmmaloney3/gapstat\n",
    "from gapstat import gapstat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering(X, k_max=10):\n",
    "    \"\"\"Run clustering for the data set in X.\"\"\"\n",
    "    number_of_clusters = range(1, k_max + 1)\n",
    "    clusters = []\n",
    "    for i in number_of_clusters:\n",
    "        cluster_k = KMeans(n_clusters=i, n_init=\"auto\")\n",
    "        cluster_k.fit(X)\n",
    "        clusters.append(cluster_k)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # For fun, add a progress bar!\n",
    "\n",
    "\n",
    "def calculate_sse(clusters):\n",
    "    return np.array([(i.n_clusters, i.inertia_) for i in tqdm(clusters)])\n",
    "\n",
    "\n",
    "def calculate_silhouette(clusters, X):\n",
    "    results = []\n",
    "    for i in tqdm(clusters):\n",
    "        if i.n_clusters < 2:\n",
    "            sil = float(\"nan\")\n",
    "        else:\n",
    "            sil = silhouette_score(X, i.labels_)\n",
    "        results.append([i.n_clusters, sil])\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def calculate_gap(clusters, X):\n",
    "    results = []\n",
    "    for i in tqdm(clusters):\n",
    "        gapi, _, _, _, err = gapstat_score(\n",
    "            X, i.labels_, k=i.n_clusters, calcStats=True\n",
    "        )\n",
    "        results.append([i.n_clusters, gapi, err])\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def plot_cluster_metrics(clusters, X):\n",
    "    print(\"Calculate SSE...\")\n",
    "    sse = calculate_sse(clusters)\n",
    "\n",
    "    print(\"Calculate silhouette...\")\n",
    "    silhouette = calculate_silhouette(clusters, X)\n",
    "\n",
    "    print(\"Calculate Gap\")\n",
    "    gap = calculate_gap(clusters, X)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        constrained_layout=True, ncols=3, figsize=(9, 3), sharex=True\n",
    "    )\n",
    "    axes[0].plot(sse[:, 0], sse[:, 1], marker=\"o\")\n",
    "    axes[0].set_xticks(sse[:, 0])\n",
    "    axes[0].set(ylabel=\"SSE\")\n",
    "\n",
    "    axes[1].plot(silhouette[:, 0], silhouette[:, 1], marker=\"o\")\n",
    "    axes[1].set(ylabel=\"Average silhouette\")\n",
    "\n",
    "    axes[2].errorbar(\n",
    "        gap[:, 0],\n",
    "        gap[:, 1],\n",
    "        yerr=gap[:, 2],\n",
    "        marker=\"o\",\n",
    "        capsize=4,\n",
    "        elinewidth=2,\n",
    "    )\n",
    "    axes[2].set(ylabel=\"Gap statistic\")\n",
    "\n",
    "    for axi in axes:\n",
    "        axi.set(xlabel=\"Number of clusters\")\n",
    "\n",
    "    sns.despine(fig=fig)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_wine = run_clustering(X_wine, k_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figcluster = plot_cluster_metrics(clusters_wine, X_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.2(b): What seems to be the best number of clusters to use?\n",
    "\n",
    "Hard to say! In the plot of the sum of squared distances, there is no\n",
    "clear â€œelbowâ€, however, we see that there is a\n",
    "large drop when going from 2 to 3 clusters, and smaller drops when increasing the number\n",
    "of clusters. We can take this as an indication of there being three clusters in the data\n",
    "set. (The drop from 3-4 is about the same as between 4-5 and so on, so if we do not stop\n",
    "at 3, then we should continue on to around 8).\n",
    "\n",
    "The silhouette plot shows a maximum at three clusters. This indicates that there are\n",
    "three clusters in the data.\n",
    "\n",
    "The Gap statistic levels off around three clusters, but it keeps on increasing. However,\n",
    "the change from 3 to 4 is small and it makes sense to stop at 3 clusters (it is just within\n",
    "the errorbar at 4 clusters).\n",
    "\n",
    "Based on these three plots together, there seems to be three clusters in the data set.\n",
    "The strongest point for this is made by the average silhouette plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2(c)\n",
    "The clustering you just have done used all the variables. Visualizing the clusters (and potential regions for the different types) in\n",
    "this 13-dimensional space is difficult! We will therefore use the scores from the principal\n",
    "component analysis where we just stored two components. This means\n",
    "that we now have a 2-dimensional problem!\n",
    "\n",
    "Rerun the cluster analysis for the scores (again, vary the number of clusters)\n",
    "and make the same plots as you made in [8.2(b)](#8.2(b)). What is the\n",
    "best number of clusters to use now? Are your results different from the\n",
    "cluster analysis on the full data set, and how does it compare to\n",
    "what we know - that the samples come from three different cultivators of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(\"scores.csv\")\n",
    "X_scores = scores[[\"PC1\", \"PC2\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_scores = run_clustering(X_scores, k_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figcluster2 = plot_cluster_metrics(clusters_scores, X_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer to question 8.2(c):\n",
    "\n",
    "In this case, the results are more clear and there is\n",
    "a pronounced peak in the silhouette values for three clusters. The Gap statistic is\n",
    "also largest for three clusters. The SSE is again less defined, but the two other agree\n",
    "on there being three clusters. \n",
    "\n",
    "The PCA component analysis resulted in new variables that are better\n",
    "suited for distinguishing between the cultivators.\n",
    "We conclude that there are three clusters in the data set, and this is what we know\n",
    "from before (there are three cultivators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2(d) Bonus: Showing the decision regions.\n",
    "Since we reduced the problem to two dimensions in [8.2(c)](#8.2(c)), we\n",
    "can plot the clusters. Here, we can also plot the so-called decision\n",
    "regions, which show the areas that belong to each cluster. Use the code below\n",
    "to show the decision regions for the best clustering you found in [8.2(c)](#8.2(c))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "cluster = KMeans(n_clusters=3, n_init=\"auto\").fit(X_scores)\n",
    "\n",
    "y = cluster.labels_  # Use the assigned labeles\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\")\n",
    "# Show the samples:\n",
    "colors = []\n",
    "for i in sorted(set(y)):\n",
    "    scat = ax.scatter(X_scores[y == i, 0], X_scores[y == i, 1], label=i)\n",
    "    colors.append(scat.get_facecolors())  # Store colors, so we can reuse them\n",
    "# Draw the boundaries:\n",
    "cmap = matplotlib.colors.ListedColormap(colors)  # Use same colors\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    cluster,\n",
    "    X_scores,\n",
    "    grid_resolution=200,\n",
    "    ax=ax,\n",
    "    cmap=cmap,\n",
    "    alpha=0.1,\n",
    ")\n",
    "ax.legend(title=\"Cluster no.\")\n",
    "ax.set_title(\"Decision regions\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
