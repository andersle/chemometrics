{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 4\n",
    "\n",
    "> This exercise aims to show you how to perform **least squares regression** \n",
    "> for real experimental data. In the first part, we will use data that\n",
    "> contains uncertainties, and we are going \n",
    "> to make use of this in the fitting and for estimating errors in\n",
    "> the fitted parameter.\n",
    "> In the second part, we will use testing/training to estimate\n",
    "> what kind of errors we can expect when using a model for estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "\n",
    "In this exercise we will use least-squares regression to investigate a physical phenomenon: the decay of\n",
    "beer froth with time. The file [Data/erdinger.csv](Data/erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights.\n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this work. In\n",
    "the [original study](https://doi.org/10.1088/0143-0807/23/1/304), Leike reported data\n",
    "for two more beers. The data for these two are in the\n",
    "files [Data/augustinerbrau.csv](Data/augustinerbrau.csv) and [Data/budweiser.csv](Data/budweiser.csv).\n",
    "If you have extra time, you can try to redo [4.1(d)](#4.1(d)) also for these two beers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(a)\n",
    "Create a linear model for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the coefficient of determination, $R^2$ , and plot\n",
    "the residuals. What do you think about your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Styling of plots\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"muted\")\n",
    "\n",
    "data = pd.read_csv(\"Data/erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = data[\"time\"].to_numpy()\n",
    "height = data[\"height\"].to_numpy()\n",
    "height_error = data[\"height-error\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, you are encouraged to try sklearn and its LinearRegression method:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make linear model:\n",
    "model1 = LinearRegression(fit_intercept=True)  # Create the model, and include the constant term.\n",
    "X = time.reshape(-1, 1)\n",
    "model1.fit(X, height)\n",
    "\n",
    "y_hat_1 = model1.predict(X)\n",
    "# To calculate R²:\n",
    "r2_model1 = model1.score(X, height)\n",
    "# And a mean squared error:\n",
    "mse_model1 = mean_squared_error(height, y_hat_1)\n",
    "# Summarize the model with some short text:\n",
    "model1_txt = f\"y = {model1.coef_[0]:.3g}x + {model1.intercept_:.3g}\"\n",
    "model1_txt = f\"{model1_txt}\\n(R² = {r2_model1:.3g}, MSE = {mse_model1:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is a hint for the plotting - use errorbar to display the errors in the raw data:\n",
    "fig, (ax1, ax2) = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4, # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    y_hat_1,\n",
    "    lw=3,\n",
    "    label=model1_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(y_hat_1, height - y_hat_1)\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Residual, y - ŷ\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(a): \"What do you think about your model?\"\n",
    "\n",
    "Well, the R² is quite hight and the model seems to be OK overall. But it is missing\n",
    "the fact that the change in the height is not constant. Further, we clearly see that\n",
    "the residuals are dependent on the predicted heights and this also points toward that something\n",
    "is missing in the model.\n",
    "\n",
    "The error is increasing towards the end in the left hand figure. And if we were to predict outside the\n",
    "range we used for making the model, we would probably make a large error.\n",
    "\n",
    "For a time greater than 480 s the model predicts a negative height.\n",
    "Of course, the height cannot be smaller than zero, so this can mean two things: (i) the model only apply\n",
    "between 0 and 360 s or (ii) the model is not physically sound and we should improve it. We will actually improve it (so option (ii)) in the next questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that we get\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter.\n",
    "We will assume that $h(0)$ is a known parameter equal to the height of the froth at the initial time.\n",
    "\n",
    "Show how you can transform the equation above to a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$.\n",
    "\n",
    "**Note:** The equation $y=bx$ does not include the usual constant term.\n",
    "This will modify the least squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept)\n",
    "You can use the equation from the appendix to calculate $b$ in the following or (recommended!)\n",
    "make use of methods where you can turn off the intercept, for instance\n",
    "[``LinearRegression(fit_intercept=False)``](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(b):\n",
    "\n",
    "If we take the natural logarithm on both sides of the equation, we get,\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln \\left( \\frac{h(t)}{h(0)} \\right) = -\\frac{t}{\\tau} = -\\frac{1}{\\tau} \\times t .\n",
    "\\end{equation}\n",
    "\n",
    "Setting,\n",
    "\\begin{equation}\n",
    "y = \\ln \\left( \\frac{h(t)}{h(0)} \\right), \\quad x = t, \\quad b=-\\frac{1}{\\tau},\n",
    "\\end{equation}\n",
    " we get,\n",
    "\\begin{equation}\n",
    "\\underbrace{\\ln \\left( \\frac{h(t)}{h(0)} \\right)}_{y} = -\\frac{t}{\\tau} = \\underbrace{-\\frac{1}{\\tau}}_{b} \\times \\underbrace{t}_{x},\n",
    "\\end{equation}\n",
    "\n",
    "or $y = bx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(c)\n",
    "Use the transformation you found above to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y:\n",
    "data[\"y\"] = np.log(height / height[0])\n",
    "data.head()\n",
    "\n",
    "X = time.reshape(-1, 1)  # This is the same as before\n",
    "y = data[\"y\"]  # This is the new y\n",
    "\n",
    "model2 = LinearRegression(fit_intercept=False)  # New model, without intercept\n",
    "model2.fit(X, y)\n",
    "r2_model2 = model2.score(X, y)\n",
    "y_hat_2 = model2.predict(X)\n",
    "# Convert y back into a height:\n",
    "height_hat_2 = height[0] * np.exp(y_hat_2)  # Height, estimated by model 2\n",
    "mse_model2 = mean_squared_error(height, height_hat_2)  # MSE, convert to height to compare with model 1\n",
    "\n",
    "\n",
    "tau = -1.0 / model2.coef_[0]\n",
    "print(f\"tau = {tau:.4g} s\")\n",
    "\n",
    "model2_txt = f\"h(t) = h(0) exp(-t/{tau:4g})\"\n",
    "model2_txt = f\"{model2_txt}\\n(R² = {r2_model2:.3g}, MSE = {mse_model2:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4, # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_2,\n",
    "    lw=3,\n",
    "    label=model2_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Let us plot these for heights so we can compare with model 1\n",
    "ax2.scatter(height_hat_2, height - height_hat_2)\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Residual, y - ŷ\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(c): What value did you get for $\\tau$?\n",
    "\n",
    "From the coefficient found in the least squares fit: $\\tau \\approx 290$. We see that the residuals are now all\n",
    "smaller i size, but we are overestimating the height for a lot of the points. We still seem to have a trend in the residuals, so maybe the model is not perfect, but R² is now really close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276$ s which is probably lower than the\n",
    "value you found in the previous task.\n",
    "We will now try to reproduce the results of Leike, but to\n",
    "do that, we have to do weighted least squares.\n",
    "\n",
    "As you have seen,\n",
    "the raw data includes errors that are not constant. We can use\n",
    "these errors to give weights to the data points in the fitting:\n",
    "we give more importance\n",
    "to points with smaller errors and less importance to points with larger errors.\n",
    "\n",
    "One way forward is to assign weights ($w_i$) as $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting to $y = \\log (h(t) / h(0))$, and this will also modify the errors.\n",
    "If you remember [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that $\\sigma_y^2 = \\sigma_h^2 / h^2$, and this is\n",
    "the transformation we need.\n",
    "\n",
    "Do the following steps to perform the weighted\n",
    "least squares:\n",
    "* (i) Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "* (ii) Calculate weights for your $y$ values as $w = 1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "* (iii) Run a weighted least squares fitting using your $w$'s as weights (see the Jupyter notebook version\n",
    "  for more hints), and find $\\tau$. Plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearRegression(fit_intercept=False)\n",
    "weights_h = 1.0 / height_error**2\n",
    "weights_h[weights_h == float(\"inf\")] = 0\n",
    "weights_h /= sum(weights_h)\n",
    "\n",
    "# i)\n",
    "sigma_y_sq = height_error**2 / height**2\n",
    "# ii)\n",
    "weights = 1.0 / sigma_y_sq\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "# iii)\n",
    "model3.fit(X, y, sample_weight=weights)  # Do fitting, but use the weights\n",
    "r2_model3 = model3.score(X, y, sample_weight=weights)  # Calculate R² (considering the weights).\n",
    "y_hat_3 = model3.predict(X)\n",
    "height_hat_3 = height[0] * np.exp(y_hat_3)\n",
    "mse_model3 = mean_squared_error(height, height_hat_3, sample_weight=weights_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_ = -1.0 / model3.coef_[0]\n",
    "print(f\"tau = {tau_:.4g} s\")\n",
    "\n",
    "model3_txt = f\"h(t) = h(0) exp(-t/{tau_:4g})\"\n",
    "model3_txt = f\"{model3_txt}\\n(R² = {r2_model3:.3g}, MSE = {mse_model3:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4, # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_3,\n",
    "    lw=3,\n",
    "    label=model3_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Let us plot these for heights so we can compare with model 1\n",
    "ax2.scatter(height_hat_3, np.sqrt(weights_h) * (height - height_hat_3))\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Weighted residual, w × (y - ŷ)\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(d): What value did you get for $\\tau$?\n",
    "\n",
    "With the weighted approach, we do get a $\\tau = 277$ s, which is close to the $276$ s stated in the text.\n",
    "The weighted residuals are closer to zero, but there might still be a weak trend in it. R² is still \n",
    "high and the model seems to capture the general trend quite fine.\n",
    "\n",
    "If we plot this new model and the previous\n",
    "one, we see that there is no big difference between them. It could be that the $\\tau=290$ and $\\tau=277$\n",
    "we have found are equal within the experimental uncertainty. Let us quantify the uncertainty next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1(e)\n",
    "Since we do have measured errors here, we can use them to estimate the error in the\n",
    "parameter you just found. For a weighted least squares fit to the equation $y = bx$,\n",
    "the error estimate ($\\sigma_b$) for $b$ is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_b^2 = \\frac{1}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "Estimate the error for the $\\tau$-value you just found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_b = np.sqrt(1.0 / np.sum(weights * time * time))\n",
    "print(sigma_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the uncertainty in the $b$ parameter. To find the uncertainty for $\\tau$, we can either\n",
    "calculate what $b$ can be with\n",
    "this uncertainty, or we can use propagation of errors:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_\\tau^2 = \\left(\\frac{\\partial \\tau}{\\partial b}\\right)^2 \\times \\sigma_b^2 =\\tau^4 \\times \\sigma_b^2 \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking:\n",
    "b = model3.coef_[0]\n",
    "tau_1 = -1.0 / (b + sigma_b)\n",
    "tau_2 = -1.0 / (b - sigma_b)\n",
    "# Let us take the average of the difference:\n",
    "sigma_tau = 0.5 * (abs(tau_ - tau_1) + abs(tau_ - tau_2))\n",
    "print(f\"σ(tau) = {sigma_tau:.4g}, round up to {np.ceil(sigma_tau):.1g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation of errors:\n",
    "sigma_tau =np.sqrt(sigma_b**2 * tau_**4)\n",
    "print(f\"σ(tau) = {sigma_tau:.4g}, round up to {np.ceil(sigma_tau):.1g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.1(e): What boundaries ($\\pm$) did you get for $\\tau$?\n",
    "\n",
    "The uncertainty is around 7 s, so $\\tau = 277 \\pm 7$ s. With the double standard deviation we\n",
    "get $\\tau = 277 \\pm 14$ s.\n",
    "\n",
    "This compares well with the results of Leike: $\\tau = 276 \\pm 7$ s at 68% confidence and $\\tau = 276 \\pm 14$ s\n",
    "at 95%\n",
    "confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "[Forbes](https://doi.org/10.1017/S0080456800032075) investigated the\n",
    "relationship between the boiling point of water and\n",
    "the atmospheric pressure, and collected data in the Alps and Scotland.\n",
    "Forbes' goal\n",
    "was to estimate altitudes from the boiling point alone. We will see if we can\n",
    "estimate the atmospheric pressure from Forbes' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2(a) \n",
    "Load the data from Forbes (data file [Data/forbes.csv](Data/forbes.csv)), plot it,\n",
    "and create a linear model\n",
    "that predicts the atmospheric pressure from the temperature. Report the R² and [mean\n",
    "squared error (MSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error  # Note: sklearn has a method for the MSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forbes = pd.read_csv(\"Data/forbes.csv\")\n",
    "data_forbes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = data_forbes[\"Temperature (F)\"].to_numpy()\n",
    "X_temp = temperature.reshape(-1, 1)\n",
    "pressure = data_forbes[\"Pressure (inches Hg)\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pressure = LinearRegression(fit_intercept=True)\n",
    "model_pressure.fit(X_temp, pressure)\n",
    "pressure_hat = model_pressure.predict(X_temp)\n",
    "\n",
    "r2_pressure = model_pressure.score(X_temp, pressure)\n",
    "# And a mean squared error:\n",
    "mse_pressure= mean_squared_error(pressure, pressure_hat)\n",
    "# Summarize the model with some short text:\n",
    "text = f\"y = {model_pressure.coef_[0]:.3g}x + {model_pressure.intercept_:.3g}\"\n",
    "text = f\"{text}\\n(R² = {r2_pressure:.3g}, MSE = {mse_pressure:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(temperature, pressure, label=\"Raw data\", color=\"0.3\")\n",
    "ax.set(xlabel=\"Temperature (°F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "ax.plot(temperature, pressure_hat, label=text, lw=3)\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.2(a): What R² did you get and what was the MSE?\n",
    "\n",
    "The R² is 0.994 and the mean squared error is 0.0478."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2(b) \n",
    "\n",
    "Estimate the error you can expect to make if you use your model for predicting the pressure.\n",
    "Do this by Leave-one-out cross-validation (LOOCV) and calculate the mean squared error\n",
    "of cross-validation ($\\text{MSE}_\\text{CV}$)\n",
    "\n",
    "LOOCV is a special case of **training** and **testing**, and you can find a short description of it\n",
    "in [appendix B](#B.-Leave-one-out-cross-validation). Please see the Jupyter notebook for a code example you can use. The code\n",
    "example for LOOCV is concise, so make sure you understand what goes on here (that is,\n",
    "what LOOCV is doing). If you are working with someone, try explaining testing/training\n",
    "and how LOOCV works to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 of LOOCV:\n",
    "# sklearn has a method to pick out samples for leave-one-out:\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "error = []\n",
    "for train_index, test_index in loo.split(X_temp):  # Split into training and testing\n",
    "    # train_index = index of samples to use for training\n",
    "    # test_index = index of samples to use for testing\n",
    "    # Pick out samples (for training and testing):\n",
    "    X_train, X_test = X_temp[train_index], X_temp[test_index]\n",
    "    y_train, y_test = pressure[train_index], pressure[test_index]\n",
    "    # Fit a new model with the training set:\n",
    "    model = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    # Predict y for the test set:\n",
    "    y_hat = model.predict(X_test)\n",
    "    # Compare the predicted y values in the test set with the measured ones:\n",
    "    error.append((y_test - y_hat) ** 2)\n",
    "mse_cv_1 = np.mean(error)\n",
    "print(f\"MSE_CV = {mse_cv_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 of LOOCV:\n",
    "# sklearn has a method for leave-one-out selection, and a method for\n",
    "# cross-validation. And these two can be combined:\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "# Create \"empty\" model for fitting:\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "# Run cross validation, where we select testing and training with LeaveOneOut:\n",
    "scores = cross_val_score(model, X_temp, pressure, scoring=\"neg_mean_squared_error\", cv=LeaveOneOut())\n",
    "mse_cv_2 = np.mean(-scores)\n",
    "print(f\"MSE_CV = {mse_cv_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The scoring is `\"neg_mean_squared_error\"` above, which is the negative of the mean squared error. This is maybe schematics, but many methods in sklearn return a \"score\", and for most of us, a better score = a better result. So if we used the mean squared error as the score, then a larger score = a larger error = a poorer result. However, with the negative sign, a larger score (closer to zero) = smaller error = better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, let us also code it to check the formula.\n",
    "# OBS! First a detail that is easy to miss!! the X used for H will include the\n",
    "# column of ones here.\n",
    "X_matrix = np.column_stack((np.ones_like(temperature), temperature))\n",
    "H = X_matrix @ np.linalg.pinv(X_matrix)\n",
    "hii = np.diagonal(H)\n",
    "mse_cv_3 = np.mean(((pressure - pressure_hat) / (1 - hii))**2)\n",
    "print(f\"MSE_CV = {mse_cv_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse_cv_1 / mse_cv_2, mse_cv_1 / mse_cv_3, mse_cv_1 / mse_pressure, np.sqrt(mse_cv_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to question 4.2(b): What $\\text{MSE}_\\text{CV}$ did you get?\n",
    "\n",
    "I got $\\text{MSE}_\\text{CV} = 0.059$. This is approximately 23% greater than the mean squared error\n",
    "and it is a more realistic estimate of the error we can expect to make for new samples. Just to see what\n",
    "this estimate looks like, we can highlight the region within the root mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(temperature, pressure, label=\"Raw data\", color=\"0.3\")\n",
    "ax.set(xlabel=\"Temperature (°F)\", ylabel=\"Pressure (inches Hg)\")\n",
    "\n",
    "line, = ax.plot(temperature, pressure_hat, label=text, lw=3)\n",
    "rmsep = np.sqrt(mse_cv_1)\n",
    "upper = pressure_hat + rmsep\n",
    "lower = pressure_hat - rmsep\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    [temperature[0], temperature[-1]],\n",
    "    [upper[0], upper[-1]],\n",
    "    lw=1, ls=\":\", color=line.get_color(),\n",
    "    label=\"$\\hat{y} + \\mathrm{RMSEP}$\",\n",
    ")\n",
    "ax.plot(\n",
    "    [temperature[0], temperature[-1]],\n",
    "    [lower[0], lower[-1]],\n",
    "    lw=1, ls=\"--\", color=line.get_color(),\n",
    "    label=\"$\\hat{y} - \\mathrm{RMSEP}$\",\n",
    ")\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the points fall within the error estimated from the cross validation. However, there is one single\n",
    "point that seems off... But this is a story for a later time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimizing the sum of squared errors (assuming that we have $n$\n",
    "measurements of $y$ and $x$),\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "We have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimization,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in the ``normal'' least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Leave-one-out cross-validation\n",
    "\n",
    "In Leave-one-out cross-validation (LOOCV), we first pick one sample,\n",
    "measurement number $j$, and we fit the model using the $n-1$ other points\n",
    "(all points except $j$). After the fitting, we check how well the model can predict\n",
    "measurement $j$ by calculating the difference between the\n",
    "measured ($y_j$) and predicted ($\\tilde{y}_j$) value. This difference, $r_j = y_{j} - \\tilde{y}_j$, is\n",
    "called the predicted residual, and it tells us the error we just made.\n",
    "\n",
    "There is nothing special about picking point $j$, and we can try all possibilities\n",
    "of leaving one point out, fitting the model using the remaining $n-1$\n",
    "measurements, and predicting the value we left out.\n",
    "After doing this for all possibilities, we have fitted the model\n",
    "$n$ times and calculated $n$ predicted residuals. The mean squared error (obtained from the squared\n",
    "residuals), $\\mathrm{MSE}_{\\mathrm{CV}}$, can then be used\n",
    "to estimate the error in the model,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the measured $y$ in experiment $i$, and $\\tilde{y}_i$ is the\n",
    "predicted $y$, using a model which was fitted using all points *except* $y_i$.\n",
    "\n",
    "For a polynomial fitting, there is an alternative to refitting the model $n$ times. In fact,\n",
    "we can show that for polynomial fitting, the mean squared error can\n",
    "be obtained by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{MSE}_{\\mathrm{CV}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\tilde{y}_i)^2 =\n",
    "\\frac{1}{n}\\sum_{i=1}^{m} \\left(\\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{y}_i$'s are predicted values using the\n",
    "model fitted with *all data points*,\n",
    "and $h_{ii}$ is the $i$'th diagonal element of the\n",
    "$\\mathbf{H}$ matrix (the projection matrix,\n",
    "see Eq.(4.49) on page 49 in our textbook),\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{H} =\n",
    "\\mathbf{X} \n",
    "\\left( \n",
    "  \\mathbf{X}^\\mathrm{T} \\mathbf{X}\n",
    "\\right)^{-1}\n",
    "\\mathbf{X}^\\mathrm{T} = \\mathbf{X} \\mathbf{X}^+,\n",
    "\\end{equation}\n",
    "\n",
    "Note the difference between $\\hat{y}_i$ and $\\tilde{y}_i$, and the\n",
    "fact that we  do not have to do the\n",
    "refitting(!) to obtain the $\\mathrm{MSE}_{\\mathrm{CV}}$.\n",
    "\n",
    "When you calculate $\\mathrm{MSE}_{\\mathrm{CV}}$ use one of the two approaches above or both\n",
    "if you want to see if they give the same answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
