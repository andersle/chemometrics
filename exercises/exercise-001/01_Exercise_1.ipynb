{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Exercise set 1: Least squares regression\n",
    "\n",
    "This exercise focuses on practical applications of least squares regression with Python. You will learn how to apply least squares regression to fit models of different kinds and how to evaluate the results.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Apply ordinary least squares regression to fit a linear model to data, using [NumPy](https://numpy.org/), [SciPy](https://scipy.org/), [scikit-learn](https://scikit-learn.org/), and [statsmodels](https://www.statsmodels.org/).\n",
    "* Compare the quality of different regression models by inspecting their residuals.\n",
    "* Estimate errors for coefficients by obtaining their confidence intervals.\n",
    "* Apply weighting to data points in least squares regression.\n",
    "* Use non-linear least squares regression.\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [1.1(c)](#1.1(c)) and [1.1(d)](#1.1(d)): Ordinary least squares regression and comparison of quality by residuals.\n",
    "- [1.2(d)](#1.2(d)) and [1.2(e)](#1.2(e)): Weighted least squares regression and how to obtain confidence intervals for regression parameters.\n",
    "- [1.3(b)](#1.3(b)): Non-linear least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1759",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Polynomial regression with least squares\n",
    "\n",
    "The temperature (°C) is measured continuously over time at a high altitude\n",
    "in the atmosphere using a\n",
    "weather balloon. Every hour a measurement is made and sent to an onboard computer.\n",
    "The measurements are \n",
    "shown in Fig. 1 and can be found in [the data file](temperature.csv) (located at 'temperature.csv').\n",
    "\n",
    "<figure>\n",
    "<img src=\"Fig_1_1.png\" width=\"50%\">\n",
    "<figcaption><p style='text-align: center;'><b>Fig. 1:</b> Measured temperature as a function of time.</p></figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75853a",
   "metadata": {},
   "source": [
    "### 1.1(a)\n",
    "\n",
    "To model the temperature ($y$) as a function of the time ($x$), we choose a second-order polynomial:\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2.\n",
    "\\end{equation}\n",
    "\n",
    "Explain how you can formulate this on a form suitable for least-squares regression,\n",
    "$\\mathbf{y} = \\mathbf{X} \\mathbf{b}$. That is:\n",
    "\n",
    "1. What do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ contain?\n",
    "2. What does the matrix $\\mathbf{X}$ contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418ebb",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(a): What are $\\mathbf{y}$,  $\\mathbf{b}$, and $\\mathbf{X}$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e143df",
   "metadata": {},
   "source": [
    "### 1.1(b)\n",
    "\n",
    "Fit a second-order polynomial model,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "to the given data by making use of [numpy.polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). \n",
    "\n",
    "1. Obtain the parameters $b_0$, $b_1$, and $b_2$.\n",
    "2. Plot your model: Create a scatter plot of the original data points. Overlay a line plot of the fitted quadratic model on the same graph.\n",
    "3. Calculate the [residuals](https://en.wikipedia.org/wiki/Errors_and_residuals) and create a scatter plot of the residuals against the fitted values. \n",
    "4. Based on your results, how do you assess your model? Please see [What is Considered a Good vs. Bad Residual Plot?](https://www.statology.org/good-vs-bad-residual-plot/) for a short explanation of what to look for in the residual plot.\n",
    "\n",
    "Below, you will find some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "data = pd.read_csv(\"temperature.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452d298",
   "metadata": {},
   "source": [
    "To fit a polynomial to your data, you have several options. Here are three popular choices:\n",
    "\n",
    "1. [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). This is the simplest option, suitable for most basic polynomial fitting tasks. It uses a least squares approach to fit a polynomial of a given degree to your data. \n",
    "2. [Ordinary least squares (OLS)](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html) from [statsmodels](https://www.statsmodels.org): This method provides more detailed results than polyfit, including error estimates for the coefficients. It's a good choice if you need more information about the fit. \n",
    "3. [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [scikit-learn](https://scikit-learn.org/): This is a more general approach that can be used for a variety of regression tasks, including polynomial fitting. It's particularly useful if you want to combine polynomial fitting with other methods, such as cross-validation.\n",
    "\n",
    "For options 2 and 3, we have to \"construct\" the $\\mathbf{X}$-matrix (see [1.1(a)](#1.1(a))), while `polyfit` will do this for us. We select the simplest option and use `polyfit` in this exercise. Here is one example for finding a first-order polynomial to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "param = np.polyfit(x, y, deg=1)  # Selects a second order polynomial\n",
    "\n",
    "# param now contains the parameters:\n",
    "equation = f\"y = {param[0]:.3f}x + {param[1]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# And we can find the residuals/errors\n",
    "residual = y - y_hat\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "ax1.legend()\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe755f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for fitting the model, plotting it, and the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c95ca",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(b): What are the coefficients of the second-order polynomial and how do you assess (based on the two plots you made) your model?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee026a2",
   "metadata": {},
   "source": [
    "### 1.1(c)\n",
    "\n",
    "In this problem, you will explore how the choice of polynomial order affects the model's ability to fit the temperature data.\n",
    "\n",
    "1. Extend your code from [1.1(b)](#1.1(b)) to fit polynomial models of orders 1 to 5 to the temperature data.\n",
    "2. Plot the fitted curves for all models together with the raw data on the same graph (time vs. temperature).\n",
    "3. Plot the residuals for each model in separate scatter plots.\n",
    "\n",
    "Which polynomial order best models temperature as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5874b4",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(c): Which polynomial order best models temperature as a function of time?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863e1c9",
   "metadata": {},
   "source": [
    "### 1.1(d)\n",
    "Obtain the sum of squared residuals for each polynomial you made in [1.1(c)](#1.1(c)) and plot this as a function of the\n",
    "polynomial degree. Use this plot to determine (from visual inspection) the best polynomial\n",
    "order for modelling the temperature as a function of time. Does this agree with your\n",
    "assessment from [1,1(c)](#1.1(c))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56429fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ca394",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(d): What polynomial order do you recommend?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36ee05",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Weighted least squares\n",
    "\n",
    "In this exercise, we will use least squares regression to investigate a real-world phenomenon: the decay of beer froth over time. This is to demonstrate the application of regression in the study of a physical phenomenon to obtain a physical quantity (in this case a characteristic time for the decay). \n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this [research on the decay of beer froth](https://doi.org/10.1088/0143-0807/23/1/304), and we will here reproduce the data analysis. In particular, we will use the reported raw data and carry out a weighted least squares regression. In addition, we will also obtain an error estimate (as a confidence interval) for the determined physical quantity.\n",
    "\n",
    "\n",
    "The file [erdinger.csv](erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights. \n",
    "\n",
    "\n",
    "**Please use [scikit-learn](https://scikit-learn.org/) and [startsmodels](https://www.statsmodels.org) for the fitting in this exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c00b5",
   "metadata": {},
   "source": [
    "### 1.2(a)\n",
    "Create a linear model (first-order polynomial) for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the [coefficient of determination ($R^2$)](https://en.wikipedia.org/wiki/Coefficient_of_determination), and plot\n",
    "the residuals. Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "**Note:** You do not need to add code for this part, but make sure that you understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = data[\"time\"].to_numpy()\n",
    "height = data[\"height\"].to_numpy()\n",
    "height_error = data[\"height-error\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# To fit a model with scikit-learn we do the following:\n",
    "model1 = LinearRegression(fit_intercept=True)\n",
    "X = time.reshape(-1, 1)\n",
    "model1.fit(X, height)\n",
    "\n",
    "# We can use the model for prediction by:\n",
    "y_hat_1 = model1.predict(X)\n",
    "\n",
    "# To calculate R²:\n",
    "r2_model1 = model1.score(X, height)\n",
    "# or:\n",
    "r2_model1 = r2_score(height, y_hat_1)\n",
    "\n",
    "# We can calculate the mean squared error (MSE) for the model:\n",
    "mse_model1 = mean_squared_error(height, y_hat_1)\n",
    "# Summarize the model with some short text:\n",
    "model1_txt = f\"y = {model1.coef_[0]:.3g}x + {model1.intercept_:.3g}\"\n",
    "model1_txt = f\"{model1_txt}\\n(R² = {r2_model1:.3g}, MSE = {mse_model1:.3g})\"\n",
    "print(model1_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ed443",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4,  # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    y_hat_1,\n",
    "    lw=3,\n",
    "    label=model1_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(y_hat_1, height - y_hat_1)\n",
    "ax2.set(xlabel=\"Predicted by the model (ŷ)\", ylabel=\"Residuals (y - ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faead54",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(a): Is this linear model suitable for estimating the froth height as a function of time?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a6adb",
   "metadata": {},
   "source": [
    "### 1.2(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that this leads to\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter.\n",
    "We will assume that $h(0)$ is a known parameter, equal to the initial height of the froth.\n",
    "\n",
    "Show how you can transform the equation above to a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12feeb3",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(b):\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941180",
   "metadata": {},
   "source": [
    "### 1.2(c)\n",
    "Use the linear transformation you found in [1.2(b)](#1.2(b)) to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$.\n",
    "\n",
    "**Hint:** The equation, $y=bx$, above does not include the usual constant term.\n",
    "This will modify the least squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept).\n",
    "To do the fitting without the intercept, set `fit_intercept=False` when creating the linear regression model:\n",
    "```python\n",
    "model2 = LinearRegression(fit_intercept=False)  # New model, without intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade452",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(c): What value did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae3b04",
   "metadata": {},
   "source": [
    "### 1.2(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276 \\pm 14$s which is probably lower than the\n",
    "value you found in [1.2(c)](#1.2(c)).\n",
    "We will now attempt to reproduce the results of Leike by using weighted least squares regression. The motivation for using weighted least squares regression is that the errors given in the raw data are not constant. To account for this, we can use weighted least squares regression which gives more weight to data points with smaller errors.\n",
    "\n",
    "To assign the weights ($w_i$) we can use $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting log-transformed values to $y = \\log (h(t) / h(0))$, and this will modify the errors.\n",
    "If you are familiar with [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that the error in $y$ ($\\sigma_y$) is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_y^2 = \\frac{\\sigma_h^2}{h^2} ,\n",
    "\\end{equation}\n",
    "\n",
    "which says that we can get the error in $y$ by dividing the measured error by the measured height.\n",
    "\n",
    "Do the following steps to perform the weighted\n",
    "least squares:\n",
    "\n",
    "1. Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "2. Calculate weights for your $y$ values as $1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "3. Perform a weighted least squares fitting using the calculated weights. Estimate $\\tau$, plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to run weighted least squares:\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# Just create some weights (not correct for 1.2(d))\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "model.fit(X, height, sample_weight=weights)  # Do fitting, but use the weights\n",
    "r2 = model.score(\n",
    "    X, height, sample_weight=weights\n",
    ")  # Calculate R² (considering the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe90764",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(d): What value did you get for $\\tau$? How does it compare to Leike's result?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd117a24",
   "metadata": {},
   "source": [
    "### 1.2(e)\n",
    "We can use the measured errors to estimate the error in the $\\tau$ parameter. Adopt the example code below, using [statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/wls.html) to compute a 95% confidence interval for $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some example code to help you get started with statsmodels:\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = time.reshape(-1, 1)\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "model_wls = sm.WLS(height, X, weights=weights)\n",
    "results_wls = model_wls.fit()\n",
    "print(results_wls.summary(alpha=0.05))\n",
    "# (Setting alpha=0.05 will calculate a 100(1-alpha)% confidence interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7668117",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in [Appendix B](#B.-The-summary-results-from-statsmodels). We only need this part:\n",
    "\n",
    "```code\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "x1             0.0452      0.015      2.997      0.010       0.013       0.077\n",
    "==============================================================================\n",
    "```\n",
    "\n",
    "where `coef` gives the fitted coefficient and the numbers below `[0.025      0.975]` is the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can get the confidence interval by:\n",
    "results_wls.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707141e",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(e): What confidence interval did you get for $\\tau$?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be7fbe",
   "metadata": {},
   "source": [
    "## Exercise 1.3: Non-linear least squares\n",
    "\n",
    "In spectroscopy, it is often necessary to \"deconvolve\" a spectrum into a number of overlapping spectral peaks. The individual peaks can often be approximated as Gaussian functions of amplitude $\\beta$, peak wavelength $\\lambda$   and standard deviation $\\sigma$. Hence, the spectrum $S(\\lambda)$ may be written as, \n",
    "\n",
    "\\begin{equation}\n",
    "S(\\lambda) = \\sum_{i=1}^N \\beta_i \\text{e}^{-(\\lambda - \\lambda_i)^2 / 2\\sigma_i^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of overlapping peaks.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Parameters for spectral peaks</p>  \n",
    "\n",
    "|             | Peak 1   | Peak 2   | Peak 3   |\n",
    "|-------------|----------|----------|----------|\n",
    "| $\\beta_i$   | 0.2      | 0.4      | 0.3      |\n",
    "| $\\lambda_i$ | 4.0      | 5.5      | 7.2      |\n",
    "| $\\sigma_i$  | 0.5      | 0.8      | 0.9      |\n",
    "\n",
    "\n",
    "We will in this exercise use non-linear least squares to determine the parameters in Table 1 for a noisy spectrum.\n",
    "We will use [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) from [SciPy](https://scipy.org/) to do this. You can also use [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) from [SciPy](https://scipy.org/) for non-linear least squares: this is simpler and requires less setup, but you have less control over the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b013cf",
   "metadata": {},
   "source": [
    "### 1.3(a)\n",
    "\n",
    "1. Use the parameter values in Table 1 to create a noise-free absorption spectrum $A_0 (\\lambda)$ and plot it as a function of $\\lambda$.\n",
    "2. Simulate a noisy spectrum $A(\\lambda)$ by adding Gaussian noise of amplitude (standard deviation) $\\gamma=0.05$ to $A_0(\\lambda)$. Plot this spectrum as well.\n",
    "\n",
    "**Hint:** You can use [numpy.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) to generate Gaussian noise. For example (to generate with a standard deviation of 0.01 with the same shape as a given matrix `mat`):\n",
    "```python\n",
    "np.random.normal(loc=0, scale=0.01, size=mat.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e938f8",
   "metadata": {},
   "source": [
    "### 1.3(b)\n",
    "\n",
    "Use non-linear least squares to estimate the Gaussian parameters for the noisy spectrum $A(\\lambda)$ you generated in [1.3(a)](#1.3(a)). Investigate how the solution depends on your initial guess and the noise-level $\\gamma$ (try at least two values for $\\gamma$: 0.05 and 0.01).\n",
    "\n",
    "**Hint:** The code below outlines how to set up and perform non-linear least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example will fit data to the following function:\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = 0.5 * np.sin(10 * x) + 2.0 * np.exp(x + 1)\n",
    "\n",
    "\n",
    "# First we define the model we will fit:\n",
    "def model(x, params):\n",
    "    \"\"\"Calculate y using the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x: The independent variable.\n",
    "        params: A list of parameters:\n",
    "\n",
    "    Returns:\n",
    "        The value of the function at x.\n",
    "    \"\"\"\n",
    "    return params[0] * np.sin(params[1] * x) + params[2] * np.exp(\n",
    "        x + params[3]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the objective function we will minimize:\n",
    "def objective(params, x, y):\n",
    "    y_fit = model(x, params)\n",
    "    return np.sum((y - y_fit) ** 2)  # Return sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimization\n",
    "\n",
    "# Initial guess for the parameters:\n",
    "initial_guess = [1.0, 20, 1.0, 2]\n",
    "\n",
    "# Set up boundaries for the coefficients, these\n",
    "# are on the form (min, max) for each parameter\n",
    "bounds = [\n",
    "    (0.1, 4.0),\n",
    "    (1, 20.0),\n",
    "    (0.1, 4.0),\n",
    "    (0.1, 5),\n",
    "]\n",
    "\n",
    "result = minimize(\n",
    "    objective,\n",
    "    initial_guess,\n",
    "    args=(x, y),\n",
    "    # method=\"L-BFGS-B\", # you can specify the method, or make SciPy choose\n",
    "    bounds=bounds,\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxiter\": 5000,\n",
    "    },  # Print information, and do maximum 5000 iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted parameters:\n",
    "fit_params = result.x\n",
    "print(fit_params)\n",
    "# Recalculate using the fitted parameters:\n",
    "y_fit = model(x, fit_params) \n",
    "# Plot the fitted signal\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(x, y, label=\"True signal\", lw=5, alpha=0.4)\n",
    "ax.plot(x, y_fit, label=\"Fitted model\", color=\"black\", lw=2, ls=\":\")\n",
    "ax.set(xlabel=r\"$\\lambda$\", ylabel=r\"$S(\\lambda)$\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c4526",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(b): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise amplitude?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1eb9e",
   "metadata": {},
   "source": [
    "### 1.3(c)\n",
    "\n",
    "Modify your code to use separable least squares (SLS). Investigate how the solution depends on your initial guess and the noise level ($\\gamma$) (try at least two values for $\\gamma$: 0.05 and 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f36d",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(c): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise amplitude? Are they different from the ones found in [1.3(b)](#1.3(b))?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84451aff",
   "metadata": {},
   "source": [
    "### 1.3(d) (Optional)\n",
    "\n",
    "Use bootstrapping (see page 54 in our textbook) with replacement to obtain error estimates for the Gaussian parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e94294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f0acf",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(d): What are your error estimates?\n",
    "*Double click here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643b634",
   "metadata": {},
   "source": [
    "## Your feedback for Exercise 1\n",
    "\n",
    "We highly value your feedback as it will help us improve this exercise for future students (and also gauge the level for the following exercises). Please take a few minutes to answer the following questions:\n",
    "\n",
    "1. Length and difficulty:\n",
    "   - How long did it take you to complete this exercise?\n",
    "   - On a scale of 1 to 5 (1=too short, 5=too long), how would you rate the length of the exercise?\n",
    "   - What was the most challenging part of this exercise?\n",
    "   - On a scale of 1 to 5 (1=too easy, 5=too difficult), how would you rate the difficulty of the exercise?\n",
    "2. Example code:\n",
    "   - Would you have preferred more or less example code?\n",
    "   - Were there any parts of the exercise where you would have liked to see more code examples?\n",
    "3. Errors and inconsistencies:\n",
    "   - Did you encounter any errors or inconsistencies in the exercise instructions or data?\n",
    "4. General feedback\n",
    "   - How could this exercise be improved?\n",
    "   - Do you have any other comments or suggestions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71138c6c",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1159e",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimizing the sum of squared errors ($S$). With $n$\n",
    "measurements of $y$ and $x$,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2 = \\sum_{i=1}^n r_i^2\n",
    "\\end{equation}\n",
    "\n",
    "To minimize $S$ we calculate the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimization,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in \"normal\" least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9ef76",
   "metadata": {},
   "source": [
    "## B. The summary results from `statsmodels`\n",
    "\n",
    "The summary method in `statsmodels` prints out a lot of information.\n",
    "We have here fitted a model $y=a + bx$ to 10 $(x, y)$ points with `statsmodels`\n",
    "and the resulting summary output is printed below. This output is described in the\n",
    "sections below. \n",
    "\n",
    "```text\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                      y   R-squared:                       0.956\n",
    "Model:                            OLS   Adj. R-squared:                  0.951\n",
    "Method:                 Least Squares   F-statistic:                     175.6\n",
    "Date:                Tue, 14 Feb 2023   Prob (F-statistic):           1.00e-06\n",
    "Time:                        08:42:06   Log-Likelihood:                -16.957\n",
    "No. Observations:                  10   AIC:                             37.91\n",
    "Df Residuals:                       8   BIC:                             38.52\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          4.4248      0.931      4.754      0.001       2.278       6.571\n",
    "x1             1.9235      0.145     13.253      0.000       1.589       2.258\n",
    "==============================================================================\n",
    "Omnibus:                        3.674   Durbin-Watson:                   2.067\n",
    "Prob(Omnibus):                  0.159   Jarque-Bera (JB):                0.755\n",
    "Skew:                           0.464   Prob(JB):                        0.686\n",
    "Kurtosis:                       3.975   Cond. No.                         13.0\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be87c33",
   "metadata": {},
   "source": [
    "### B.1. Information about the model\n",
    "\n",
    "\n",
    "- **Dep. Variable:** The dependent variable (the variable we are predicting, $y$) in the model.\n",
    "- **Model:** The type of model we have created (OLS = Ordinary Least Squares).\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date & Time:** The date and time for when we created the model.\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 ($x$,$y$) values here).\n",
    "\n",
    "### B.2. Information about the calculation\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares). \n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of variables. In our case: $n - k - 1 = 10 - 1 - 1 = 8$. If we did the\n",
    "  fitting without the constant term (for instance, by centring the data first), this\n",
    "  number would be $n-k = 10-1=9$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** Calculations of standard errors assume homoscedastic errors.\n",
    "  If this is not the case, then the standard error is not computed correctly. There\n",
    "  are alternative ways of calculating the standard error; this field tells you\n",
    "  if statsmodels used a more robust method.\n",
    "  \n",
    "### B.3. Information about the overall quality\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "- **Adj. R-squared:** The adjusted $R^2$ for the model. Useful from comparing\n",
    "  models as this one will only increase (when adding more variables) if the\n",
    "  increase in $R^2$ is more than one would expect by chance.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero! Effectively, this compares the model we\n",
    "  have just made to an alternative model equal to the constant intercept term. \n",
    "  To use this value, we would have to decide on a $\\alpha$ level and look up a critical F-value.\n",
    "  This is some extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at\n",
    "  least as extreme as the one above if all regression coefficients are zero. \n",
    "  It is also known as the $p$-value.\n",
    "  If we have selected $\\alpha$ value, we will reject the null hypothesis if \n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis: We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error.\n",
    "  This is equivalent (if the errors are normally distributed)\n",
    "  to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** The\n",
    "  [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n",
    "  [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "  These can be directly calculated from the Log-Likelihood and are useful for comparing alternative\n",
    "  models. Generally, we prefer models with lower AIC and BIC.\n",
    "  \n",
    "### B.4. Information about the coefficients\n",
    "\n",
    "- **coef:** The determined coefficients for the model.\n",
    "\n",
    "- **std err:** The standard of the coefficients. This\n",
    "  is calculated from,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  s^2 = \\frac{SSE}{n - k - 1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "\n",
    "- **t, P>|t|, and [0.025 0.975]:** Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the $p$-value for such a $t$-test.\n",
    "  Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, these coefficients are indeed different from\n",
    "  zero.\n",
    "  Finally, the **[0.025 0.975]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not specify \n",
    "  $\\alpha$ here, but we can give it as a parameter. The default is $\\alpha=0.05$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
