{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d360fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For formatting of the code\n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY313,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ad08a",
   "metadata": {},
   "source": [
    "# Solution to Exercise set 1: Least squares regression\n",
    "\n",
    "This exercise focuses on practical applications of least squares regression with Python. You will learn how to apply least squares regression to fit models of different kinds and how to evaluate the results.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "After completing this exercise set, you will be able to:\n",
    "\n",
    "* Apply ordinary least squares regression to fit a linear model to data, using [NumPy](https://numpy.org/), [SciPy](https://scipy.org/), [scikit-learn](https://scikit-learn.org/), and [statsmodels](https://www.statsmodels.org/).\n",
    "* Compare the quality of different regression models by inspecting their residuals.\n",
    "* Estimate errors for coefficients by obtaining their confidence intervals.\n",
    "* Apply weighting to data points in least squares regression.\n",
    "* Use non-linear least squares regression.\n",
    "\n",
    "\n",
    "**To get the exercise approved, complete the following problems:**\n",
    "\n",
    "- [1.1(c)](#1.1(c)) and [1.1(d)](#1.1(d)): Ordinary least squares regression and comparison of quality by residuals.\n",
    "- [1.2(d)](#1.2(d)) and [1.2(e)](#1.2(e)): Weighted least squares regression and how to obtain confidence intervals for regression parameters.\n",
    "- [1.3(b)](#1.3(b)): Non-linear least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc1759",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Polynomial regression with least squares\n",
    "\n",
    "The temperature (°C) is measured continuously over time at a high altitude\n",
    "in the atmosphere using a\n",
    "weather balloon. Every hour a measurement is made and sent to an onboard computer.\n",
    "The measurements are \n",
    "shown in Fig. 1 and can be found in [the data file](temperature.csv) (located at 'temperature.csv').\n",
    "\n",
    "<figure>\n",
    "<img src=\"Fig_1_1.png\" width=\"50%\">\n",
    "<figcaption><p style='text-align: center;'><b>Fig. 1:</b> Measured temperature as a function of time.</p></figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75853a",
   "metadata": {},
   "source": [
    "### 1.1(a)\n",
    "\n",
    "To model the temperature ($y$) as a function of the time ($x$), we choose a second-order polynomial:\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2.\n",
    "\\end{equation}\n",
    "\n",
    "Explain how you can formulate this on a form suitable for least-squares regression,\n",
    "$\\mathbf{y} = \\mathbf{X} \\mathbf{b}$. That is:\n",
    "\n",
    "1. What do the vectors $\\mathbf{y}$ and $\\mathbf{b}$ contain?\n",
    "2. What does the matrix $\\mathbf{X}$ contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73418ebb",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(a): What are $\\mathbf{y}$,  $\\mathbf{b}$, and $\\mathbf{X}$?\n",
    "\n",
    "First, we rewrite the given model as a linear model by introducing\n",
    "the variables $x_1 = x$ and $x_2 = x^2$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = b_0 + b_1 x + b_2 x^2 = b_0 + b_1 x_1 + b_2 x_2 .\n",
    "\\end{equation*}\n",
    "\n",
    "We then let $y_{i}$ be the result of\n",
    "measurement no. $i$ and\n",
    "$x_{ij}$ the value of variable $j$ in the same measurement. Our model for this particular\n",
    "point is then:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} .\n",
    "\\end{equation*}\n",
    "\n",
    "In matrix form (assuming we have $n$ measurements), we get,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} \\\\\n",
    "1 & x_{21} & x_{22} \\\\\n",
    "\\vdots & \\vdots & \\vdots  \\\\\n",
    "1 & x_{n1} & x_{n2} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix} .\n",
    "$$\n",
    "\n",
    "We can also write the $\\mathbf{X}$ matrix using the original\n",
    "variables,\n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} \\\\\n",
    "1 & x_{21} & x_{22} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^2 \\\\\n",
    "1 & x_{2} & x_{2}^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{n} & x_{n}^2 \n",
    "\\end{bmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "Thus we have the following:\n",
    "\n",
    "1. $\\mathbf{y}$ contains the measured $y_i$ values and $\\mathbf{b}$ contains the parameters $b_0$, $b_1$, $b_2$, $b_3$.\n",
    "2. $\\mathbf{X}$ contains the variables as columns and a column of $1$'s\n",
    "  to account for the constant term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e143df",
   "metadata": {},
   "source": [
    "### 1.1(b)\n",
    "\n",
    "Fit a second-order polynomial model,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 x + b_2 x^2 ,\n",
    "\\end{equation}\n",
    "\n",
    "to the given data by making use of [numpy.polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). \n",
    "\n",
    "1. Obtain the parameters $b_0$, $b_1$, and $b_2$.\n",
    "2. Plot your model: Create a scatter plot of the original data points. Overlay a line plot of the fitted quadratic model on the same graph.\n",
    "3. Calculate the [residuals](https://en.wikipedia.org/wiki/Errors_and_residuals) and create a scatter plot of the residuals against the fitted values. \n",
    "4. Based on your results, how do you assess your model? Please see [What is Considered a Good vs. Bad Residual Plot?](https://www.statology.org/good-vs-bad-residual-plot/) for a short explanation of what to look for in the residual plot.\n",
    "\n",
    "Below, you will find some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "data = pd.read_csv(\"temperature.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452d298",
   "metadata": {},
   "source": [
    "To fit a polynomial to your data, you have several options. Here are three popular choices:\n",
    "\n",
    "1. [polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) from [NumPy](https://numpy.org/). This is the simplest option, suitable for most basic polynomial fitting tasks. It uses a least squares approach to fit a polynomial of a given degree to your data. \n",
    "2. [Ordinary least squares (OLS)](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html) from [statsmodels](https://www.statsmodels.org): This method provides more detailed results than polyfit, including error estimates for the coefficients. It's a good choice if you need more information about the fit. \n",
    "3. [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from [scikit-learn](https://scikit-learn.org/): This is a more general approach that can be used for a variety of regression tasks, including polynomial fitting. It's particularly useful if you want to combine polynomial fitting with other methods, such as cross-validation.\n",
    "\n",
    "For options 2 and 3, we have to \"construct\" the $\\mathbf{X}$-matrix (see [1.1(a)](#1.1(a))), while `polyfit` will do this for us. We select the simplest option and use `polyfit` in this exercise. Here is one example for finding a first-order polynomial to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "param = np.polyfit(x, y, deg=1)  # Selects a second order polynomial\n",
    "\n",
    "# param now contains the parameters:\n",
    "equation = f\"y = {param[0]:.3f}x + {param[1]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# And we can find the residuals/errors\n",
    "residual = y - y_hat\n",
    "\n",
    "ssr = np.sum(residual**2)  # Sum of squared residuals\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "ax1.legend()\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "ax2.set_title(f\"SSR = {ssr:.3f}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe755f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "param = np.polyfit(x, y, deg=2)  # Selects a second order polynomial\n",
    "\n",
    "# param now contains the parameters:\n",
    "equation = f\"y = {param[0]:.3f}x² + {param[1]:.3f}x + {param[2]:.3f}\"\n",
    "\n",
    "# To evaluate the polynomial, we use np.polyval:\n",
    "y_hat = np.polyval(param, x)\n",
    "\n",
    "# And we can find the residuals/errors\n",
    "residual = y - y_hat\n",
    "\n",
    "ssr = np.sum(residual**2)  # Sum of squared residuals\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.scatter(data[\"hour\"], data[\"temperature\"])\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.plot(x, y_hat, lw=3, color=\"black\", label=f\"Fitted line\\n{equation}\")\n",
    "ax1.legend()\n",
    "ax2.scatter(y_hat, residual)\n",
    "ax2.set(xlabel=\"Predicted by model ($ŷ_i$)\", ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "ax2.set_title(f\"SSR = {ssr:.3f}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c95ca",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(b): What are the coefficients of the second-order polynomial and how do you assess (based on the two plots you made) your model?\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table A:</b> Coefficients for the quadratic polynomial</p>  \n",
    "\n",
    "|  Coefficient | Value       |\n",
    "|--------------|-------------|\n",
    "| $b_0$        | 12.96       |\n",
    "| $b_1$        | 0.0091      |\n",
    "| $b_2$        | -0.012      |\n",
    "\n",
    "\n",
    "The quadratic curve in the figure above seems to miss a lot of the points. The residuals show a trend, which indicates that the model is not fully capturing the relationship between time and temperature. While both the quadratic model and the first-order model (from the example code) predict the overall decreasing trend in temperature, the quadratic model does not appear to provide a significantly better fit than the simpler linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee026a2",
   "metadata": {},
   "source": [
    "### 1.1(c)\n",
    "\n",
    "In this problem, you will explore how the choice of polynomial order affects the model's ability to fit the temperature data.\n",
    "\n",
    "1. Extend your code from [1.1(b)](#1.1(b)) to fit polynomial models of orders 1 to 5 to the temperature data.\n",
    "2. Plot the fitted curves for all models together with the raw data on the same graph (time vs. temperature).\n",
    "3. Plot the residuals for each model in separate scatter plots.\n",
    "\n",
    "Which polynomial order best models temperature as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"hour\"]\n",
    "y = data[\"temperature\"]\n",
    "\n",
    "\n",
    "order = [1, 2, 3, 4, 5]\n",
    "models = [np.polyfit(x, y, deg=orderi) for orderi in order]\n",
    "\n",
    "fig1, ax1 = plt.subplots(constrained_layout=True)\n",
    "ax1.scatter(x, y, label=\"Raw data\", color=\"black\")\n",
    "\n",
    "fig2, axes2 = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(order),\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=(len(order) * 4, 4),\n",
    ")\n",
    "fig2.suptitle(\"Residuals:\")\n",
    "\n",
    "\n",
    "ssr_all = []\n",
    "for i, orderi in enumerate(order):\n",
    "    param = np.polyfit(x, y, deg=orderi)\n",
    "    y_hat = np.polyval(param, x)\n",
    "    residual = y - y_hat\n",
    "    ssr = np.sum(residual**2)\n",
    "    ssr_all.append(ssr)\n",
    "\n",
    "    ax1.plot(x, y_hat, label=f\"Order: {orderi}\")\n",
    "    axes2[i].scatter(y_hat, residual)\n",
    "    if i == 0:\n",
    "        axes2[i].set(ylabel=\"Residual ($y_i - ŷ_i$)\")\n",
    "    axes2[i].set(xlabel=\"Predicted by model ($ŷ_i$)\")\n",
    "    axes2[i].set_title(f\"Order: {orderi}, SSR = {ssr:.3f}\", loc=\"left\")\n",
    "\n",
    "\n",
    "# Plot the fitted polynomial and residuals\n",
    "ax1.set(xlabel=\"Time (hour)\", ylabel=\"Temperature (°C)\")\n",
    "ax1.legend()\n",
    "\n",
    "sns.despine(fig=fig1)\n",
    "sns.despine(fig=fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5874b4",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(c): Which polynomial order best models temperature as a function of time?\n",
    "\n",
    "The residual plots for the third, fourth, and fifth-order models show no clear patterns or trends, indicating that these models are capturing the underlying structure of the data well. However, the higher-order models might be overfitting (modelling the noise). A third-order polynomial provides a good balance between complexity and goodness of fit. It captures the main features of the data without introducing unnecessary complexity. I would, therefore, recommend using a third-order polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863e1c9",
   "metadata": {},
   "source": [
    "### 1.1(d)\n",
    "Obtain the sum of squared residuals for each polynomial you made in [1.1(c)](#1.1(c)) and plot this as a function of the\n",
    "polynomial degree. Use this plot to determine (from visual inspection) the best polynomial\n",
    "order for modeling the temperature as a function of time. Does this agree with your\n",
    "assessment from [1,1(c)](#1.1(c))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56429fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data stored from 1.1(c):\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(order, ssr_all, ls=\"--\", marker=\"o\")\n",
    "ax.set_title(\"Sum of squared residuals\", loc=\"left\")\n",
    "ax.set_xticks(order)\n",
    "ax.set_xlabel(\"Polynomial degree\")\n",
    "ax.set_ylabel(\"Sum of squared residuals (°C)²\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ca394",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.1(d): What polynomial order do you recommend?\n",
    "\n",
    "We observe a significant drop in the sum of squared residuals when going from a second to a third-order polynomial. After that, the decrease in the sum of squared residuals becomes negligible. This suggest that further increases in complexity (for instance, going to a 4th or 5th order polynomial) does not substantially improve the fit. This is consisten with our findings in 1.1(c), where we observed that a third-order polynomial captured the main features of the data without unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36ee05",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Weighted least squares\n",
    "\n",
    "In this exercise we will use least squares regression to investigate a real-world phenomenon: the decay of beer froth over time. This is to demonstrate the application of regression in the study of a physical phenomena to obtain a physical quantity (in this case a characteristic time for the decay). \n",
    "\n",
    "Arnd Leike was awarded the 2002 [Ig Nobel prize](https://en.wikipedia.org/wiki/Ig_Nobel_Prize) for this [research on the decay of beer froth](https://doi.org/10.1088/0143-0807/23/1/304), and we will here reproduce the data analysis. In particular, we will use the reported raw data and carry out a weighted least squaers regression. In addition, we will also obtain an error estimate (as a confidence interval) for the determined physical quantity.\n",
    "\n",
    "\n",
    "The file [Data/erdinger.csv](Data/erdinger.csv)\n",
    "contains [measured heights](https://doi.org/10.1088/0143-0807/23/1/304) for beer\n",
    "froth as a function of time, along with the errors in the measured heights. \n",
    "\n",
    "\n",
    "**Please use [scikit-learn](https://scikit-learn.org/) and [startsmodels](https://www.statsmodels.org) for the fitting in this exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c00b5",
   "metadata": {},
   "source": [
    "### 1.2(a)\n",
    "Create a linear model (first-order polynomial) for the beer froth height as a function of time using least squares.\n",
    "Plot your model with the raw data, calculate the [coefficient of determination ($R^2$)](https://en.wikipedia.org/wiki/Coefficient_of_determination), and plot\n",
    "the residuals. Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "**Note:** You do not need to add code for this part, but make sure that you understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"erdinger.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87bfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = data[\"time\"].to_numpy()\n",
    "height = data[\"height\"].to_numpy()\n",
    "height_error = data[\"height-error\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85426c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# To fit a model with scikit-learn we do the following:\n",
    "model1 = LinearRegression(fit_intercept=True)\n",
    "X = time.reshape(-1, 1)\n",
    "model1.fit(X, height)\n",
    "\n",
    "# We can use the model for prediction by:\n",
    "y_hat_1 = model1.predict(X)\n",
    "\n",
    "# To calculate R²:\n",
    "r2_model1 = model1.score(X, height)\n",
    "# or:\n",
    "r2_model1 = r2_score(height, y_hat_1)\n",
    "\n",
    "# We can calculate the mean squared error (MSE) for the model:\n",
    "mse_model1 = mean_squared_error(height, y_hat_1)\n",
    "# Summarize the model with some short text:\n",
    "model1_txt = f\"y = {model1.coef_[0]:.3g}x + {model1.intercept_:.3g}\"\n",
    "model1_txt = f\"{model1_txt}\\n(R² = {r2_model1:.3g}, MSE = {mse_model1:.3g})\"\n",
    "print(model1_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ed443",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4,  # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    y_hat_1,\n",
    "    lw=3,\n",
    "    label=model1_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(y_hat_1, height - y_hat_1)\n",
    "ax2.set(xlabel=\"Predicted by the model (ŷ)\", ylabel=\"Residuals (y - ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faead54",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(a): Is this linear model suitable for estimating the froth height as a function of time?\n",
    "\n",
    "The linear model captures most of the trend in the decreasing height as a function of time and R² is quite high (0.944). However, it fails to account for the curvature in the data, especially at earlier times. The residual plot shows a clear pattern (U-shape), and the presence of a pattern in the residuals suggests that the linear model does not capture the relationship between height and froth accurately. Furthermore, the linear model predicts a negative froth height at around 480 seconds, which is physically impossible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a87c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate to see where we get negative heights:\n",
    "X2 = np.arange(400, 500, 10).reshape(-1, 1)\n",
    "model1.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a6adb",
   "metadata": {},
   "source": [
    "### 1.2(b)\n",
    "If we assume that the change in froth volume is proportional\n",
    "to the volume present at any given time, we can show that this leads to\n",
    "exponential decay of the froth height,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{h(t)}{h(0)} = \\exp \\left(-\\frac{t}{\\tau} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $h(t)$ is the height of the froth as a function of time $t$, and $\\tau$ is a parameter.\n",
    "We will assume that $h(0)$ is a known parameter, equal to the initial height of the froth.\n",
    "\n",
    "Show how you can transform the equation above to a linear equation of the form,\n",
    "\n",
    "\\begin{equation}\n",
    "y = b x,\n",
    "\\end{equation}\n",
    "\n",
    "and express $b, x, y$ in terms of $h, h(0), t, \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39c9ce",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(b):\n",
    "\n",
    "\n",
    "If we take the natural logarithm on both sides of the equation, we get,\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln \\left( \\frac{h(t)}{h(0)} \\right) = -\\frac{t}{\\tau} = -\\frac{1}{\\tau} \\times t .\n",
    "\\end{equation}\n",
    "\n",
    "Setting,\n",
    "\\begin{equation}\n",
    "y = \\ln \\left( \\frac{h(t)}{h(0)} \\right), \\quad x = t, \\quad b=-\\frac{1}{\\tau},\n",
    "\\end{equation}\n",
    " we get,\n",
    "\\begin{equation}\n",
    "\\underbrace{\\ln \\left( \\frac{h(t)}{h(0)} \\right)}_{y} = -\\frac{t}{\\tau} = \\underbrace{-\\frac{1}{\\tau}}_{b} \\times \\underbrace{t}_{x},\n",
    "\\end{equation}\n",
    "\n",
    "or $y = bx$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941180",
   "metadata": {},
   "source": [
    "### 1.2(c)\n",
    "Use the linear transformation you found in [1.2(b)](#1.2(b)) to create a new linear model where you estimate\n",
    "the value of $\\tau$. Plot your new model together with the raw data and calculate $R^2$.\n",
    "\n",
    "**Hint:** The equation, $y=bx$, above does not include the usual constant term.\n",
    "This will modify the least squares equation as shown in [Appendix A](#A.-Least-squares-without-the-intercept).\n",
    "To do the fitting without the intercept, set `fit_intercept=False` when creating the linear regression model:\n",
    "```python\n",
    "model2 = LinearRegression(fit_intercept=False)  # New model, without intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, transform y:\n",
    "y = np.log(height / height[0])\n",
    "X = time.reshape(-1, 1)\n",
    "\n",
    "# Create the new model, without intercept:\n",
    "model2 = LinearRegression(fit_intercept=False)\n",
    "model2.fit(X, y)\n",
    "\n",
    "# Calculate R²:\n",
    "r2_model2 = model2.score(X, y)\n",
    "\n",
    "# Convert predicted y back to heights:\n",
    "y_hat_2 = model2.predict(X)\n",
    "height_hat_2 = height[0] * np.exp(y_hat_2)\n",
    "\n",
    "# Calculate the mean squared error, based on the heights\n",
    "# (this is to compare with model 1)\n",
    "mse_model2 = mean_squared_error(height, height_hat_2)\n",
    "\n",
    "tau = -1.0 / model2.coef_[0]\n",
    "print(f\"τ = {tau:.4g} s\")\n",
    "\n",
    "model2_txt = f\"h(t) = h(0) exp(-t/{tau:4g})\"\n",
    "model2_txt = f\"{model2_txt}\\n(R² = {r2_model2:.3g}, MSE = {mse_model2:.3g})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4,  # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_2,\n",
    "    lw=3,\n",
    "    label=model2_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Let us plot these for heights so we can compare with model 1\n",
    "ax2.scatter(height_hat_2, height - height_hat_2)\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Residual, y - ŷ\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade452",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(c): What value did you get for $\\tau$?\n",
    "\n",
    "From the coefficient found in the least squares fit: $\\tau \\approx 290$. We see that the residuals are now all\n",
    "smaller in magnitude, but we are overestimating the height for a lot of the points. The residuals still show a slight curvature, suggesting that the exponential model might not be perfectly capturing the decay of the froth height.\n",
    "\n",
    "The R² value is now very close to 1, indicating that the exponential model explains a large proportion of the variance in the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae3b04",
   "metadata": {},
   "source": [
    "### 1.2(d)\n",
    "[Leike](https://doi.org/10.1088/0143-0807/23/1/304) found a\n",
    "value of $\\tau = 276 \\pm 14$s which is probably lower than the\n",
    "value you found in [1.2(c)](#1.2(c)).\n",
    "We will now attempt to reproduce the results of Leike by using weighted least squares regression. The motivation for using weighted least squares regression is that the errors given in the raw data are not contant. To account for this, we can use wighted least squares regression which gives more weight to data points with smaller errors.\n",
    "\n",
    "To assign the weights ($w_i$) we can use $w_i = 1/\\sigma_i^2$ where $\\sigma_i$ is the\n",
    "reported error for observation $i$. But we need to consider the fact that we\n",
    "are now fitting log-transformed values to $y = \\log (h(t) / h(0))$, and this will modify the errors.\n",
    "If you are familiar with [propagation of errors](https://en.wikipedia.org/wiki/Propagation_of_uncertainty),\n",
    "you should be able to show that the error in $y$ ($\\sigma_y$) is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_y^2 = \\frac{\\sigma_h^2}{h^2} ,\n",
    "\\end{equation}\n",
    "\n",
    "which says that we can get the error in $y$ by dividing the measured error by the measured height.\n",
    "\n",
    "Do the following steps to perform the weighted\n",
    "least squares:\n",
    "\n",
    "1. Calculate errors for your $y$ values according to $\\sigma_y^2 = \\sigma_{h}^2 / h^2$.\n",
    "\n",
    "2. Calculate weights for your $y$ values as $1/\\sigma_y^2$. Note: If\n",
    "  a $\\sigma_y$ value is zero, set the corresponding weight to zero.\n",
    "  \n",
    "3. Perform a weighted least squares fitting using the calculated weights. Estimate $\\tau$, plot your new model and calculate $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to run weighted least squares:\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# Just create some weights (not correct for 1.2(d))\n",
    "weights = 1.0 / data[\"height-error\"].to_numpy() ** 2\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "model.fit(X, height, sample_weight=weights)  # Do fitting, but use the weights\n",
    "r2 = model.score(\n",
    "    X, height, sample_weight=weights\n",
    ")  # Calculate R² (considering the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearRegression(fit_intercept=False)\n",
    "\n",
    "\n",
    "# 1. Calculate errors for y:\n",
    "sigma_y_sq = height_error**2 / height**2\n",
    "# 2. Calculate weights for y:\n",
    "weights = 1.0 / sigma_y_sq\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "# 3. Do weighted least squares:\n",
    "model3.fit(X, y, sample_weight=weights)\n",
    "# Calculate R² (considering the weights).\n",
    "r2_model3 = model3.score(X, y, sample_weight=weights)\n",
    "y_hat_3 = model3.predict(X)\n",
    "# Recalculate the heights:\n",
    "height_hat_3 = height[0] * np.exp(y_hat_3)\n",
    "\n",
    "# Calculate MSE, based on heights. For this, we need the weights for the\n",
    "# heights:\n",
    "weights_h = 1.0 / height_error**2\n",
    "weights_h[weights_h == float(\"inf\")] = 0\n",
    "weights_h /= sum(weights_h)\n",
    "# Normalize the weights (since scikit-learn is using normalized weights)\n",
    "\n",
    "mse_model3 = mean_squared_error(height, height_hat_3, sample_weight=weights_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_ = -1.0 / model3.coef_[0]\n",
    "print(f\"τ = {tau_:.4g} s\")\n",
    "\n",
    "model3_txt = f\"h(t) = h(0) exp(-t/{tau_:4g})\"\n",
    "model3_txt = f\"{model3_txt}\\n(R² = {r2_model3:.3g}, MSE = {mse_model3:.3g})\"\n",
    "print(model3_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1caa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    constrained_layout=True, ncols=2, figsize=(8, 4)\n",
    ")\n",
    "ax1.errorbar(\n",
    "    time,\n",
    "    height,\n",
    "    yerr=height_error,\n",
    "    label=\"Raw data\",\n",
    "    fmt=\"o\",  # Just show the symbols and no lines\n",
    "    capsize=4,  # Size of end of the error bars\n",
    ")\n",
    "ax1.plot(\n",
    "    time,\n",
    "    height_hat_3,\n",
    "    lw=3,\n",
    "    label=model3_txt,\n",
    ")\n",
    "ax1.set(xlabel=\"Time (s)\", ylabel=\"Height (cm)\")\n",
    "ax1.legend()\n",
    "\n",
    "# Let us plot these for heights so we can compare with model 1\n",
    "ax2.scatter(height_hat_3, (height - height_hat_3) * np.sqrt(weights_h))\n",
    "ax2.set(xlabel=\"ŷ\", ylabel=\"Weighted residual, w × (y - ŷ)\")\n",
    "ax2.set_ylim(-1.1, 2.0)\n",
    "\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe90764",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(d): What value did you get for $\\tau$? How does it compare to Leike's result?\n",
    "\n",
    "With the weighted approach, we get $\\tau = 277$ s, which is very close to the $276$ s reported by Leike.\n",
    "The weighted residuals are smaller in magnitude, but there might still be a weak trend. The high R² value indicates that the model captures the general trend well.\n",
    "\n",
    "We will next quantify the uncertainty in our estimate of $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd117a24",
   "metadata": {},
   "source": [
    "### 1.2(e)\n",
    "We can use the measured errors to estimate the error in the $\\tau$ parameter. Adopt the example code below, using [statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/wls.html) to compute a 95% confidence interval for $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some example code to help you get started with statsmodels:\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = time.reshape(-1, 1)\n",
    "# 1. Calculate errors for y:\n",
    "sigma_y_sq = height_error**2 / height**2\n",
    "# 2. Calculate weights for y:\n",
    "weights = 1.0 / sigma_y_sq\n",
    "weights[weights == float(\"inf\")] = 0  # Set infinite values to zero\n",
    "\n",
    "model_wls = sm.WLS(y, X, weights=weights)\n",
    "results_wls = model_wls.fit()\n",
    "print(results_wls.summary(alpha=0.05))\n",
    "# (Setting alpha=0.05 will calculate a 100(1-alpha)% confidence interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7668117",
   "metadata": {},
   "source": [
    "**Note:** A description of the summary from statsmodels can be found in [Appendix B](#B.-The-summary-results-from-statsmodels). We only need this part:\n",
    "\n",
    "```code\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "x1             0.0452      0.015      2.997      0.010       0.013       0.077\n",
    "==============================================================================\n",
    "```\n",
    "\n",
    "where `coef` gives the fitted coefficient and the numbers below `[0.025      0.975]` is the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_statsmodels = -1.0 / results_wls.params[0]\n",
    "tau_ci = results_wls.conf_int()[0]\n",
    "lower_limit = min(-1.0 / tau_ci)\n",
    "upper_limit = max(-1.0 / tau_ci)\n",
    "print(f\"τ = {tau_statsmodels:.4g} s\")\n",
    "print(f\"Confidence interval: {lower_limit:.4g} to {upper_limit:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get this as a ±\n",
    "uncertainty1 = upper_limit - tau_statsmodels\n",
    "uncertainty2 = tau_statsmodels - lower_limit\n",
    "uncertainty = max(uncertainty1, uncertainty2)\n",
    "print(f\"τ = {round(tau_statsmodels, 0):.4g} ± {round(uncertainty, 0):.4g} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707141e",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.2(e): What confidence interval did you get for $\\tau$?\n",
    "\n",
    "The confidence interval was 265.7 to 290.1 s. The uncertainty is 13 s, so τ = 277 ± 13 s. \n",
    "This compares well with the results of Leike: τ = 276 ± 14 s at 95% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be7fbe",
   "metadata": {},
   "source": [
    "## Exercise 1.3: Non-linear least squares\n",
    "\n",
    "In spectroscopy, it is often necessary to \"deconvolve\" a spectrum into a number of overlapping spectral peaks. The individual peaks can often be approximated as Gaussian functions of amplitude $\\beta$, peak wavelength $\\lambda$   and standard deviation $\\sigma$. Hence, the spectrum $S(\\lambda)$ may be written as, \n",
    "\n",
    "\\begin{equation}\n",
    "S(\\lambda) = \\sum_{i=1}^N \\beta_i \\text{e}^{-(\\lambda - \\lambda_i)^2 / 2\\sigma_i^2},\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of overlapping peaks.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Parameters for spectral peaks</p>  \n",
    "\n",
    "|             | Peak 1   | Peak 2   | Peak 3   |\n",
    "|-------------|----------|----------|----------|\n",
    "| $\\beta_i$   | 0.2      | 0.4      | 0.3      |\n",
    "| $\\lambda_i$ | 4.0      | 5.5      | 7.2      |\n",
    "| $\\sigma_i$  | 0.5      | 0.8      | 0.9      |\n",
    "\n",
    "\n",
    "We will in this exercise use non-linear least squares to determine the parameters in Table 1 for a noisy spectrum.\n",
    "We will use [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) from [SciPy](https://scipy.org/) to do this. You can also use [curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) from [SciPy](https://scipy.org/) for non-linear least squares: this is simpler and requires less setup, but you have less control over the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b013cf",
   "metadata": {},
   "source": [
    "### 1.3(a)\n",
    "\n",
    "1. Use the parameter values in Table 1 to create a noise-free absorption spectrum $A_0 (\\lambda)$ and plot it as a function of $\\lambda$.\n",
    "2. Simulate a noisy spectrum $A(\\lambda)$ by adding Gaussian noise of amplitude (standard deviation) $\\gamma=0.05$ to $A_0(\\lambda)$. Plot this spectrum as well.\n",
    "\n",
    "**Hint:** You can use [numpy.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) to generate Gaussian noise. For example (to generate with a standard deviation of 0.01 with the same shape as a given matrix `mat`):\n",
    "```python\n",
    "np.random.normal(loc=0, scale=0.01, size=mat.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get you started:\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Styling for plots:\n",
    "sns.set_theme(style=\"ticks\", context=\"notebook\", palette=\"colorblind\")\n",
    "\n",
    "# For showing matplotlib in jupyter:\n",
    "# (you can experiment with replacing \"inline\" with \"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will be using random number, we set the\n",
    "# seed to get reproducible numbers:\n",
    "np.random.seed(4175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculating A0\n",
    "betas = [0.2, 0.4, 0.3]\n",
    "lambdas = [4.0, 5.5, 7.2]\n",
    "sigmas = [0.5, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def gaussian(x, beta_i, lambda_i, sigma_i):\n",
    "    \"\"\"Helper method to calculate a gaussian.\"\"\"\n",
    "    return beta_i * np.exp(-((x - lambda_i) ** 2) / (2.0 * sigma_i**2))\n",
    "\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "spectrum_without_noise = np.zeros_like(x)\n",
    "\n",
    "for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "    spectrum_without_noise += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "\n",
    "# 2. Adding noise:\n",
    "# Let us make some variants with different noise levels:\n",
    "gammas = [0.01, 0.025, 0.05]\n",
    "spectrum_with_noise = []\n",
    "for gamma in gammas:\n",
    "    noise = np.random.normal(\n",
    "        loc=0, scale=gamma, size=spectrum_without_noise.shape\n",
    "    )\n",
    "    spectrum_with_noise.append(spectrum_without_noise + noise)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=3, figsize=(9, 3), sharex=True, sharey=True\n",
    ")\n",
    "for gamma, ax, spectrum in zip(gammas, axes, spectrum_with_noise):\n",
    "    ax.scatter(x, spectrum, label=\"With noise\")\n",
    "    ax.plot(\n",
    "        x,\n",
    "        spectrum_without_noise,\n",
    "        color=\"black\",\n",
    "        label=\"Without noise\",\n",
    "    )\n",
    "    ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "axes[0].legend(fontsize=\"x-small\", loc=\"upper left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e938f8",
   "metadata": {},
   "source": [
    "### 1.3(b)\n",
    "\n",
    "Use non-linear least squares to estimate the Gaussian parameters for the noisy spectrum $A(\\lambda)$ you generated in [1.3(a)](#1.3(a)). Investigate how the solution depends on your initial guess and the noise-level $\\gamma$.\n",
    "\n",
    "**Hint:** The code below outlines how to set up and perform non-linear least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the model we will fit:\n",
    "def model(x, params):\n",
    "    \"\"\"Calculate y using the given parameters.\n",
    "\n",
    "    Args:\n",
    "        x: The independent variable.\n",
    "        params: A list of parameters:\n",
    "\n",
    "    Returns:\n",
    "        The value of the function at x.\n",
    "    \"\"\"\n",
    "    betas = params[0::3]\n",
    "    lambdas = params[1::3]\n",
    "    sigmas = params[2::3]\n",
    "    signal = np.zeros_like(x)\n",
    "\n",
    "    for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "        signal += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the objective function we will minimize:\n",
    "def objective(params, x, y):\n",
    "    y_fit = model(x, params)\n",
    "    return np.sum((y - y_fit) ** 2)  # Return sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First experiment, we keep the initial guess the\n",
    "# same for all noise levels:\n",
    "\n",
    "# Initial guess for the parameters:\n",
    "initial_guess = [\n",
    "    # Peak 1:\n",
    "    0.3,\n",
    "    4.0,\n",
    "    1.0,\n",
    "    # Peak 2\n",
    "    0.3,\n",
    "    6.0,\n",
    "    1.0,\n",
    "    # Peak 3\n",
    "    0.3,\n",
    "    8.0,\n",
    "    1.0,\n",
    "]\n",
    "\n",
    "# Set up boundaries for the coefficients, these\n",
    "# are on form (min, max) for each parameter\n",
    "bounds = [\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for spectrum in spectrum_with_noise:\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_guess,\n",
    "        args=(x, spectrum),\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"disp\": False,\n",
    "            \"maxiter\": 5000,\n",
    "        },  # Print information, and do maximum 5000 iterations\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results:\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, ncols=3, figsize=(9, 3), sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "for result, gamma, spectrum, ax in zip(\n",
    "    results, gammas, spectrum_with_noise, axes\n",
    "):\n",
    "    fit_params = result.x\n",
    "    # Recalculate using the fitted parameters:\n",
    "    y_fit = model(x, fit_params)\n",
    "    ax.scatter(x, spectrum, label=\"Spectrum with noise\", alpha=0.3)\n",
    "    # ax.plot(x, A_zero, label=\"True spectrum without noise\", lw=5, alpha=0.4)\n",
    "    ax.plot(x, y_fit, label=\"Fitted model\", color=\"black\")\n",
    "    ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80805f",
   "metadata": {},
   "source": [
    "To assess accuracy, we compare the fitted models to the noise-free spectrum. We plot the fitted absorption values against the true values. Ideally, the points would fall on the line $y=x$, indicating a perfect fit. Deviations from this line reveal discrepancies between the model and the true spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162655ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(constrained_layout=True, ncols=2, figsize=(8, 4))\n",
    "\n",
    "axes[0].plot(\n",
    "    x,\n",
    "    spectrum_without_noise,\n",
    "    label=\"Without noise\",\n",
    "    color=\"black\",\n",
    "    lw=4,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "for result, gamma, spectrum in zip(results, gammas, spectrum_with_noise):\n",
    "    fit_params = result.x\n",
    "    y_fit = model(x, fit_params)\n",
    "    axes[0].plot(x, y_fit, label=f\"Fit to γ = {gamma}\", lw=2)\n",
    "    r2 = r2_score(spectrum_without_noise, y_fit)\n",
    "    mse = mean_squared_error(spectrum_without_noise, y_fit)\n",
    "    axes[1].scatter(\n",
    "        spectrum_without_noise,\n",
    "        y_fit,\n",
    "        label=f\"γ = {gamma}\\nR² = {r2:.3f}, MSE = {mse:.2g}\",\n",
    "    )\n",
    "axes[0].legend(fontsize=\"x-small\")\n",
    "axes[0].set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "\n",
    "axes[1].legend(fontsize=\"x-small\")\n",
    "# To help read the plot, we add the y=x line:\n",
    "xlim = axes[1].get_xlim()\n",
    "ylim = axes[1].get_ylim()\n",
    "axes[1].plot(xlim, ylim, color=\"black\", ls=\"--\")\n",
    "axes[1].set_xlim(xlim)\n",
    "axes[1].set_ylim(ylim)\n",
    "axes[1].set(xlabel=\"Spectrum without noise (y)\", ylabel=\"Fitted model (ŷ)\")\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "# Make a function to create a table for the fitted parameters:\n",
    "def make_table(result):\n",
    "    table_data = {\n",
    "        \"Peak 1\": [round(i, ndigits=3) for i in result.x[:3]],\n",
    "        \"Peak 2\": [round(i, ndigits=3) for i in result.x[3:6]],\n",
    "        \"Peak 3\": [round(i, ndigits=3) for i in result.x[6:]],\n",
    "    }\n",
    "    tablei = pd.DataFrame(\n",
    "        table_data, index=[r\"$\\beta_i$\", r\"$\\lambda_i$\", r\"$\\sigma_i$\"]\n",
    "    )\n",
    "    return tablei\n",
    "\n",
    "\n",
    "# Print out the parameters:\n",
    "for result in results:\n",
    "    tablei = make_table(result)\n",
    "    display_html(tablei)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf4d58",
   "metadata": {},
   "source": [
    "From the results above, we see that the non-linear least squares method finds a good overall fit to the noisy spectrum, as indicated by the high R² values and low mean squared errors in all cases. However, increasing the noise level leads to overfitting (we start to model more of the noise) and deviations from the true spectral shape. This is examplified by the peak visible (around $\\lambda = 4$) in the fitted model for the highest noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57020ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Second experiment, we try different initial conditions:\n",
    "# Here, we experiment with a few different values\n",
    "\n",
    "initial_guesses = [\n",
    "    [\n",
    "        0.3,\n",
    "        4.0,\n",
    "        1.0,\n",
    "        0.3,\n",
    "        6.0,\n",
    "        1.0,\n",
    "        0.3,\n",
    "        8.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        0.3,\n",
    "        4.0,\n",
    "        1.0,\n",
    "        0.3,\n",
    "        3.0,\n",
    "        1.0,\n",
    "        0.3,\n",
    "        2.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        0.1,\n",
    "        4.0,\n",
    "        1.5,\n",
    "        0.3,\n",
    "        6.0,\n",
    "        1.5,\n",
    "        0.2,\n",
    "        8.0,\n",
    "        1.5,\n",
    "    ],\n",
    "    [\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        0.1,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        0.1,\n",
    "        2.0,\n",
    "        1.0,\n",
    "        0.1,\n",
    "        3.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        0.8,\n",
    "        5.0,\n",
    "        0.25,\n",
    "        1.6,\n",
    "        5.0,\n",
    "        0.25,\n",
    "        1.9,\n",
    "        5.0,\n",
    "        0.25,\n",
    "    ],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, nrows=2, ncols=3, sharex=True, sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for i, initial_guess in enumerate(initial_guesses):\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_guess,\n",
    "        args=(x, spectrum_with_noise[2]),\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"disp\": False,\n",
    "            \"maxiter\": 5000,\n",
    "        },  # Print information, and do maximum 5000 iterations\n",
    "    )\n",
    "    fit_params = result.x\n",
    "    tablei = make_table(result)\n",
    "    display_html(tablei)\n",
    "    y_fit = model(x, fit_params)\n",
    "    mse = mean_squared_error(spectrum_with_noise[2], y_fit)\n",
    "    r2 = r2_score(spectrum_with_noise[2], y_fit)\n",
    "    axes[i].set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    axes[i].set_title(f\"R² = {r2:.3f}\", loc=\"left\")\n",
    "    axes[i].plot(x, y_fit)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b0c2f",
   "metadata": {},
   "source": [
    "Inspecting the plots above, we see that the fitted parameters can vary significantly depending on the initial guess. The initial guess for the peak position seem to be important for the quality of the fitted curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882c4526",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(b): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise amplitude?\n",
    "\n",
    "Please see the tables above for the parameter values. The obtained parameters are sensitive to both the initial guess and the noise amplitude. For instance, the peak position of the last peak varies from 5.934 to 9.945 when varying the initial conditions as in the code above.\n",
    "\n",
    "However, we find that we can get reasonable results in many cases, both for high noise levels and for initial guesses that are somewhat far off. This suggests that while careful selection of initial guesses and noise reduction are important, the method exhibits some robustness to variations in these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1eb9e",
   "metadata": {},
   "source": [
    "### 1.3(c)\n",
    "\n",
    "Modify your code to use separable least squares (SLS). Investigate how the solution depends on your initial guess and the noise-level ($\\gamma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We modify our set-up so that we can find the beta parameters\n",
    "# via least squares, and the other parameters via non-linear least squares:\n",
    "\n",
    "\n",
    "def model_sls(x, betas, lambdas, sigmas):\n",
    "    \"\"\"Calculate y using the given parameters.\"\"\"\n",
    "    signal = np.zeros_like(x)\n",
    "    for beta_i, lambda_i, sigma_i in zip(betas, lambdas, sigmas):\n",
    "        signal += gaussian(x, beta_i, lambda_i, sigma_i)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def calculate_beta(x, y, lambdas, sigmas):\n",
    "    X = []\n",
    "    for lambda_i, sigma_i in zip(lambdas, sigmas):\n",
    "        X.append(gaussian(x, 1, lambda_i, sigma_i))\n",
    "    X = np.array(X).T\n",
    "    return np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "\n",
    "def sls_objective(nonlinear_params, x, y):\n",
    "    lambdas = nonlinear_params[0:3]\n",
    "    sigmas = nonlinear_params[3:]\n",
    "\n",
    "    betas = calculate_beta(x, y, lambdas, sigmas)\n",
    "    y_fit = model_sls(x, betas, lambdas, sigmas)\n",
    "    return np.sum((y - y_fit) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fd4d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_guess_sls = [\n",
    "    # Lambda:\n",
    "    4.0,\n",
    "    6.0,\n",
    "    8.0,\n",
    "    # Sigma:\n",
    "    1.0,\n",
    "    1.0,\n",
    "    1.0,\n",
    "]\n",
    "\n",
    "# Set up boundaries for the coefficients, these\n",
    "# are on form (min, max) for each parameter\n",
    "bounds_sls = [\n",
    "    (0.0, 10.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "]\n",
    "\n",
    "result = minimize(\n",
    "    sls_objective,\n",
    "    initial_guess_sls,\n",
    "    args=(x, spectrum_with_noise[2]),\n",
    "    bounds=bounds_sls,\n",
    "    options={\n",
    "        \"disp\": False,\n",
    "        \"maxiter\": 5000,\n",
    "    },\n",
    ")\n",
    "\n",
    "lambdas = result.x[0:3]\n",
    "sigmas = result.x[3:]\n",
    "betas = calculate_beta(x, spectrum_with_noise[2], lambdas, sigmas)\n",
    "\n",
    "\n",
    "table_data = {\n",
    "    \"Peak 1\": [round(i, ndigits=3) for i in [betas[0], lambdas[0], sigmas[0]]],\n",
    "    \"Peak 2\": [round(i, ndigits=3) for i in [betas[1], lambdas[1], sigmas[1]]],\n",
    "    \"Peak 3\": [round(i, ndigits=3) for i in [betas[2], lambdas[2], sigmas[2]]],\n",
    "}\n",
    "table = pd.DataFrame(\n",
    "    table_data, index=[r\"$\\beta_i$\", r\"$\\lambda_i$\", r\"$\\sigma_i$\"]\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = model_sls(x, betas, lambdas, sigmas)\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.scatter(x, spectrum_with_noise[2], label=\"Spectrum with noise\", alpha=0.3)\n",
    "ax.plot(\n",
    "    x, spectrum_without_noise, label=\"Spectrum without noise\", alpha=0.7, lw=3\n",
    ")\n",
    "ax.plot(x, y_fit, label=\"Fitted model\", color=\"black\", alpha=0.5, lw=3)\n",
    "ax.set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "ax.set_title(f\"γ = {gamma}\", loc=\"left\")\n",
    "ax.legend()\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second experiment, we try different initial conditions:\n",
    "# Here, we experiment with a few different values\n",
    "\n",
    "initial_guesses_sls = [\n",
    "    [\n",
    "        4.0,\n",
    "        6.0,\n",
    "        8.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        4.0,\n",
    "        3.0,\n",
    "        2.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        4.0,\n",
    "        6.0,\n",
    "        8.0,\n",
    "        1.5,\n",
    "        1.5,\n",
    "        1.5,\n",
    "    ],\n",
    "    [\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],\n",
    "    [\n",
    "        1.0,\n",
    "        2.0,\n",
    "        3.0,\n",
    "        0.25,\n",
    "        1.0,\n",
    "        1.5,\n",
    "    ],\n",
    "    [\n",
    "        5.0,\n",
    "        5.0,\n",
    "        5.0,\n",
    "        0.25,\n",
    "        0.25,\n",
    "        0.25,\n",
    "    ],\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    constrained_layout=True, nrows=2, ncols=3, sharex=True, sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for i, initial_guess in enumerate(initial_guesses_sls):\n",
    "    result = minimize(\n",
    "        sls_objective,\n",
    "        initial_guess,\n",
    "        args=(x, spectrum_with_noise[2]),\n",
    "        bounds=bounds_sls,\n",
    "        options={\n",
    "            \"disp\": False,\n",
    "            \"maxiter\": 5000,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    lambdas = result.x[0:3]\n",
    "    sigmas = result.x[3:]\n",
    "    betas = calculate_beta(x, spectrum_with_noise[2], lambdas, sigmas)\n",
    "\n",
    "    y_fit = model_sls(x, betas, lambdas, sigmas)\n",
    "    mse = mean_squared_error(spectrum_with_noise[2], y_fit)\n",
    "    r2 = r2_score(spectrum_with_noise[2], y_fit)\n",
    "    axes[i].set(xlabel=\"λ\", ylabel=\"S(λ)\")\n",
    "    axes[i].set_title(f\"R² = {r2:.3f}\", loc=\"left\")\n",
    "    axes[i].plot(x, y_fit)\n",
    "sns.despine(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f36d",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(c): What values did you find for the parameters? Do you find your parameters to depend on the initial guess and the noise amplitude? Are they different from the ones found in [1.3(b)](#1.3(b))?\n",
    "\n",
    "The SLS method results in parameters that are similar to the ones found by non-linear least squares. For a noise level of 0.05 (and similar initial guesses), we get the same values:\n",
    "\n",
    "|             |   Peak 1 |   Peak 2 |   Peak 3 |\n",
    "|:------------|---------:|---------:|---------:|\n",
    "| $\\beta_i$   |    0.118 |    0.481 |    0.103 |\n",
    "| $\\lambda_i$ |    4.124 |    5.899 |    7.971 |\n",
    "| $\\sigma_i$  |    0.294 |    1.289 |    0.489 |\n",
    "\n",
    "\n",
    "The parameters depend on both the initial guess and the noise amplitude (it is easier to fit to lower noise levels). Varying the initial guess indicates that SLS is not particularly more robust than non-linear least squares. Nevertheless, SLS successfully recovers reasonable parameter estimates in many scenarios, even with high noise or less accurate initial guesses.  Careful selection of initial guesses and noise reduction remain important factors to obtain accurate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84451aff",
   "metadata": {},
   "source": [
    "### 1.3(d) (Optional)\n",
    "\n",
    "Use bootstrapping (see page 54 in our textbook) with replacement to obtain error estimates for the Gaussian parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = [\n",
    "    # Peak 1:\n",
    "    0.3,\n",
    "    4.0,\n",
    "    1.0,\n",
    "    # Peak 2\n",
    "    0.3,\n",
    "    6.0,\n",
    "    1.0,\n",
    "    # Peak 3\n",
    "    0.3,\n",
    "    8.0,\n",
    "    1.0,\n",
    "]\n",
    "\n",
    "bounds = [\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.01, 2.0),\n",
    "    (0.0, 10.0),\n",
    "    (0.01, 2.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e94294",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 1000\n",
    "\n",
    "parameters_bootstrap = []\n",
    "for _ in range(number_of_samples):\n",
    "    idx = np.random.choice(len(x), size=len(x), replace=True)\n",
    "    x_sample = x[idx]\n",
    "    y_sample = spectrum_with_noise[2][idx]\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_guess,\n",
    "        args=(x_sample, y_sample),\n",
    "        bounds=bounds,\n",
    "        options={\n",
    "            \"disp\": False,\n",
    "            \"maxiter\": 5000,\n",
    "        },  # Print information, and do maximum 5000 iterations\n",
    "    )\n",
    "    if result.success:\n",
    "        parameters_bootstrap.append(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32431",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_bootstrap = np.array(parameters_bootstrap)\n",
    "for i in range(parameters_bootstrap.shape[1]):\n",
    "    param = parameters_bootstrap[:, i]\n",
    "    mean = np.mean(param)\n",
    "    std = np.std(param)\n",
    "    print(f\"{mean:.4g} {std:.4g}\")\n",
    "    lower_bounds = np.percentile(param, 2.5)\n",
    "    upper_bounds = np.percentile(param, 97.5)\n",
    "    print(f\"95% Confidence Intervals: {lower_bounds:.4g}, {upper_bounds:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f0acf",
   "metadata": {},
   "source": [
    "#### Your answer to question 1.3(d): What are your error estimates?\n",
    "\n",
    "Please see the numbers above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643b634",
   "metadata": {},
   "source": [
    "## Your feedback for Exercise 1\n",
    "\n",
    "We highly value your feedback as it will help us improve this exercise for future students (and also gauge the level for the following exercises). Please take a few minutes to answer the following questions:\n",
    "\n",
    "1. Length and difficulty:\n",
    "   - How long did it take you to complete this exercise?\n",
    "   - On a scale of 1 to 5 (1=too short, 5=too long), how would you rate the length of the exercise?\n",
    "   - What was the most challenging part of this exercise?\n",
    "   - On a scale of 1 to 5 (1=too easy, 5=too difficult), how would you rate the difficulty of the exercise?\n",
    "2. Example code:\n",
    "   - Would you have preferred more or less example code?\n",
    "   - Were there any parts of the exercise where you would have liked to see more code examples?\n",
    "3. Errors and inconsistencies:\n",
    "   - Did you encounter any errors or inconsistencies in the exercise instructions or data?\n",
    "4. General feedback\n",
    "   - How could this exercise be improved?\n",
    "   - Do you have any other comments or suggestions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71138c6c",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1159e",
   "metadata": {},
   "source": [
    "## A. Least squares without the intercept\n",
    "We are going to determine the parameter $b$ for the linear model,\n",
    "\n",
    "\\begin{equation}\n",
    "y =  b x,\n",
    "\\end{equation}\n",
    "\n",
    "and we do this by minimizing the sum of squared errors ($S$). With $n$\n",
    "measurements of $y$ and $x$,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n (y_i - b x_i)^2 = \\sum_{i=1}^n r_i^2\n",
    "\\end{equation}\n",
    "\n",
    "To minimize $S$ we calculate the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial S}{\\partial b} = -2 \\sum_{i=1}^n r_i x_i, \\quad\n",
    "\\frac{\\partial^2 S}{\\partial b^2} = 2\\sum_{i=1}^n x_i^2 \\geq 0,\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the second derivative is positive, except for the\n",
    "trivial case when $x_i = 0$, and we are indeed going to\n",
    "find a minimum.\n",
    "Requiring that $\\frac{\\partial S}{\\partial b} = 0$ gives,\n",
    "\n",
    "\\begin{equation}\n",
    "-2 \\sum_{i=1}^n r_i x_i = 0 \\implies \\sum_{i=1}^n (y_i x_i - b x_i^2) = 0 \\implies \n",
    "b = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "We can also repeat this derivation for weighted least squares. The sum of squared errors\n",
    "is then,\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\sum_{i=1}^n w_i (y_i - b x_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $w_i$ are the weights and, after minimization,\n",
    "\n",
    "\\begin{equation}\n",
    "b = \\frac{\\sum_{i=1}^n w_i y_i x_i}{\\sum_{i=1}^n w_i x_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "You can find more information on the weighted least squares method (with error analysis)\n",
    "in Bevington and Robinson <a name=\"cite_ref-1\"></a>[[1]](#bevington).\n",
    "Taylor <a name=\"cite_ref-2\"></a>[[2]](#taylor) states error formulas for\n",
    "the parameters that might be useful for cases when\n",
    "the error in $y$ is known and constant (e.g., as in \"normal\" least squares).\n",
    "\n",
    "\n",
    "<a name=\"bevington\"></a>[[1]](#cite_ref-1) Philip R. Bevington and D. Keith Robinson. Data reduction and error analysis for the physical sciences. 3rd ed. New York, NY: McGraw-Hill, 2003.\n",
    "\n",
    "<a name=\"taylor\"></a>[[2]](#cite_ref-2) John R. Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical\n",
    "    Measurements. 2nd ed. Sausalito, CA: University Science Books, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9ef76",
   "metadata": {},
   "source": [
    "## B. The summary results from `statsmodels`\n",
    "\n",
    "The summary method in `statsmodels` prints out a lot of information.\n",
    "We have here fitted a model $y=a + bx$ to 10 $(x, y)$ points with `statsmodels`\n",
    "and the resulting summary output is printed below. This output is described in the\n",
    "sections below. \n",
    "\n",
    "```text\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                      y   R-squared:                       0.956\n",
    "Model:                            OLS   Adj. R-squared:                  0.951\n",
    "Method:                 Least Squares   F-statistic:                     175.6\n",
    "Date:                Tue, 14 Feb 2023   Prob (F-statistic):           1.00e-06\n",
    "Time:                        08:42:06   Log-Likelihood:                -16.957\n",
    "No. Observations:                  10   AIC:                             37.91\n",
    "Df Residuals:                       8   BIC:                             38.52\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          4.4248      0.931      4.754      0.001       2.278       6.571\n",
    "x1             1.9235      0.145     13.253      0.000       1.589       2.258\n",
    "==============================================================================\n",
    "Omnibus:                        3.674   Durbin-Watson:                   2.067\n",
    "Prob(Omnibus):                  0.159   Jarque-Bera (JB):                0.755\n",
    "Skew:                           0.464   Prob(JB):                        0.686\n",
    "Kurtosis:                       3.975   Cond. No.                         13.0\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be87c33",
   "metadata": {},
   "source": [
    "### B.1. Information about the model\n",
    "\n",
    "\n",
    "- **Dep. Variable:** The dependent variable (the variable we are predicting, $y$) in the model.\n",
    "- **Model:** The type of model we have created (OLS = Ordinary Least Squares).\n",
    "- **Method:** We have used Least squares to find the parameters.\n",
    "- **Date & Time:** The date and time for when we created the model.\n",
    "- **No. Observations:** The number of observations in the data set (we had 10 ($x$,$y$) values here).\n",
    "\n",
    "### B.2. Information about the calculation\n",
    "- **Df Residuals:** Degrees of freedom for the residuals (sum of squares). \n",
    "  This is equal to $n - k - 1$ where $n$ is the number of observations and $k$ is\n",
    "  the number of variables. In our case: $n - k - 1 = 10 - 1 - 1 = 8$. If we did the\n",
    "  fitting without the constant term (for instance, by centring the data first), this\n",
    "  number would be $n-k = 10-1=9$.\n",
    "- **Df Model:** Degrees of freedom for the model (number of variables in the model).\n",
    "- **Covariance type:** Calculations of standard errors assume homoscedastic errors.\n",
    "  If this is not the case, then the standard error is not computed correctly. There\n",
    "  are alternative ways of calculating the standard error; this field tells you\n",
    "  if statsmodels used a more robust method.\n",
    "  \n",
    "### B.3. Information about the overall quality\n",
    "- **R-squared:** Coefficient of determination ($R^2$) for the model.\n",
    "- **Adj. R-squared:** The adjusted $R^2$ for the model. Useful from comparing\n",
    "  models as this one will only increase (when adding more variables) if the\n",
    "  increase in $R^2$ is more than one would expect by chance.\n",
    "- **F-statistic:** This is the result of an F-test where the null hypothesis is that all\n",
    "  regression coefficients are equal to zero! Effectively, this compares the model we\n",
    "  have just made to an alternative model equal to the constant intercept term. \n",
    "  To use this value, we would have to decide on a $\\alpha$ level and look up a critical F-value.\n",
    "  This is some extra work for us, so we typically rather focus on the **Prob (F-statistic)**.\n",
    "- **Prob (F-statistic):** This is the probability of getting an **F-statistic** at\n",
    "  least as extreme as the one above if all regression coefficients are zero. \n",
    "  It is also known as the $p$-value.\n",
    "  If we have selected $\\alpha$ value, we will reject the null hypothesis if \n",
    "  the $p$-value is smaller than $\\alpha$. Here, we have a very small $p$-value, and we reject the\n",
    "  null hypothesis: We conclude that at least one regression parameter is\n",
    "  significant for predicting $y$.\n",
    "- **Log-Likelihood:** In least squares, we are minimizing the squared error.\n",
    "  This is equivalent (if the errors are normally distributed)\n",
    "  to maximizing the likelihood. The value printed here is the\n",
    "  logarithm of the likelihood for the model.\n",
    "- **AIC and BIC:** The\n",
    "  [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and\n",
    "  [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "  These can be directly calculated from the Log-Likelihood and are useful for comparing alternative\n",
    "  models. Generally, we prefer models with lower AIC and BIC.\n",
    "  \n",
    "### B.4. Information about the coefficients\n",
    "\n",
    "- **coef:** The determined coefficients for the model.\n",
    "\n",
    "- **std err:** The standard of the coefficients. This\n",
    "  is calculated from,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\text{Var}(\\mathbf{b}) = s^2 \\cdot \\text{diag} \\left(\\mathbf{X}^\\top \\mathbf{X} \\right)^{-1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where,\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  s^2 = \\frac{SSE}{n - k - 1},\n",
    "  \\end{equation*}\n",
    "  \n",
    "  and $SSE$ is the sum of squared error/residuals, $n$ the number of data points (10 in this case)\n",
    "  and $k$ the number of variables (1 in this case).\n",
    "\n",
    "- **t, P>|t|, and [0.025 0.975]:** Some statistics for the\n",
    "  coefficients. **t** is the $t$ statistic, which is obtained by dividing\n",
    "  the coefficient by the standard error.\n",
    "  This is the statistic in a test where the null hypothesis is that the coefficient is zero.\n",
    "  To use the $t$ statistic we would have to consult a table with critical $t$-values for $n-k-1$\n",
    "  degrees of freedom. The **P>|t|** is the $p$-value for such a $t$-test.\n",
    "  Here, the $t$ statistic\n",
    "  is high (and the p-value is low) and we would reject this null hypothesis for both the\n",
    "  constant and x1. In other words, these coefficients are indeed different from\n",
    "  zero.\n",
    "  Finally, the **[0.025 0.975]**\n",
    "  represents a $100(1-\\alpha)\\%$ confidence interval for the coefficients. We did not specify \n",
    "  $\\alpha$ here, but we can give it as a parameter. The default is $\\alpha=0.05$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
